## LangGraph workflows

LangChain workflows are implemented as directed graphs that take a TypedDict object as input, and produce the interaction with the language model, or indeed any kind of output. The TypedDict object contains the _state_ of the graph, and the nodes are functions or co-routines that take a state as input and return a possibly modified state. A special key in the dictionary, 'messages', contains the messages exchanged with the language model. As the state goes through the function nodes, the state is modified. When the graph reaches the end, the state has its final form.

There are two design issues to contend with here:

- the graph represents one interaction within a chat. When the user interacts with the application, the chat develops, creating a workflow on top of the LangChain workflow. Importantly, in the Gradio framework the state is saved on the client. This means that only the messages history is saved between chats. At the beginning of each chat, the graph may be re-initalized to the history. However, if a user message is modified prior to sending it to the model (for example, by integrating it with context from the vector database), this goes into the state object, not into the messages history. It is therefore important to model mentally the fact that the chat state is curtailed between chat turns, unlike the within-graph state.
- like any Runnable object, the graph may be called with .invoke/.ainvoke or .stream/.astream. The response to the caller differs in these two cases, because invoke returns a state, while stream streams something that is not necessarily the whole state. When calling stream, a stream_mode argument determines what the graph is streaming: when set to 'messages', it will only stream the messages portion of the state. After the streaming is over, everything that is not 'messages' is lost (unless one uses checkpointers).

These two issues are related as they both concern the state management model. The curtailed state content between chat turns forces a RAG application to repeat retrieval of context at each chat turn. The necessity to stream only 'messages' through the Gradio app means that the stream from the graph must be de-multiplexed if one needs retrieving anything more than that (this last problem does not arise with invoke, since this function always blocks to return everything, which may be then used as needed).

The design choice here was to separate the design of the graph from that of streaming it. The graph design must accommodate the fact that memory of past context retrievals is lost, and repeat context retrieval at each chat turns. Streaming is provided by several streaming co-routines that may be chosen at runtime depending on the configuration of the application. The stream_adapters module contains stream adapters that, combined with the right invocation of .stream/.astream's stream_model argument, prograssively extract messages and other parts of the state. At the end, only the last message text is streamed to Gradio, but the whole state may be saved to allow logging to the chat database the interaction.

Furthermore, other aspects of the interaction with the model (like validation of content) take place at the streaming stage. The validating stream adapter needs write access to the state object to record the rejection of the interaction. This means that the state content evolution does not necessarily terminate with the graph completion, but will have been finalized when streaming terminates.

Logging requires trade-offs between its intrinsic coupling to the specific content produced by the graph and the coupling to the correct adapter saving the state for logging to be executed at the end of streaming. For this reason, the logging is specified at the end of the graph definition within the same module, even if its execution depend on the caller of the graph selecting the right stream adapter and calling the logger on it. Note that, if the stream adapter changes the state, this must be considered at the logger definition stage.
# Configuration

The file `config.toml` in the directory of the project contains the instructions to configure its function. Essentially all user-configurable options are specified in this file.

## Use of language models

Language models are specified through the sections `embeddings`, `major`, `minor`, and `aux`. 

Embeddings are used to store and retrieve text from vector databases. The embeddings that are used by the project are dense embeddings (classically used by RAG applications) and sparse embeddings.

Specification of dense embeddings contain an embedding provider and a model name, separated by '/':

    OpenAI/text-embedding-3-small
    SentenceTransformers/all-MiniLM-L6-v2
    SentenceTransformers/distiluse-base-multilingual-cased-v1"

Two sparse embeddings are supported at present.

    Qdrant/bm25
    prithivida/Splade_PP_en_v1

The default sparse embedding is Qdrant/bm25 as it is a multilingual embeddings.

It is possible to configure three different language models, used in different parts of the project.

| model | use |
| --- | --- |
| major | in direct interactions with the end user |
| minor | to form summaries, generate questions the text answers |
| aux | to classify text |

The language models are specified by separating the model provider and the model name by '/':

    OpenAI/gpt-4.1-mini
    OpenAI/gpt-4.1-nano
    Mistral/mistral-small-latest

It is also possible to specify other parameters of the model, such as temperature. See the instructions of the individual models for further details.

A special language model specification is Debug/debug, which avoids using a real language model.

## Server

In this section, the server port can be specified. The server serves the web protocol through which the RAG chat is offered.

## Storage

This section configures where the vector database is located. Specify a folder to save the vector database locally, or a valid internet address/port number for a Qdrant server.

## Annotation model

The annotation model specifies what properties of the markdown files are used for encoding. 'Encoding' is the process to generate a dense, sparse, or combined representation of a piece of text. The 'Annotation model' specifies what information goes into the encoding. It refers to metadata properties of nodes in the markdown files (individual blocks of text, or heading and the text below them). These metadata properties are filled in manually or automatically through the use of a language model.

The annotation model is used internally to record properties for encoding that are specified elsewhere. For example, if the RAG model includes creating questions that the text answer as a property generated by language models and then used for encoding, then the annotation model is updated automatically to reflect this fact. Therefore, there is no need to synchronize the model. However, manual access to the model allows one to include any possible property in the encoding, for example properties generated manually in the markdown text.

The annotation model is specified as a list of keys under `inherited_properties` and `own_properties`. These two types of specifications refer to how annotations are sought. When specified as inherited, the property is sought in the node and in all its ancenstor, until it is found (if present anywhere in the ancestor tree). When specified as own, only the node is sought.

The specification `filters` in the annotation model is reserved for future use. Setting this specification has no effect on the working of the program at present.

## Encoding models

## Content validation

Content validation can be implemented at two levels. The first is the prompt, instructing the language model only to reply to queries about a certain content, or that are represented in the retrieved context. The second is to send query and the first part of the response to a LMM to classify the content.

The default chat prompt includes text for the first level of validation. To switch on the second, you set the `check_response` property in config.toml to true:

```ini
[check_response]
check_response = true
allowed_content = ["statistics", "R programming"]
initial_buffer_size = 320
```

If you do that, you must also provide one or more allowed contents in the `allowed_content` list. The `initial_buffer_size` controls how much of the initial response of the model is sent for content classification (in characters). If the chatbot is replying that it cannot respond to legitimate queries due to misclassification, try and increase the buffer size. This is a tredeoff between the accuracy of content assessment and the latency with which the response starts streaming in the chatbot.

It is important to bear in mind that the instructions about the content of the interaction that are put in the prompt at the time in which the context is retrieved become progressively irrelevant during a chat exchange. Therefore, any strategy to control content cannot be based on verifying that the question is consistent with material retireved from the vector database. There are two ways to control the content of prolonged chats:

* introduce a content limitation in the system prompt, instead of leaving to something generic like "You are a helpful assistant". The system prompt is present in all chat interactions, irrespective of how prolonged.
* switch on content validation, which checks each response of the language model.


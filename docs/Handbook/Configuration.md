# Configuration

The file `config.toml` in the directory of the project contains the instructions to configure its function. Essentially all user-configurable options are specified in this file.

## Use of language models

Language models are specified through the sections `embeddings`, `major`, `minor`, and `aux`. 

Embeddings are used to store and retrieve text from vector databases. The embeddings that are used by the project are dense embeddings (classically used by RAG applications) and sparse embeddings.

Specification of dense embeddings contain an embedding provider and a model name, separated by '/':

    OpenAI/text-embedding-3-small
    SentenceTransformers/all-MiniLM-L6-v2
    SentenceTransformers/distiluse-base-multilingual-cased-v1"

Two sparse embeddings are supported at present.

    Qdrant/bm25
    prithivida/Splade_PP_en_v1

The default sparse embedding is Qdrant/bm25 as it is a multilingual embeddings.

It is possible to configure three different language models, used in different parts of the project.

| model | use |
| --- | --- |
| major | in direct interactions with the end user |
| minor | to form summaries, generate questions the text answers |
| aux | to classify text |

The language models are specified by separating the model provider and the model name by '/':

    OpenAI/gpt-4.1-mini
    OpenAI/gpt-4.1-nano
    Mistral/mistral-small-latest

It is also possible to specify other parameters of the model, such as temperature. See the instructions of the individual models for further details.

A special language model specification is Debug/debug, which avoids using a real language model.

## Server

In this section, the server port can be specified. The server serves the web protocol through which the RAG chat is offered.

## Storage

This section configures where the vector database is located. Specify a folder to save the vector database locally, or a valid internet address/port number for a Qdrant server.

## Annotation model

The annotation model specifies what properties of the markdown files are used for encoding. 'Encoding' is the process to generate a dense, sparse, or combined representation of a piece of text. The 'Annotation model' specifies what information goes into the encoding. It refers to metadata properties of nodes in the markdown files (individual blocks of text, or heading and the text below them). These metadata properties are filled in manually or automatically through the use of a language model.

The annotation model is used internally to record properties for encoding that are specified elsewhere. For example, if the RAG model includes creating questions that the text answer as a property generated by language models and then used for encoding, then the annotation model is updated automatically to reflect this fact. Therefore, there is no need to synchronize the model. However, manual access to the model allows one to include any possible property in the encoding, for example properties generated manually in the markdown text.

The annotation model is specified as a list of keys under `inherited_properties` and `own_properties`. These two types of specifications refer to how annotations are sought. When specified as inherited, the property is sought in the node and in all its ancenstor, until it is found (if present anywhere in the ancestor tree). When specified as own, only the node is sought.

The specification `filters` in the annotation model is reserved for future use. Setting this specification has no effect on the working of the program at present.

## Encoding models
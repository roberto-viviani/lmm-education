{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#lm-markdown-for-education","title":"LM markdown for education","text":"<p>LM markdown for education allows educators to develop lectures online, using language models for the interaction with students. It is a package developed at the Institute of Psychology at the University of Innsbruck.</p> <p>LM markdown is a package that provides the instruments to edit and prepare markdown files, including for RAG. Educators can use markdown as a pivot document to review and prepare the material that they intend to serve in their education class. The reason to choose markdown is its versatility: instruments already exist to transform markdown documents into articles, books, or slides. Here, it may additionally be transformed into virtual lectures and language-model mediated interactions.</p> <p>In addition, markdown is a simple format that allows the user to easily review and control its content. A key purpose of LM markdown for education is to give educators the instruments to exploit AI in their own teaching, as opposed to adopting an off-the-shelf package. When students interact with the chatbot produced by LM markdown for education, they interact with the material selected and prepared by the instructor. This means that educators remain the originators of the material that is used in teaching, but with the amplified capabilities given by the increased availability and interactivity of AI-based teaching.</p> <p>The workflow for LM markdown for education starts from the collection of the teaching material. This material could be lecture notes, recordings of lectures, or anything that the teacher may want to develop expressely for the purpose of creating an AI-supported lecture series. The textual component of this material is then transformed into markdown format for review. Finally, the markdown documents are transformed into other artefacts, such as online lectures, a supportive RAG system, or a combination of the two.</p> <p>Start here for the sintructions to install the package and links to a quickstart.</p>"},{"location":"API/appbase/","title":"AppBase","text":"<p>This is a module that centralizes set up for all app modules when the app start. All app modules are possible entry points of a specific application. By importing this module, set up takes place in a unified way across all apps.</p> <p>At import, this module does the following:</p> <ul> <li>It reads the name of the config files defined in config.config and config.appchat, and checks that they exist. If not, default config files are created and saved to disk.</li> <li>It reads in the config files. Validation errors are raised at this point if any entry in the config files is malformed or illegal. The config objects are exported as module variables <code>settings</code> and <code>chat_settings</code>.</li> <li>It creates a BaseRetriever object to read and write from the vector database. This object is exported as the module-level variable <code>retriever</code>, but the underlying database stream is managed globally.</li> <li>It creates an embedding object. This object is not used by the clients of the module, but the creation loads the embedding model, which is lengthy and takes place at best before the app starts serving possible clients.</li> </ul> Example <pre><code>try:\n    from lmm_education.appbase import appbase as base\nexcept Exception as e:\n    logger.error(f\"Could not set up app: {e}\")\n    exit()\n\nretriever = base.retriever\n</code></pre> <p>options:     show_root_heading: false</p>"},{"location":"API/apputils/","title":"App Utils","text":"<p>Utility functions for app modules</p> <p>options:     show_root_heading: false</p>"},{"location":"API/apputils/#lmm_education.apputils.preproc_markdown_factory","title":"<code>preproc_markdown_factory(model_settings)</code>","text":"<p>This function allows centralizing preprocessing of strings using a latex style.</p> Source code in <code>lmm_education/apputils.py</code> <pre><code>def preproc_markdown_factory(\n    model_settings: LanguageModelSettings,\n) -&gt; Callable[[str], str]:\n    \"\"\"This function allows centralizing preprocessing\n    of strings using a latex style.\"\"\"\n    from lmm.markdown.ioutils import (\n        convert_dollar_latex_delimiters,\n        convert_backslash_latex_delimiters,\n    )\n\n    model: str = model_settings.get_model_source()\n    return (\n        convert_backslash_latex_delimiters\n        if model == \"Mistral\"\n        else convert_dollar_latex_delimiters\n    )\n</code></pre>"},{"location":"API/background_task_manager/","title":"Background Task Manager","text":"<p>Defines a generic infrastructure to manage fire-and-forget background tasks.</p> <p>This module provides a mechanism to schedule asynchronous tasks that should run in the background without blocking the main execution flow. These tasks are tracked and automatically awaiting during application shutdown.</p> Example <pre><code>import asyncio\nfrom lmm_education.background_task_manager import schedule_task\n\nasync def background_job(data: str) -&gt; None:\n    await asyncio.sleep(1)\n    print(f\"Processed: {data}\")\n\n# Schedule the coroutine execution\nschedule_task(background_job(\"some data\"))\n</code></pre> <p>options:     show_root_heading: false</p>"},{"location":"API/background_task_manager/#lmm_education.background_task_manager.schedule_task","title":"<code>schedule_task(coro, error_callback=None)</code>","text":"<p>Schedules a fire-and-forget background task.</p> <p>The task is added to a module-level set to be awaited at application exit.</p> <p>Parameters:</p> Name Type Description Default <code>coro</code> <code>Coroutine[Any, Any, None]</code> <p>The coroutine to schedule.</p> required <code>error_callback</code> <code>Callable[[Exception], None] | None</code> <p>Optional callback to handle exceptions raised by the coroutine. Called immediately when the task fails with the exception as argument. If not provided, exceptions are only reported during shutdown.</p> <code>None</code> <p>Returns:</p> Type Description <code>Task[None]</code> <p>The scheduled asyncio Task.</p> Source code in <code>lmm_education/background_task_manager.py</code> <pre><code>def schedule_task(\n    coro: Coroutine[Any, Any, None],\n    error_callback: Callable[[Exception], None] | None = None,\n) -&gt; asyncio.Task[None]:\n    \"\"\"\n    Schedules a fire-and-forget background task.\n\n    The task is added to a module-level set to be awaited at application exit.\n\n    Args:\n        coro: The coroutine to schedule.\n        error_callback: Optional callback to handle exceptions raised by the\n            coroutine. Called immediately when the task fails with the\n            exception as argument. If not provided, exceptions are only\n            reported during shutdown.\n\n    Returns:\n        The scheduled asyncio Task.\n    \"\"\"\n    task: asyncio.Task[None] = asyncio.create_task(coro)\n    active_tasks.add(task)\n\n    def handle_completion(t: asyncio.Task[None]) -&gt; None:\n        \"\"\"Handle task completion, including exception handling.\"\"\"\n        try:\n            # This raises if the task failed with an exception\n            t.result()\n        except asyncio.CancelledError:\n            # Task was cancelled - this is normal, don't call error_callback\n            pass\n        except Exception as e:\n            # Task failed with an exception\n            if error_callback:\n                error_callback(e)\n        finally:\n            # Always remove from active tasks\n            active_tasks.discard(t)\n\n    task.add_done_callback(handle_completion)\n    return task\n</code></pre>"},{"location":"API/chat_agent/","title":"Agent","text":"<p>Agentic workflow for handling RAG interactions. The agent decides how to use the vector database to retrieve information to respond to the user.</p> <p>The workflow handles: - Query validation, including responding to confused queries - Integration of previous messages - Context retrieval from vector store - LLM response generation</p> <p>The graph workflow is created with the <code>create_chat_agent</code> function taking a ConfigSettings argument to specify major, minor, and aux models:</p> <pre><code>settings = ConfigSettings()\nworkflow = create_chat_agent(settings)\nstate: ChatState = create_initial_state(\"What is logistic regression?\")\ndependencies = ChatWorkflowContext.from_default_config()\nstream = workflow.astream(state, stream_mode=\"messages\",\n                context=dependencies)\n</code></pre> <p>Please note that the major model must support tool use. When using an OpenAI model, be sure that config.toml specifies the responses API:</p> <pre><code>[major.provider_params]\nuse_responses_api = true\n</code></pre> <p>After creation, the stream may be consumed directly. Note that this is a LangGraph stream, returning tuples of (chunk, metadata):</p> <pre><code>async for chunk, metadata in stream:\n    ...\n    print(chunk.text, sep=\"\", flush=True)\n</code></pre> <p>Use stream adapters from the stream_adapters module to set up and transform the stream. For example, to emit strings:</p> <pre><code>from lmm_education.models.langchain.workflows.stream_adapters import (\n    tier_2_to_3_adapter\n)\ntext_stream = tier_2_to_3_adapter(stream)\nasync for txt in text_stream:\n    print(txt, sep=\"\", flush=True)\n</code></pre> <p>The function graph_logger supports writing the user/llm interaction to a database, and is meant to be used by a streaming object.</p> Note <p>to support streaming, the graph does not provide the output in the <code>messages</code>, but in the <code>response</code> channel instead. When used with <code>.ainvoke()</code>, extract the response from this key in the returned state. When used with <code>.astream()</code>, consume the stream.</p> <p>options:     show_root_heading: false</p>"},{"location":"API/chat_agent/#lmm_education.workflows.langchain.chat_agent.create_chat_agent","title":"<code>create_chat_agent(settings, workflow_context, llm_major=None)</code>","text":"<p>Create the agentic chat workflow graph with native streaming support.</p> <p>The graph implements the following flow:</p> <p>START               ---&gt; validate query validate query      -.-&gt; END [empty query]                     -.-&gt; END [long query]                     -.-&gt; integrate history integrate history   -.-&gt; END [error in retrieval]                     -.-&gt; format query format query        ---&gt; generate generate            -x-&gt; tool_caller -x-&gt; check_tool_result                         check_tool_result -x-&gt; generate                     -x-&gt; END</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>ConfigSettings</code> <p>a ConfigSettings object (for settings.major, settings.minor, settings.aux)</p> required <code>llm_major</code> <code>BaseChatModel | None</code> <p>an override of settings.major (used to inject a model for testing purposes)</p> <code>None</code> <p>Returns:</p> Type Description <code>ChatStateGraphType</code> <p>Compiled StateGraph ready for streaming</p> Source code in <code>lmm_education/workflows/langchain/chat_agent.py</code> <pre><code>def create_chat_agent(\n    settings: ConfigSettings,\n    workflow_context: ChatWorkflowContext,\n    llm_major: BaseChatModel | None = None,\n) -&gt; ChatStateGraphType:\n    \"\"\"\n    Create the agentic chat workflow graph with native streaming\n    support.\n\n    The graph implements the following flow:\n\n    START               ---&gt; validate query\n    validate query      -.-&gt; END [empty query]\n                        -.-&gt; END [long query]\n                        -.-&gt; integrate history\n    integrate history   -.-&gt; END [error in retrieval]\n                        -.-&gt; format query\n    format query        ---&gt; generate\n    generate            -x-&gt; tool_caller -x-&gt; check_tool_result\n                            check_tool_result -x-&gt; generate\n                        -x-&gt; END\n\n    Args:\n        settings: a ConfigSettings object (for settings.major,\n            settings.minor, settings.aux)\n        llm_major: an override of settings.major (used to inject\n            a model for testing purposes)\n\n    Returns:\n        Compiled StateGraph ready for streaming\n    \"\"\"\n\n    # Shared nodes from nodes.py\n    integrate_history = create_integrate_history_node(settings)\n\n    def format_query(state: ChatState) -&gt; dict[str, str | AIMessage]:\n        \"\"\"Format the query with retrieved context using prompt\n        template.\"\"\"\n\n        query: str = state[\"query\"]\n\n        # Format with prompt template\n        template = PromptTemplate.from_template(\n            \"\"\"Please assist students by responding to their QUERY by \nusing the context obtained by searching the database. If such context \ndoes not provide information for your answer, integrate the context \nonly for the use and syntax of R. Otherwise, reply that you do not \nhave information to answer the query, as the course focuses on linear \nmodels and their use in R.\n\n####\nQUERY: \"{query}\"\n\"\"\"\n        )\n        formatted_query: str = template.format(\n            query=query,\n        )\n\n        return {\"refined_query\": formatted_query}\n\n    @tool(\n        \"search_database\",\n        description=(\n            \"Searches the vector database to retrieve context \"\n            \"information to answer the user's question.\\n\"\n            \"Args:\\n\"\n            \"   query: str. A query text to be matched in the \"\n            \"vector database.\"\n        ),\n    )\n    async def retrieve_context(\n        query: str,\n        runtime: ToolRuntime[ChatWorkflowContext, ChatState],\n    ) -&gt; str:\n        config: ChatWorkflowContext = runtime.context\n\n        try:\n            documents: list[Document] = (\n                await config.retriever.ainvoke(query)\n            )\n            context: str = \"\\n-----\\n\".join(\n                [d.page_content for d in documents]\n            )\n\n            return context\n\n        except Exception as e:\n            config.logger.error(\n                f\"Error retrieving from vector database:\\n{e}\"\n            )\n            raise RuntimeError(\n                \"Error retrieving from vector database\"\n            ) from e\n\n    tools: list[BaseTool] = [retrieve_context]\n\n    # the type of the model bound to tools is complex\n    base_model: BaseChatModel = (\n        llm_major or create_model_from_settings(settings.major)\n    )\n    model_with_tools: Runnable[\n        Sequence[BaseMessage],\n        AIMessage,\n    ] = base_model.bind_tools(tools)\n\n    def check_tool_result(\n        state: ChatState, runtime: Runtime[ChatWorkflowContext]\n    ) -&gt; dict[str, str | AIMessage | float]:\n        \"\"\"Check if the tool execution resulted in an error.\n\n        When handle_tool_errors=True, ToolNode catches exceptions\n        and creates a ToolMessage with the error text in .content\n        starting with 'Error:'.\n        \"\"\"\n        from langchain_core.messages import ToolMessage\n\n        messages = state.get(\"messages\", [])\n        if not messages:\n            return {}\n\n        last_message: BaseMessage = messages[-1]\n\n        # Check if last message is a ToolMessage with error content\n        if isinstance(last_message, ToolMessage):\n            # ToolNode with handle_tool_errors=True creates error messages\n            # with content starting with \"Error:\"\n            if (\n                last_message.content\n                and last_message.content.startswith(\"Error\")\n            ):\n                settings = runtime.context.chat_settings\n                runtime.context.logger.error(\n                    f\"Tool execution failed: {last_message.content}\"\n                )\n                return {\n                    'status': \"error\",\n                    'response': settings.MSG_ERROR_QUERY,\n                    'messages': AIMessage(\n                        content=settings.MSG_ERROR_QUERY\n                    ),\n                }\n\n        latency: timedelta = datetime.now() - state['timestamp']\n        return {\n            'status': \"valid\",\n            'context': str(last_message.content),\n            'time_to_context': latency.total_seconds(),\n            'tool_call_count': state.get('tool_call_count', 0) + 1,\n        }  # Tool succeeded\n\n    async def generate(\n        state: ChatState, runtime: Runtime[ChatWorkflowContext]\n    ) -&gt; dict[str, str | AIMessage | float]:\n        # Call the tool calling model\n\n        system_message: str = (\n            runtime.context.chat_settings.SYSTEM_MESSAGE\n            + \"\\n\"\n            + \"You use a searchable database to obtain the context \"\n            \"on which you base your responses. To respond to users,\"\n            \"base your actions on the following steps:\\n\"\n            \"1. determine if the QUERY of the user makes sense and is\"\n            \"intelligible. If so, go to the next step. If not, respond \"\n            \"immediately inviting the user to clarify their intended\"\n            \" meaning.\\n\"\n            \"2. Search the vector database using the user's QUERY as \"\n            \"argument to obtain material to answer it. If the QUERY \"\n            \"includes different concepts or entities, you may \"\n            \"optionally search the database by rewriting the query \"\n            \"and splitting it into multiple queries, to be used in \"\n            \"separate search database calls.\\n\"\n            \"3. Answer the query as specified in the user message.\"\n        )\n        messages = prepare_messages_for_llm(\n            state, runtime.context, system_message=system_message\n        )\n\n        response_chunks: list[str] = []\n        response_message = AIMessageChunk(\n            content=\"\", type=\"AIMessageChunk\"\n        )\n        try:\n            # While declared as AIMessage, astream may return\n            # AIMessageChunk. At any rate, the .text member is\n            # inherited from AIMessage, but __add__ gives\n            # AIMessageChunk's, not ChatPromptTemplate objects.\n            chunk: AIMessageChunk\n            async for chunk in model_with_tools.astream(messages):  # type: ignore\n                # Extract text from AIMessageChunk (from AIMessage).\n                # https://docs.langchain.com/oss/python/langchain/messages#attributes\n                if hasattr(chunk, \"text\") and chunk.text:\n                    response_chunks.append(chunk.text)\n\n                response_message = response_message + chunk\n\n        except Exception as e:\n            context: ChatWorkflowContext = runtime.context\n            context.logger.error(\n                f\"Error while streaming in 'generate' node: {e}\"\n            )\n            return {\n                'status': \"error\",\n                'response': context.chat_settings.MSG_ERROR_QUERY,\n                'messages': AIMessage(\n                    content=context.chat_settings.MSG_ERROR_QUERY\n                ),\n            }\n\n        latency: timedelta = datetime.now() - state['timestamp']\n        if response_message.tool_calls:\n            return {'messages': response_message}\n        else:\n            # Store complete response in state\n            return {\n                'model_identification': settings.major.model,\n                'response': \"\".join(response_chunks),\n                'time_to_response': latency.total_seconds(),\n            }\n\n    # Build the graph------------------------------------------------\n    workflow: StateGraph[\n        ChatState, ChatWorkflowContext, ChatState, ChatState\n    ] = StateGraph(ChatState, ChatWorkflowContext)\n\n    generate_fallback = create_generate_node(settings, llm_major)\n\n    # Add nodes\n    max_tool_retries = workflow_context.chat_settings.max_tool_retries\n    workflow.add_node(\"validate_query\", validate_query)\n    workflow.add_node(\"integrate_history\", integrate_history)\n    workflow.add_node(\"format_query\", format_query)\n    workflow.add_node(\n        \"tool_caller\",\n        ToolNode(tools, name=\"tool_caller\", handle_tool_errors=True),\n    )\n    workflow.add_node(\"check_tool_result\", check_tool_result)\n    workflow.add_node(\n        \"generate\",\n        generate,\n        # automatically retry network issues and rate limits\n        retry_policy=RetryPolicy(\n            max_attempts=max_tool_retries, initial_interval=1.0\n        ),\n    )\n    workflow.add_node(\n        \"generate_fallback\",\n        generate_fallback,\n    )\n\n    # Add edges\n    workflow.add_edge(START, \"validate_query\")\n    workflow.add_conditional_edges(\n        \"validate_query\",\n        continue_if_valid(\"integrate_history\"),\n    )\n    workflow.add_conditional_edges(\n        \"integrate_history\",\n        continue_if_no_error(\"format_query\"),\n    )\n    workflow.add_edge(\"format_query\", \"generate\")\n    workflow.add_conditional_edges(\n        \"generate\", continue_after_tool_call(\"tool_caller\", END)\n    )\n    workflow.add_edge(\"tool_caller\", \"check_tool_result\")\n    workflow.add_conditional_edges(\n        \"check_tool_result\",\n        continue_to_generate_or_fallback(\n            \"generate\", \"generate_fallback\", max_tool_calls=max_tool_retries,\n        ),\n    )\n    workflow.add_edge(\"generate_fallback\", END)\n\n    return workflow.compile()\n</code></pre>"},{"location":"API/chat_stream_adapters/","title":"Chat stream adapters","text":"<p>Stream adapter for validating a chat_graph stream. This module contains domain-specific adapters - it uses ChatState directly and understands the semantics of the \"status\" field.</p> <p>options:     show_root_heading: false</p>"},{"location":"API/chat_stream_adapters/#lmm_education.workflows.langchain.chat_stream_adapters.stateful_validation_adapter","title":"<code>stateful_validation_adapter(multi_mode_stream, *, validator_model, allowed_content, source_nodes=None, buffer_size=320, error_message='Content not allowed', max_retries=2, continue_on_fail=False, logger=ConsoleLogger())</code>  <code>async</code>","text":"<p>Multi-mode adapter that validates message content.</p> <p>This is a Tier 1 (state-aware) adapter that: - Buffers message chunks until sufficient content is collected - Validates content using a separate LLM - If validation fails:   * Yields a rejection message   * Modifies ChatState.status to \"rejected\"   * Stops streaming - If validation passes:   * Releases buffered content   * Continues streaming normally - If validation is unavailable (network/service failure):   * Behavior depends on continue_on_fail parameter   * If continue_on_fail=True: allows content through with warning (fail-open)   * If continue_on_fail=False: rejects content for safety (fail-closed)</p> <p>Parameters:</p> Name Type Description Default <code>multi_mode_stream</code> <code>tier_1_iterator</code> <p>Source stream yielding (mode, event) tuples</p> required <code>validator_model</code> <code>RunnableType</code> <p>Runnable for content classification</p> required <code>allowed_content</code> <code>list[str]</code> <p>List of allowed content categories</p> required <code>source_nodes</code> <code>list[str] | None</code> <p>List of graph node names whose messages should be validated. If None, all message streams are validated. Use this to exclude error messages from nodes like 'validate_query' and only validate LLM-generated content from nodes like 'generate'.</p> <code>None</code> <code>buffer_size</code> <code>int</code> <p>Characters to buffer before validation</p> <code>320</code> <code>error_message</code> <code>str</code> <p>Message to yield if validation fails</p> <code>'Content not allowed'</code> <code>max_retries</code> <code>int</code> <p>Validation retry attempts before marking unavailable</p> <code>2</code> <code>continue_on_fail</code> <code>bool</code> <p>If True, content is allowed when validation fails (fail-open). If False, content is rejected when validation fails (fail-closed). Defaults to False for safety.</p> <code>False</code> <code>logger</code> <code>LoggerBase</code> <p>Logger for diagnostics</p> <code>ConsoleLogger()</code> <p>Yields:</p> Type Description <code>tier_1_iterator</code> <p>(mode, event) tuples with potentially modified state</p> Note <p>This adapter works with streams that emit 'messages' events only, or combined 'messages' + 'values' events. If no 'values' events are emitted, the adapter will yield a final state containing validation metadata merged with an empty initial state.</p> Source code in <code>lmm_education/workflows/langchain/chat_stream_adapters.py</code> <pre><code>async def stateful_validation_adapter(\n    multi_mode_stream: tier_1_iterator,\n    *,\n    validator_model: RunnableType,\n    allowed_content: list[str],\n    source_nodes: list[str] | None = None,\n    buffer_size: int = 320,\n    error_message: str = \"Content not allowed\",\n    max_retries: int = 2,\n    continue_on_fail: bool = False,\n    logger: LoggerBase = ConsoleLogger(),\n) -&gt; tier_1_iterator:\n    \"\"\"\n    Multi-mode adapter that validates message content.\n\n    This is a Tier 1 (state-aware) adapter that:\n    - Buffers message chunks until sufficient content is collected\n    - Validates content using a separate LLM\n    - If validation fails:\n      * Yields a rejection message\n      * Modifies ChatState.status to \"rejected\"\n      * Stops streaming\n    - If validation passes:\n      * Releases buffered content\n      * Continues streaming normally\n    - If validation is unavailable (network/service failure):\n      * Behavior depends on continue_on_fail parameter\n      * If continue_on_fail=True: allows content through with warning (fail-open)\n      * If continue_on_fail=False: rejects content for safety (fail-closed)\n\n    Args:\n        multi_mode_stream: Source stream yielding (mode, event) tuples\n        validator_model: Runnable for content classification\n        allowed_content: List of allowed content categories\n        source_nodes: List of graph node names whose messages should be\n            validated. If None, all message streams are validated. Use\n            this to exclude error messages from nodes like 'validate_query'\n            and only validate LLM-generated content from nodes like 'generate'.\n        buffer_size: Characters to buffer before validation\n        error_message: Message to yield if validation fails\n        max_retries: Validation retry attempts before marking unavailable\n        continue_on_fail: If True, content is allowed when validation fails\n            (fail-open). If False, content is rejected when validation fails\n            (fail-closed). Defaults to False for safety.\n        logger: Logger for diagnostics\n\n    Yields:\n        (mode, event) tuples with potentially modified state\n\n    Note:\n        This adapter works with streams that emit 'messages' events only,\n        or combined 'messages' + 'values' events. If no 'values' events are\n        emitted, the adapter will yield a final state containing validation\n        metadata merged with an empty initial state.\n    \"\"\"\n    buffer_chunks: list[tuple[str, Any]] = []\n    buffer_text: str = \"\"\n    validation_complete: bool = False\n    # captured in 'message' stream\n    metadata: dict[str, Any] = {}\n    # captured in 'values' stream (state from graph)\n    captured_state: ChatState = create_initial_state(\"\")\n    # state values changes by validation\n    modified_state: dict[str, str | float] = {}\n\n    class ValidationResult(TypedDict):\n        status: ValidationStatus\n        classification: str\n        reason: str | None\n\n    async def _validate_content(content: str) -&gt; ValidationResult:\n        \"\"\"Validate content with retry logic.\"\"\"\n        for attempt in range(max_retries + 1):\n            try:\n                classification: str = await validator_model.ainvoke(\n                    {\"text\": content}\n                )\n                logger.info(\n                    \"Model content classification: \"\n                    + classification.replace(\"\\n\", \" \")\n                )\n\n                if classification in allowed_content + [\n                    \"apology\",\n                    \"human interaction\",\n                ]:\n                    return ValidationResult(\n                        status=\"valid\",\n                        classification=classification,\n                        reason=None,\n                    )\n                else:\n                    return ValidationResult(\n                        status=\"invalid\",\n                        classification=classification,\n                        reason=f\"Classification '{classification}' not in allowed list\",\n                    )\n\n            except Exception as e:\n                logger.warning(\n                    f\"Content check attempt {attempt + 1}/\"\n                    f\"{max_retries + 1} failed: {e}\"\n                )\n\n                if attempt == max_retries:\n                    logger.error(\n                        f\"Content checker failed after \"\n                        f\"{max_retries + 1} attempts: {e}\"\n                    )\n                    return ValidationResult(\n                        status=\"unavailable\",\n                        classification=\"\",\n                        reason=str(e),\n                    )\n\n                await asyncio.sleep(0.5 * (attempt + 1))\n\n        return ValidationResult(\n            status=\"error\",\n            classification=\"\",\n            reason=\"Unexpected: exhausted retries without returning\",\n        )\n\n    async for mode, event in multi_mode_stream:\n        match mode:\n            case \"values\":\n                # Track latest state for potential modification\n                captured_state = event\n                yield (mode, event)\n\n            case \"messages\" if not validation_complete:\n                # Validation logic for streamed data in the messages mode\n                chunk, metadata = event\n\n                # Filter by source node if specified\n                if source_nodes is not None:\n                    node_name = metadata.get(\"langgraph_node\", \"\")\n                    if node_name not in source_nodes:\n                        # Pass through without validation\n                        yield (mode, event)\n                        continue\n\n                buffer_chunks.append((mode, event))\n                buffer_text += chunk.text\n\n                if len(buffer_text) &gt;= buffer_size:\n                    # Validate buffered content\n                    query: str = (\n                        captured_state['query']\n                        if captured_state\n                        else \"\"\n                    )\n                    validation_result = await _validate_content(\n                        query + \"\\n\\n\" + buffer_text + \"...\"\n                    )\n                    validation_complete = True\n\n                    match validation_result[\"status\"]:\n                        case \"valid\":\n                            # Validation passed - release buffer\n                            latency: timedelta = (\n                                datetime.now()\n                                - captured_state['timestamp']\n                            )\n                            modified_state = {\n                                \"query_classification\": validation_result[\n                                    \"classification\"\n                                ],\n                                \"time_to_FB\": latency.total_seconds(),\n                            }\n                            # Yield buffered content\n                            for (\n                                buffered_mode,\n                                buffered_event,\n                            ) in buffer_chunks:\n                                yield (buffered_mode, buffered_event)\n\n                        case \"invalid\":\n                            # Validation failed - reject and stop\n                            logger.info(\n                                f\"Content rejected with classification: \"\n                                f\"{validation_result['classification']}\"\n                            )\n\n                            # Yield rejection message\n                            yield (\n                                \"messages\",\n                                (\n                                    AIMessageChunk(\n                                        content=error_message\n                                    ),\n                                    metadata,\n                                ),\n                            )\n\n                            # Save changed state values to reflect rejection\n                            modified_state = {\n                                \"model_identification\": validator_model.get_name()\n                                or \"unknown validator\",\n                                \"response\": buffer_text,\n                                \"query_classification\": validation_result[\n                                    \"classification\"\n                                ],\n                                \"status\": \"rejected\",\n                            }\n                            yield (\n                                \"values\",\n                                {**captured_state, **modified_state},\n                            )\n\n                            return  # Stop streaming\n\n                        case \"unavailable\" | \"error\":\n                            # Validator failed\n                            if continue_on_fail:\n                                # Fail-open: allow content through\n                                logger.warning(\n                                    f\"Validation {validation_result['status']}: \"\n                                    f\"{validation_result['reason']}. \"\n                                    f\"Continuing without validation (continue_on_fail=True)\"\n                                )\n                                modified_state = {\n                                    \"model_identification\": validator_model.get_name()\n                                    + (\" (unavailable)\"),\n                                    \"query_classification\": f\"&lt;validation {validation_result['status']}&gt;\",\n                                }\n                                # Yield buffered content\n                                for (\n                                    buffered_mode,\n                                    buffered_event,\n                                ) in buffer_chunks:\n                                    yield (\n                                        buffered_mode,\n                                        buffered_event,\n                                    )\n                            else:\n                                # Fail-closed: reject for safety\n                                logger.error(\n                                    f\"Validation {validation_result['status']}: \"\n                                    f\"{validation_result['reason']}. \"\n                                    f\"Rejecting content (continue_on_fail=False)\"\n                                )\n                                # Yield rejection message\n                                yield (\n                                    \"messages\",\n                                    (\n                                        AIMessageChunk(\n                                            content=error_message\n                                        ),\n                                        metadata,\n                                    ),\n                                )\n                                # Save changed state\n                                modified_state = {\n                                    \"model_identification\": validator_model.get_name()\n                                    + \" (failure)\",\n                                    \"response\": buffer_text,\n                                    \"query_classification\": f\"&lt;validation {validation_result['status']}&gt;\",\n                                    \"status\": \"rejected\",\n                                }\n                                yield (\n                                    \"values\",\n                                    {\n                                        **captured_state,\n                                        **modified_state,\n                                    },\n                                )\n                                return  # Stop streaming\n\n            case _:\n                # Pass through non-message events or post-validation messages\n                yield (mode, event)\n    # end async for mode, event\n\n    # Handle case where stream ended before buffer was full\n    if not validation_complete and buffer_text:\n        query_text: str = (\n            captured_state['query'] if captured_state else \"\"\n        )\n        validation_result = await _validate_content(\n            query_text + \"\\n\\n\" + buffer_text\n        )\n\n        match validation_result[\"status\"]:\n            case \"valid\":\n                # Validation passed\n                modified_state = {\n                    \"query_classification\": validation_result[\n                        \"classification\"\n                    ],\n                }\n                # Yield buffered chunks\n                for buffered_mode, buffered_event in buffer_chunks:\n                    yield (buffered_mode, buffered_event)\n\n            case \"invalid\":\n                # Validation failed\n                logger.info(\n                    f\"Content rejected with classification: \"\n                    f\"{validation_result['classification']}\"\n                )\n                # the metadata will be the last captured\n                if not metadata:\n                    # this is unreachable because buffer_text was tested\n                    # as not empty, and buffer_text can fill only if chunks\n                    # are read in with the metadata\n                    raise ValueError(\n                        \"Unreachable code reached: empty metadata \"\n                        \"in stateful_validation_adapter\"\n                    )\n                yield (\n                    \"messages\",\n                    (AIMessageChunk(content=error_message), metadata),\n                )\n\n                modified_state = {\n                    \"model_identification\": validator_model.get_name(),\n                    \"status\": \"rejected\",\n                    \"response\": buffer_text,\n                    \"query_classification\": validation_result[\n                        \"classification\"\n                    ],\n                }\n                yield (\"values\", {**captured_state, **modified_state})\n                return\n\n            case \"unavailable\" | \"error\":\n                # Validator failed\n                if continue_on_fail:\n                    # Fail-open\n                    logger.warning(\n                        f\"Validation {validation_result['status']}: \"\n                        f\"{validation_result['reason']}. \"\n                        f\"Continuing without validation (continue_on_fail=True)\"\n                    )\n                    modified_state = {\n                        \"model_identification\": validator_model.get_name()\n                        + \" (unavailable)\",\n                        \"query_classification\": f\"&lt;validation {validation_result['status']}&gt;\",\n                    }\n                    # Yield buffered chunks\n                    for (\n                        buffered_mode,\n                        buffered_event,\n                    ) in buffer_chunks:\n                        yield (buffered_mode, buffered_event)\n                else:\n                    # Fail-closed\n                    logger.error(\n                        f\"Validation {validation_result['status']}: \"\n                        f\"{validation_result['reason']}. \"\n                        f\"Rejecting content (continue_on_fail=False)\"\n                    )\n                    if not metadata:\n                        raise ValueError(\n                            \"Unreachable code reached: empty metadata \"\n                            \"in stateful_validation_adapter\"\n                        )\n                    yield (\n                        \"messages\",\n                        (\n                            AIMessageChunk(content=error_message),\n                            metadata,\n                        ),\n                    )\n                    modified_state = {\n                        \"model_identification\": validator_model.get_name()\n                        + (\" (failure)\"),\n                        \"status\": \"rejected\",\n                        \"response\": buffer_text,\n                        \"query_classification\": f\"&lt;validation {validation_result['status']}&gt;\",\n                    }\n                    yield (\n                        \"values\",\n                        {**captured_state, **modified_state},\n                    )\n                    return\n\n    # Yield final state to override previous \"values\"\n    if modified_state:\n        yield (\"values\", {**captured_state, **modified_state})\n    else:\n        logger.warning(\"validation adapter: modified state not set\")\n</code></pre>"},{"location":"API/chat_workflow/","title":"Workflow","text":"<p>Default workflow for handling RAG interactions.</p> <p>The workflow handles: - Query validation - Integration of previous messages - Context retrieval from vector store - LLM response generation</p> <p>The graph workflow is created with the <code>create_chat_workflow</code> function taking a ConfigSettings argument to specify major, minor, and aux models:</p> <pre><code>settings = ConfigSettings()\nworkflow = create_chat_workflow(settings)\nstate: ChatState = create_initial_state(\"What is logistic regression?\")\ndependencies = ChatWorkflowContext.from_default_config()\nstream = workflow.astream(state, stream_mode=\"messages\",\n                context=dependencies)\n</code></pre> <p>After this, the stream may be consumed directly. Note that this is a LangGraph stream, returning tuples of (chunk, metadata):</p> <pre><code>async for chunk, metadata in stream:\n    ...\n    print(chunk.text, sep=\"\", flush=True)\n</code></pre> <p>Use stream adapters from the stream_adapters module to set up and transform the stream. For example, to emit strings:</p> <pre><code>from lmm_education.models.langchain.workflows.stream_adapters import (\n    tier_2_to_3_adapter\n)\ntext_stream = tier_2_to_3_adapter(stream)\nasync for txt in text_stream:\n    print(txt, sep=\"\", flush=True)\n</code></pre> <p>The function graph_logger supports writing the user/llm interaction to a database, and is meant to be used by a streaming object.</p> Note <p>to support streaming, the graph does not provide the output in the <code>messages</code>, but in the <code>response</code> channel instead. When used with <code>.ainvoke()</code>, extract the response from this key in the returned state. When used with <code>.astream()</code>, consume the stream.</p> <p>options:     show_root_heading: false</p>"},{"location":"API/chat_workflow/#lmm_education.workflows.langchain.chat_graph.create_chat_workflow","title":"<code>create_chat_workflow(settings, llm_major=None)</code>","text":"<p>Create the chat workflow graph with native streaming support.</p> <p>The graph implements the following flow:</p> <p>START               ---&gt; validate query validate query      -.-&gt; END [empty query]                     -.-&gt; END [long query]                     -.-&gt; integrate history integrate history   -.-&gt; END [error in retrieval]                     -.-&gt; retrieve context retrieve context    ---&gt; format query format query        ---&gt; generate generate            ---&gt; END</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>ConfigSettings</code> <p>a ConfigSettings object (for settings.major, settings.minor, settings.aux)</p> required <code>llm_major</code> <code>BaseChatModel | None</code> <p>an override of settings.major (used to inject a model for testing purposes)</p> <code>None</code> <p>Returns:</p> Type Description <code>ChatStateGraphType</code> <p>Compiled StateGraph ready for streaming</p> Source code in <code>lmm_education/workflows/langchain/chat_graph.py</code> <pre><code>def create_chat_workflow(\n    settings: ConfigSettings,\n    llm_major: BaseChatModel | None = None,\n) -&gt; ChatStateGraphType:\n    \"\"\"\n    Create the chat workflow graph with native streaming support.\n\n    The graph implements the following flow:\n\n    START               ---&gt; validate query\n    validate query      -.-&gt; END [empty query]\n                        -.-&gt; END [long query]\n                        -.-&gt; integrate history\n    integrate history   -.-&gt; END [error in retrieval]\n                        -.-&gt; retrieve context\n    retrieve context    ---&gt; format query\n    format query        ---&gt; generate\n    generate            ---&gt; END\n\n    Args:\n        settings: a ConfigSettings object (for settings.major,\n            settings.minor, settings.aux)\n        llm_major: an override of settings.major (used to inject\n            a model for testing purposes)\n\n    Returns:\n        Compiled StateGraph ready for streaming\n    \"\"\"\n\n    # Shared nodes from nodes.py\n    integrate_history = create_integrate_history_node(settings)\n\n    async def retrieve_context(\n        state: ChatState, runtime: Runtime[ChatWorkflowContext]\n    ) -&gt; dict[str, str | AIMessage | float]:\n        \"\"\"Retrieve relevant documents from vector store.\"\"\"\n\n        query: str = state[\"refined_query\"]\n        config: ChatWorkflowContext = runtime.context\n\n        try:\n            documents: list[Document] = (\n                await config.retriever.ainvoke(query)\n            )\n            context: str = \"\\n-----\\n\".join(\n                [d.page_content for d in documents]\n            )\n\n            # TODO: include doc id in logging (revise db schema)\n            # Store document metadata for logging\n            # doc_metadata: list[dict[str, Any]] = [\n            #     {\n            #         \"content\": (\n            #             d.page_content[:200] + \"...\"\n            #             if len(d.page_content) &gt; 200\n            #             else d.page_content\n            #         ),\n            #         \"metadata\": d.metadata,\n            #     }\n            #     for d in documents\n            # ]\n\n            latency: timedelta = datetime.now() - state['timestamp']\n            return {\n                \"context\": context,\n                \"time_to_context\": latency.total_seconds(),\n            }\n\n        except Exception as e:\n            config.logger.error(\n                f\"Error retrieving from vector database:\\n{e}\"\n            )\n            settings: ChatSettings = config.chat_settings\n            return {\n                \"status\": \"error\",\n                \"response\": settings.MSG_ERROR_QUERY,\n                \"messages\": AIMessage(\n                    content=settings.MSG_ERROR_QUERY\n                ),\n            }\n\n    def format_query(\n        state: ChatState, runtime: Runtime[ChatWorkflowContext]\n    ) -&gt; dict[str, str | AIMessage]:\n        \"\"\"Format the query with retrieved context using prompt\n        template.\"\"\"\n\n        context: str = state.get(\"context\", \"\")\n        query: str = state[\"query\"]\n\n        settings: ChatSettings = runtime.context.chat_settings\n\n        # Convert LaTeX delimiters for display\n        formatted_context = convert_backslash_latex_delimiters(\n            context\n        )\n\n        # Format with prompt template\n        template = PromptTemplate.from_template(\n            settings.PROMPT_TEMPLATE\n        )\n        formatted_query: str = template.format(\n            context=formatted_context,\n            query=query,\n        )\n\n        return {\"refined_query\": formatted_query}\n\n    # Extract the shared generate factory\n    generate = create_generate_node(settings, llm_major)\n\n    # Build the graph------------------------------------------------\n    workflow: StateGraph[\n        ChatState, ChatWorkflowContext, ChatState, ChatState\n    ] = StateGraph(ChatState, ChatWorkflowContext)\n\n    # Add nodes\n    workflow.add_node(\"validate_query\", validate_query)\n    workflow.add_node(\"integrate_history\", integrate_history)\n    workflow.add_node(\"retrieve_context\", retrieve_context)\n    workflow.add_node(\"format_query\", format_query)\n    workflow.add_node(\n        \"generate\",\n        generate,\n        # automatically retry network issues and rate limits\n        retry_policy=RetryPolicy(\n            max_attempts=3, initial_interval=1.0\n        ),\n    )\n\n    # Add edges\n    workflow.add_edge(START, \"validate_query\")\n    workflow.add_conditional_edges(\n        \"validate_query\",\n        continue_if_valid(\"integrate_history\"),\n    )\n    workflow.add_conditional_edges(\n        \"integrate_history\",\n        continue_if_no_error(\"retrieve_context\"),\n    )\n    workflow.add_conditional_edges(\n        \"retrieve_context\", continue_if_no_error(\"format_query\")\n    )\n    workflow.add_edge(\"format_query\", \"generate\")\n    workflow.add_edge(\"generate\", END)\n\n    return workflow.compile()\n</code></pre>"},{"location":"API/cli_lmme/","title":"LMME","text":"<p>CLI interface for LM Markdown for Education</p> <p>options:     show_root_heading: false</p>"},{"location":"API/cli_lmme/#lmm_education.lmme.config_info","title":"<code>config_info()</code>","text":"<p>Returns information on the active configuration.</p> Source code in <code>lmm_education/lmme.py</code> <pre><code>@app.command()\ndef config_info() -&gt; None:\n    \"\"\"\n    Returns information on the active configuration.\n    \"\"\"\n    from lmm_education.config.config import (\n        load_settings,\n        serialize_settings,\n        ConfigSettings,\n    )\n\n    try:\n        settings: ConfigSettings | None = load_settings()\n        if settings is None:\n            print(\"ERROR: config.toml could not be loaded\")  # type: ignore\n            raise typer.Exit(1)\n        info: str = serialize_settings(settings)\n        print(info)  # type: ignore\n    except Exception as e:\n        logger.error(str(e))\n        raise typer.Exit(1)\n</code></pre>"},{"location":"API/cli_lmme/#lmm_education.lmme.create_default_config_file","title":"<code>create_default_config_file()</code>","text":"<p>Create default configuration files, or reset the configuration files to default values.</p> Source code in <code>lmm_education/lmme.py</code> <pre><code>@app.command()\ndef create_default_config_file() -&gt; None:\n    \"\"\"\n    Create default configuration files, or reset the configuration\n    files to default values.\n    \"\"\"\n    from lmm_education.config.config import (\n        create_default_config_file as create_config_file,\n    )\n    from lmm_education.config.appchat import (\n        create_default_config_file as create_appchat_file,\n    )\n\n    try:\n        create_config_file()\n        create_appchat_file()\n    except Exception as e:\n        logger.error(str(e))\n        raise typer.Exit(1)\n</code></pre>"},{"location":"API/cli_lmme/#lmm_education.lmme.database_info","title":"<code>database_info()</code>","text":"<p>Returns information on the database schema.</p> Source code in <code>lmm_education/lmme.py</code> <pre><code>@app.command()\ndef database_info() -&gt; None:\n    \"\"\"\n    Returns information on the database schema.\n    \"\"\"\n    from lmm_education.stores.vector_store_qdrant_utils import (\n        database_info,\n    )\n\n    try:\n        from rich import print\n    except Exception:\n        pass\n\n    try:\n        info: dict[str, object] = database_info()\n        print(info)  # type: ignore\n    except Exception as e:\n        logger.error(str(e))\n        raise typer.Exit(1)\n</code></pre>"},{"location":"API/cli_lmme/#lmm_education.lmme.ingest","title":"<code>ingest(sourcefile=typer.Argument(..., help='Markdown file to process'), save_files=typer.Option(False, '--save-files', '-s', help='Save processed blocks to files'), skip_ingest=typer.Option(False, '--skip-ingest', '-si', help='Ingest documents into the vector database'))</code>","text":"<p>Ingests a markdown file into the vector database, after processing  it as required by the encoding model.</p> Source code in <code>lmm_education/lmme.py</code> <pre><code>@app.command()\ndef ingest(\n    sourcefile: str = typer.Argument(\n        ..., help=\"Markdown file to process\"\n    ),\n    save_files: bool = typer.Option(\n        False,\n        \"--save-files\",\n        \"-s\",\n        help=\"Save processed blocks to files\",\n    ),\n    skip_ingest: bool = typer.Option(\n        False,\n        \"--skip-ingest\",\n        \"-si\",\n        help=\"Ingest documents into the vector database\",\n    ),\n) -&gt; None:\n    \"\"\"\n    Ingests a markdown file into the vector database, after processing\n     it as required by the encoding model.\n    \"\"\"\n    from lmm_education.ingest import markdown_upload\n    from lmm_education.config.config import ConfigSettings\n\n    try:\n        settings = ConfigSettings()\n        markdown_upload(\n            sourcefile,\n            config_opts=settings,\n            save_files=save_files,\n            ingest=not skip_ingest,\n            logger=logger,\n        )\n    except Exception as e:\n        logger.error(str(e))\n        raise typer.Exit(1)\n\n    # this is a workaround for the qdrant bug when clients\n    # are closed during garbage collection. We close all\n    # clients explicitly prior to exiting when this routine\n    # is called from the CLI, i.e. here Python is exiting.\n    if hasattr(sys, 'ps1'):\n        return\n    else:\n        from lmm_education.stores.vector_store_qdrant_context import (\n            qdrant_clients,\n        )\n\n        qdrant_clients.clear()\n</code></pre>"},{"location":"API/cli_lmme/#lmm_education.lmme.ingest_folder","title":"<code>ingest_folder(folder=typer.Argument(..., help='Folder to process'), extensions=typer.Option('.md;.Rmd', '--extensions', '-e', help='Extensions to process'))</code>","text":"<p>Ingests a folder into the vector database, after processing the documents as required by the encoding model.</p> Source code in <code>lmm_education/lmme.py</code> <pre><code>@app.command()\ndef ingest_folder(\n    folder: str = typer.Argument(..., help=\"Folder to process\"),\n    extensions: str = typer.Option(\n        \".md;.Rmd\",\n        \"--extensions\",\n        \"-e\",\n        help=\"Extensions to process\",\n    ),\n) -&gt; None:\n    \"\"\"\n    Ingests a folder into the vector database, after\n    processing the documents as required by the encoding model.\n    \"\"\"\n    from lmm.utils.ioutils import list_files_with_extensions\n    from lmm_education.ingest import markdown_upload\n    from lmm_education.config.config import ConfigSettings\n\n    try:\n        settings = ConfigSettings()\n        save_files = True\n        skip_ingest = False\n        files: list[str] = list_files_with_extensions(\n            folder, extensions\n        )\n        markdown_upload(\n            files,\n            config_opts=settings,\n            save_files=save_files,\n            ingest=not skip_ingest,\n            logger=logger,\n        )\n    except Exception as e:\n        logger.error(str(e))\n        raise typer.Exit(1)\n\n    # this is a workaround for the qdrant bug when clients\n    # are closed during garbage collection. We close all\n    # clients explicitly prior to exiting when this routine\n    # is called from the CLI, i.e. here Python is exiting.\n    if hasattr(sys, 'ps1'):\n        return\n    else:\n        from lmm_education.stores.vector_store_qdrant_context import (\n            qdrant_clients,\n        )\n\n        qdrant_clients.clear()\n</code></pre>"},{"location":"API/cli_lmme/#lmm_education.lmme.property_values","title":"<code>property_values(property=typer.Argument(..., help='Property to query'), collection=typer.Argument(default=None, help='Collection to query'))</code>","text":"<p>Displays the values of a property and their count in a collection.</p> Source code in <code>lmm_education/lmme.py</code> <pre><code>@app.command()\ndef property_values(\n    property: str = typer.Argument(..., help=\"Property to query\"),\n    collection: str | None = typer.Argument(\n        default=None, help=\"Collection to query\"\n    ),\n) -&gt; None:\n    \"\"\"\n    Displays the values of a property and their count in\n    a collection.\n    \"\"\"\n    from lmm_education.stores.vector_store_qdrant_utils import (\n        list_property_values,\n    )\n    from lmm_education.stores.vector_store_qdrant_context import (\n        global_client_from_config,\n    )\n    from lmm_education.config.config import ConfigSettings\n\n    settings = ConfigSettings()\n    if collection is None:\n        try:\n            if settings.database.companion_collection:\n                collection = settings.database.companion_collection\n            else:\n                collection = settings.database.collection_name\n        except Exception as e:\n            logger.error(f\"Could not load settings: {e}\")\n            raise typer.Exit(1)\n\n    client: QdrantClient\n    try:\n        client = global_client_from_config(settings.storage)\n    except Exception as e:\n        logger.error(f\"Could not load database: {e}\")\n        raise typer.Exit(1)\n    values: list[tuple[str, int]] = list_property_values(\n        client, property, collection, logger=logger\n    )\n    for item in values:\n        print(f\"{item[0]}\\t{item[1]}\")\n\n    # this is a workaround for the qdrant bug when clients\n    # are closed during garbage collection. We close all\n    # clients explicitly prior to exiting when this routine\n    # is called from the CLI, i.e. here Python is exiting.\n    if hasattr(sys, 'ps1'):\n        return\n    else:\n        from lmm_education.stores.vector_store_qdrant_context import (\n            qdrant_clients,\n        )\n\n        qdrant_clients.clear()\n</code></pre>"},{"location":"API/cli_lmme/#lmm_education.lmme.query","title":"<code>query(query_text=typer.Argument(..., help='Query text'), model=typer.Option(None, '--model', '-m', help='Language model (major, minor, aux, or provider/modelname)'), temperature=typer.Option(None, '--temperature', '-t', help='Controls randomness in model responses (0.0-2.0)'), max_tokens=typer.Option(None, '--max-tokens', '-mt', help='Maximum number of tokens to generate'), max_retries=typer.Option(None, '--max-retries', '-mr', help='Maximum number of retry attempts'), timeout=typer.Option(None, '--timeout', '-to', help='Request timeout in seconds'), validate_content=typer.Option(False, '--validate-content', '-vc', help='Validate content'), print_context=typer.Option(False, '--print-context', '-pc', help='Print context given to model'))</code>","text":"<p>Carries out a RAG query with the language model.</p> Source code in <code>lmm_education/lmme.py</code> <pre><code>@app.command()\ndef query(\n    query_text: str = typer.Argument(..., help=\"Query text\"),\n    model: str = typer.Option(\n        None,\n        \"--model\",\n        \"-m\",\n        help=\"Language model (major, minor, aux, or provider/modelname)\",\n    ),\n    temperature: float = typer.Option(\n        None,\n        \"--temperature\",\n        \"-t\",\n        help=\"Controls randomness in model responses (0.0-2.0)\",\n    ),\n    max_tokens: int = typer.Option(\n        None,\n        \"--max-tokens\",\n        \"-mt\",\n        help=\"Maximum number of tokens to generate\",\n    ),\n    max_retries: int = typer.Option(\n        None,\n        \"--max-retries\",\n        \"-mr\",\n        help=\"Maximum number of retry attempts\",\n    ),\n    timeout: float = typer.Option(\n        None, \"--timeout\", \"-to\", help=\"Request timeout in seconds\"\n    ),\n    validate_content: bool = typer.Option(\n        False, \"--validate-content\", \"-vc\", help=\"Validate content\"\n    ),\n    print_context: bool = typer.Option(\n        False,\n        \"--print-context\",\n        \"-pc\",\n        help=\"Print context given to model\",\n    ),\n) -&gt; None:\n    \"\"\"\n    Carries out a RAG query with the language model.\n    \"\"\"\n    from lmm_education.query import query\n    from lmm_education.config.config import ConfigSettings\n    from lmm_education.config.appchat import ChatSettings\n    from lmm.config.config import LanguageModelSettings\n\n    try:\n        settings = ConfigSettings()\n    except Exception as e:\n        logger.error(f\"Could not load settings: {e}\")\n        raise typer.Exit(1)\n\n    try:\n        chat_settings = ChatSettings()\n    except Exception as e:\n        logger.error(f\"Could not load settings: {e}\")\n        raise typer.Exit(1)\n\n    model_settings: LanguageModelSettings | None\n    opts: dict[str, str | float | int] = {}\n    if temperature is not None:  # type: ignore\n        opts['temperature'] = temperature\n    if max_tokens is not None:  # type: ignore\n        opts['max_tokens'] = max_tokens\n    if max_retries is not None:  # type: ignore\n        opts['max_retries'] = max_retries\n    if timeout is not None:  # type: ignore\n        opts['timeout'] = timeout\n\n    try:\n        if model is not None:  # type: ignore\n            match model:\n                case 'major':\n                    model_settings = (\n                        settings.major.from_instance(**opts)  # type: ignore\n                        if opts\n                        else settings.major\n                    )\n                case 'minor':\n                    model_settings = (\n                        settings.minor.from_instance(**opts)  # type: ignore\n                        if opts\n                        else settings.minor\n                    )\n                case 'aux':\n                    model_settings = (\n                        settings.aux.from_instance(**opts)  # type: ignore\n                        if opts\n                        else settings.aux\n                    )\n                case _:\n                    # try to parse, will give error if not supported\n                    opts['model'] = model\n                    model_settings = LanguageModelSettings(**opts)  # type: ignore\n\n        else:\n            if model is None and not opts:\n                model_settings = (\n                    None  # code reacheable, typer can give None\n                )\n            else:\n                model_settings = settings.major.from_instance(**opts)\n    except ValidationError as e:\n        errmsg: str = format_pydantic_error_message(str(e))\n        errmsg = errmsg.replace(\n            \"LanguageModelSettings\", \"language model setting\"\n        )\n        logger.error(\"Validation error: \" + errmsg)\n        raise typer.Exit(1)\n    except Exception as e:\n        logger.error(f\"Invalid model settings: {model}\\n{e}\")\n        raise typer.Exit(1)\n\n    try:\n        for text in query(\n            query_text,\n            model_settings=model_settings,\n            chat_settings=chat_settings,\n            validate_content=validate_content,\n            print_context=print_context,\n            logger=logger,\n        ):\n            print(text, end=\"\", flush=True)\n        print()\n    except Exception as e:\n        logger.error(str(e))\n        raise typer.Exit(1)\n\n    # this is a workaround for the qdrant issue when clients\n    # are closed during garbage collection. We close all\n    # clients explicitly prior to exiting when this routine\n    # is called from the CLI, i.e. here Python is exiting.\n    if hasattr(sys, 'ps1'):\n        return\n    else:\n        from lmm_education.stores.vector_store_qdrant_context import (\n            qdrant_clients,\n        )\n\n        qdrant_clients.clear()\n</code></pre>"},{"location":"API/cli_lmme/#lmm_education.lmme.querydb","title":"<code>querydb(query_text=typer.Argument(..., help='Query text'))</code>","text":"<p>Carries out a query to the RAG vector database.</p> Source code in <code>lmm_education/lmme.py</code> <pre><code>@app.command()\ndef querydb(\n    query_text: str = typer.Argument(..., help=\"Query text\"),\n) -&gt; None:\n    \"\"\"\n    Carries out a query to the RAG vector database.\n    \"\"\"\n    from lmm_education.querydb import querydb\n\n    try:\n        from rich import print\n    except Exception:\n        pass\n\n    try:\n        response: str = querydb(query_text, logger=logger)\n        print(response)  # type: ignore\n    except Exception as e:\n        logger.error(str(e))\n        raise typer.Exit(1)\n\n    # this is a workaround for the qdrant bug when clients\n    # are closed during garbage collection. We close all\n    # clients explicitly prior to exiting when this routine\n    # is called from the CLI, i.e. here Python is exiting.\n    if hasattr(sys, 'ps1'):\n        return\n    else:\n        from lmm_education.stores.vector_store_qdrant_context import (\n            qdrant_clients,\n        )\n\n        qdrant_clients.clear()\n</code></pre>"},{"location":"API/cli_lmme/#lmm_education.lmme.scan_changed_titles","title":"<code>scan_changed_titles(sourcefile=typer.Argument(..., help='Markdown file to process'))</code>","text":"<p>List titles whose text has changed and would be processed at next scan.</p> Source code in <code>lmm_education/lmme.py</code> <pre><code>@app.command()\ndef scan_changed_titles(\n    sourcefile: str = typer.Argument(\n        ..., help=\"Markdown file to process\"\n    ),\n) -&gt; None:\n    \"\"\"\n    List titles whose text has changed and would be processed at next scan.\n    \"\"\"\n    from lmm.utils.logging import ConsoleLogger\n    from lmm.markdown.parse_markdown import blocklist_haserrors, Block\n    from lmm.scan.scan import markdown_scan\n    from lmm.scan.scan_rag import get_changed_titles\n\n    logger = ConsoleLogger()\n\n    blocks: list[Block] = markdown_scan(sourcefile, logger=logger)\n    if blocklist_haserrors(blocks):\n        print(\"Errors in markdown. Fix before continuing\")\n        raise typer.Exit(0)\n\n    titles: list[str] = get_changed_titles(blocks, logger)\n    if not titles:\n        print(\"No text changes.\")\n    else:\n        for title in titles:\n            print(title)\n</code></pre>"},{"location":"API/cli_lmme/#lmm_education.lmme.scan_clear_messages","title":"<code>scan_clear_messages(sourcefile=typer.Argument(..., help='Markdown file to process'), key=typer.Option(None, '--key', '-k', help='Key to clear in the markdown file'))</code>","text":"<p>Clears all messages created during an interaction with a language model, such as a chat or the outcome of a query, or properties specified by a key</p> Source code in <code>lmm_education/lmme.py</code> <pre><code>@app.command()\ndef scan_clear_messages(\n    sourcefile: str = typer.Argument(\n        ..., help=\"Markdown file to process\"\n    ),\n    key: str = typer.Option(\n        None,\n        \"--key\",\n        \"-k\",\n        help=\"Key to clear in the markdown file\",\n    ),\n) -&gt; None:\n    \"\"\"\n    Clears all messages created during an interaction with a language\n    model, such as a chat or the outcome of a query, or properties\n    specified by a key\n    \"\"\"\n    from lmm.scan.scan_messages import markdown_clear_messages\n\n    SAVE_FILE = True\n    try:\n        if key is None:  # type: ignore\n            markdown_clear_messages(\n                sourcefile, None, SAVE_FILE, logger=logger\n            )\n        else:\n            if key:\n                markdown_clear_messages(\n                    sourcefile, [key], SAVE_FILE, logger=logger\n                )\n            else:\n                logger.error(\"Invalid or empty key\")\n\n    except Exception as e:\n        logger.error(str(e))\n        raise typer.Exit(1)\n</code></pre>"},{"location":"API/cli_lmme/#lmm_education.lmme.scan_messages","title":"<code>scan_messages(sourcefile=typer.Argument(..., help='Markdown file to process'), max_size_mb=typer.Option(50, '--max-size-mb', '-ms', help='Maximum size of the markdown file in MB'), warn_size_mb=typer.Option(10, '--warn-size-mb', '-ws', help='Warning size of the markdown file in MB'))</code>","text":"<p>Scans the markdown document for messages to send to the language model, and carries out the interaction.</p> Source code in <code>lmm_education/lmme.py</code> <pre><code>@app.command()\ndef scan_messages(\n    sourcefile: str = typer.Argument(\n        ..., help=\"Markdown file to process\"\n    ),\n    max_size_mb: float = typer.Option(\n        50,\n        \"--max-size-mb\",\n        \"-ms\",\n        help=\"Maximum size of the markdown file in MB\",\n    ),\n    warn_size_mb: float = typer.Option(\n        10,\n        \"--warn-size-mb\",\n        \"-ws\",\n        help=\"Warning size of the markdown file in MB\",\n    ),\n) -&gt; None:\n    \"\"\"\n    Scans the markdown document for messages to send to the language\n    model, and carries out the interaction.\n    \"\"\"\n    from lmm.scan.scan_messages import markdown_messages\n\n    try:\n        SAVE_FILE = True\n        markdown_messages(\n            sourcefile,\n            SAVE_FILE,\n            max_size_mb=max_size_mb,\n            warn_size_mb=warn_size_mb,\n            logger=logger,\n        )\n    except Exception as e:\n        logger.error(str(e))\n        raise typer.Exit(1)\n</code></pre>"},{"location":"API/cli_lmme/#lmm_education.lmme.scan_rag","title":"<code>scan_rag(sourcefile=typer.Argument(..., help='Markdown file to process'), titles=typer.Option(None, '--titles', '-t', help='Add titles to the markdown file'), questions=typer.Option(None, '--questions', '-q', help='Add questions to the markdown file'), questions_threshold=typer.Option(None, '--questions-threshold', '-qt', help='Threshold for questions'), summaries=typer.Option(None, '--summaries', '-s', help='Add summaries to the markdown file'), summary_threshold=typer.Option(None, '--summary-threshold', '-st', help='Threshold for summaries'), remove_messages=typer.Option(None, '--remove-messages', '-rm', help='Remove messages from the markdown file'), max_size_mb=typer.Option(50, '--max-size-mb', '-ms', help='Maximum size of the markdown file in MB'), warn_size_mb=typer.Option(10, '--warn-size-mb', '-ws', help='Warning size of the markdown file in MB'))</code>","text":"<p>Scans the markdown file and adds information required for the ingestion in the vector database.</p> Source code in <code>lmm_education/lmme.py</code> <pre><code>@app.command()\ndef scan_rag(\n    sourcefile: str = typer.Argument(\n        ..., help=\"Markdown file to process\"\n    ),\n    titles: bool = typer.Option(\n        None, \"--titles\", \"-t\", help=\"Add titles to the markdown file\"\n    ),\n    questions: bool = typer.Option(\n        None,\n        \"--questions\",\n        \"-q\",\n        help=\"Add questions to the markdown file\",\n    ),\n    questions_threshold: int = typer.Option(\n        None,\n        \"--questions-threshold\",\n        \"-qt\",\n        help=\"Threshold for questions\",\n    ),\n    summaries: bool = typer.Option(\n        None,\n        \"--summaries\",\n        \"-s\",\n        help=\"Add summaries to the markdown file\",\n    ),\n    summary_threshold: int = typer.Option(\n        None,\n        \"--summary-threshold\",\n        \"-st\",\n        help=\"Threshold for summaries\",\n    ),\n    remove_messages: bool = typer.Option(\n        None,\n        \"--remove-messages\",\n        \"-rm\",\n        help=\"Remove messages from the markdown file\",\n    ),\n    max_size_mb: float = typer.Option(\n        50,\n        \"--max-size-mb\",\n        \"-ms\",\n        help=\"Maximum size of the markdown file in MB\",\n    ),\n    warn_size_mb: float = typer.Option(\n        10,\n        \"--warn-size-mb\",\n        \"-ws\",\n        help=\"Warning size of the markdown file in MB\",\n    ),\n) -&gt; None:\n    \"\"\"\n    Scans the markdown file and adds information required for the\n    ingestion in the vector database.\n    \"\"\"\n    from lmm.scan.scan_rag import ScanOpts, markdown_rag\n    from lmm_education.config.config import (\n        ConfigSettings,\n        RAGSettings,\n    )\n\n    # read existing settings from config.toml\n    try:\n        config = ConfigSettings()\n    except Exception as e:\n        logger.error(f\"Could not load config: {e}\")\n        raise typer.Exit(1)\n\n    # override RAG settings with those given\n    try:\n        rag_settings: RAGSettings = config.RAG.from_instance(\n            titles, questions, summaries\n        )\n        # override the rest\n        config_dict: dict[str, bool | int | float] = {}\n        config_dict['titles'] = rag_settings.titles\n        config_dict['questions'] = rag_settings.questions\n        if questions_threshold is not None:  # type: ignore\n            config_dict['questions_threshold'] = questions_threshold\n        config_dict['summaries'] = rag_settings.summaries\n        if summary_threshold is not None:  # type: ignore\n            config_dict['summary_threshold'] = summary_threshold\n        if remove_messages is not None:  # type: ignore\n            config_dict['remove_messages'] = remove_messages\n        scan_opts = ScanOpts(**config_dict)  # type: ignore\n    except ValidationError as e:\n        errmsg: str = format_pydantic_error_message(str(e))\n        errmsg = errmsg.replace(\"ScanOpts\", \"setting\")\n        logger.error(\"Validation error: \" + errmsg)\n        raise typer.Exit(1)\n    except Exception as e:\n        logger.error(f\"Could not load config: {e}\")\n        raise typer.Exit(1)\n\n    try:\n        SAVE_FILE = True\n        markdown_rag(\n            sourcefile,\n            scan_opts,\n            SAVE_FILE,\n            max_size_mb=max_size_mb,\n            warn_size_mb=warn_size_mb,\n            logger=logger,\n        )\n    except Exception as e:\n        logger.error(str(e))\n        raise typer.Exit(1)\n</code></pre>"},{"location":"API/cli_lmme/#lmm_education.lmme.terminal","title":"<code>terminal()</code>","text":"<p>Opens a lmme terminal.</p> Source code in <code>lmm_education/lmme.py</code> <pre><code>@app.command()\ndef terminal() -&gt; None:\n    \"\"\"Opens a lmme terminal.\"\"\"\n\n    from .lme import main\n    import asyncio\n\n    try:\n        asyncio.run(main())\n    except KeyboardInterrupt:\n        # Catch the very final exit if needed\n        pass\n</code></pre>"},{"location":"API/cli_terminal/","title":"Terminal","text":"<p>Terminal interface module.</p> <p>options:     show_root_heading: false</p>"},{"location":"API/cli_terminal/#lmm_education.lme.LMMEngine","title":"<code>LMMEngine</code>","text":"<p>Manages the application state and resources for the LMM Education CLI.</p> Source code in <code>lmm_education/lme.py</code> <pre><code>class LMMEngine:\n    \"\"\"\n    Manages the application state and resources for the LMM Education CLI.\n    \"\"\"\n\n    def __init__(self):\n        self.logger: LoggerBase = ConsoleLogger()\n        config: ConfigSettings | None = load_settings(\n            logger=self.logger\n        )\n        if config is None:\n            print(\"Could not initialize CLI.\")\n        self.config: ConfigSettings | None = config\n\n    async def initialize(self) -&gt; bool:\n        \"\"\"\n        Initializes the engine, loading configuration and setting up logging.\n        \"\"\"\n\n        # load an embedding to load transformers library. the\n        # embeddings variable is cached in the global dictionary\n        # and the local reference is not used here.\n        from lmm.models.langchain.runnables import (\n            create_embeddings,\n        )\n\n        embeddings: Embeddings = create_embeddings()  # type: ignore # noqa\n\n        print(\" [Engine] Configuration loaded.\")\n        return True\n\n    async def shutdown(self):\n        \"\"\"\n        Cleans up resources, specifically closing database connections.\n        \"\"\"\n        if self.logger:\n            self.logger.info(\"Shutting down engine...\")\n\n        # Close all async qdrant clients\n        try:\n            global_async_clients_close()\n            print(\" [Engine] Resources released.\")\n        except Exception as e:\n            print(f\"Could not close Qdrant clients: {e}\")\n            return\n</code></pre>"},{"location":"API/cli_terminal/#lmm_education.lme.LMMEngine.initialize","title":"<code>initialize()</code>  <code>async</code>","text":"<p>Initializes the engine, loading configuration and setting up logging.</p> Source code in <code>lmm_education/lme.py</code> <pre><code>async def initialize(self) -&gt; bool:\n    \"\"\"\n    Initializes the engine, loading configuration and setting up logging.\n    \"\"\"\n\n    # load an embedding to load transformers library. the\n    # embeddings variable is cached in the global dictionary\n    # and the local reference is not used here.\n    from lmm.models.langchain.runnables import (\n        create_embeddings,\n    )\n\n    embeddings: Embeddings = create_embeddings()  # type: ignore # noqa\n\n    print(\" [Engine] Configuration loaded.\")\n    return True\n</code></pre>"},{"location":"API/cli_terminal/#lmm_education.lme.LMMEngine.shutdown","title":"<code>shutdown()</code>  <code>async</code>","text":"<p>Cleans up resources, specifically closing database connections.</p> Source code in <code>lmm_education/lme.py</code> <pre><code>async def shutdown(self):\n    \"\"\"\n    Cleans up resources, specifically closing database connections.\n    \"\"\"\n    if self.logger:\n        self.logger.info(\"Shutting down engine...\")\n\n    # Close all async qdrant clients\n    try:\n        global_async_clients_close()\n        print(\" [Engine] Resources released.\")\n    except Exception as e:\n        print(f\"Could not close Qdrant clients: {e}\")\n        return\n</code></pre>"},{"location":"API/cli_terminal/#lmm_education.lme.handle_command","title":"<code>handle_command(engine, user_input)</code>  <code>async</code>","text":"<p>Handles the user command. This function is designed to be cancellable.</p> Source code in <code>lmm_education/lme.py</code> <pre><code>async def handle_command(engine: LMMEngine, user_input: str):\n    \"\"\"\n    Handles the user command. This function is designed to be cancellable.\n    \"\"\"\n    user_input = user_input.strip()\n    if not user_input:\n        return\n\n    try:\n        parts = shlex.split(user_input)\n    except ValueError as e:\n        print(f\"Error parsing input: {e}\")\n        return\n\n    if not parts:\n        return\n\n    command = parts[0]\n    args = parts[1:]\n\n    if command == \"exit\":\n        print(\"Exiting...\")\n        sys.exit(0)\n\n    # Registry of commands\n    commands = {\n        \"scan_messages\": cmd_scan_messages,\n        \"scan_clear_messages\": cmd_scan_clear_messages,\n        \"scan_rag\": cmd_scan_rag,\n        \"scan_changed_titles\": cmd_scan_changed_titles,\n        \"ingest\": cmd_ingest,\n        \"ingest_folder\": cmd_ingest_folder,\n        \"query\": cmd_query,\n        \"querydb\": cmd_querydb,\n        \"property_values\": cmd_property_values,\n        \"database_info\": cmd_database_info,\n        \"config_info\": cmd_config_info,\n        \"create_default_config_file\": cmd_create_default_config_file,\n        \"help\": cmd_help,\n    }\n\n    if command in commands:\n        await commands[command](engine, args)\n    else:\n        print(f\"Unknown command: '{command}'\")\n        print(\"Type 'help' for a list of available commands.\")\n</code></pre>"},{"location":"API/ingest/","title":"Module ingest.py","text":"<p>The ingest module is called directly in the Python REPL to ingest markdown files.</p> <p>This module provides the facilities to ingest markdown files into a vector database for the LM markdown for education project.</p> <p>Prior to ingesting, the files are processed according to specifications read from the config.toml file. They include what annotations to create (questions, summaries) and how to use them in the encoding of the data. Because these specifications determine the schema of the database, they should not be changed after the database is created!</p> <p>Please see config/config.py for more information on the configuration options. They contain the name of the file or source where the database is located, the name of the collection or collections used to store the data, and the information to extract from text prior to storage.</p> <p>The LMM for education package allows two different models of creating storage for a RAG application. In the standard approach, the text is chunked, possibly with overlapping segments, combined with additional text ('annotations', such as questions answered by the text) and ingested. In the hierarchical graph RAG approach, the chunks provide the embeddings for retrieving larger parts of text, determined by the whole text under the same heading. The language model is then given these large coherent texts, exploiting the large context window of today's models.</p> <p>The two approaches intermingle somewhat as the hierarchical structure is exploited to extract information from the text in both cases.</p> Main functions <ul> <li>initialize_client: get a database client object for further use</li> <li>markdown_upload: ingest markdown files</li> </ul> <p>Examples:</p> <pre><code>from lmm.utils.ioutils import list_files_with_extensions\nfrom lmm_education.ingest import markdown_upload\n\ntry:\n    files: list[str] = list_files_with_extensions(\n        \"./ingest_folder\", \".md;.Rmd\"\n    )\n    markdown_upload(\n        files,\n        save_files=True,\n        ingest=True,\n    )\nexcept Exception as e:\n    print(str(e))\n</code></pre> <p>Interactive use from console.</p> <pre><code># ingests MyMarkdown.md\npython -m lmm_education.ingest MyMarkdown.md\n\n# process MyMarkdown.md without ingesting\npython -m lmm_education.ingest MyMarkdown.md False\n\n# process w/o ingesting, and save output\npython -m lmm_education.ingest MyMarkdown.md False True\n</code></pre> <p>See also the lmme module for the CLI interface to ingestion.</p> Note <p>The implementation of these functions is promarily synchronous. The main use is consistent with blocking until a whole ingestion has been completed.</p> <p>options:     show_root_heading: false</p>"},{"location":"API/ingest/#lmm_education.ingest.ablocklist_upload","title":"<code>ablocklist_upload(client, chunks, companion_chunks, opts, *, logger, ingest=True)</code>  <code>async</code>","text":"<p>Upload a list of preprocessed chunks (including document chunks) to the database. Uses the encoding strategy specified in the ConfigSettings object opts.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncQdrantClient</code> <p>the QdrantClient</p> required <code>chunks</code> <code>list[Chunk]</code> <p>list of chunks to be uploaded</p> required <code>companion_chunks</code> <code>list[Chunk]</code> <p>list of chunks for the companion collection</p> required <code>opts</code> <code>ConfigSettings</code> <p>the ConfigSettings object</p> required <code>ingest</code> <code>bool</code> <p>Whether to ingest documents into the vector database (default: True)</p> <code>True</code> <code>logger</code> <code>LoggerBase</code> <p>a logger object</p> required <p>Returns:</p> Type Description <code>list[tuple[str] | tuple[str, str]]</code> <p>A list of tuples containing the id's of the ingested objects,</p> <code>list[tuple[str] | tuple[str, str]]</code> <p>If document_collection is True, the second element of the</p> <code>list[tuple[str] | tuple[str, str]]</code> <p>tuples contains the id's of pooled text documents. The first</p> <code>list[tuple[str] | tuple[str, str]]</code> <p>element always contains the id of the chunk.</p> Source code in <code>lmm_education/ingest.py</code> <pre><code>async def ablocklist_upload(\n    client: AsyncQdrantClient,\n    chunks: list[Chunk],\n    companion_chunks: list[Chunk],\n    opts: ConfigSettings,\n    *,\n    logger: LoggerBase,\n    ingest: bool = True,\n) -&gt; list[tuple[str] | tuple[str, str]]:\n    \"\"\"\n    Upload a list of preprocessed chunks (including document chunks)\n    to the database. Uses the encoding strategy specified in the\n    ConfigSettings object opts.\n\n    Args:\n        client: the QdrantClient\n        chunks: list of chunks to be uploaded\n        companion_chunks: list of chunks for the companion collection\n        opts: the ConfigSettings object\n        ingest: Whether to ingest documents into the vector database\n            (default: True)\n        logger: a logger object\n\n    Returns:\n        A list of tuples containing the id's of the ingested objects,\n        If document_collection is True, the second element of the\n        tuples contains the id's of pooled text documents. The first\n        element always contains the id of the chunk.\n    \"\"\"\n\n    # ingestion of the chunks\n    dbOpts: DatabaseSettings = opts.database\n    model: QdrantEmbeddingModel = encoding_to_embedding_model(\n        opts.RAG.encoding_model\n    )\n\n    if ingest:\n        points: list[PointStruct] = await aupload(\n            client,\n            collection_name=dbOpts.collection_name,\n            qdrant_model=model,\n            embedding_settings=opts.embeddings,\n            chunks=chunks,\n            logger=logger,\n        )\n    else:\n        points = chunks_to_points(chunks, model, opts.embeddings)\n    if not bool(points):\n        logger.error(\"Could not upload documents\")\n        return []\n    else:\n        if ingest:\n            logger.info(\n                f\"Ingested {len(points)} chunks in main collection.\"\n            )\n\n    # ingestion of the document collection (if specified)\n    if bool(dbOpts.companion_collection):\n        if not (bool(companion_chunks)):\n            logger.error(\n                \"Companion collection specified, but no \"\n                + \"document chunks generated.\"\n            )\n            return []\n        doc_coll: str = dbOpts.companion_collection\n        if ingest:\n            logger.info(\n                f\"Ingesting companion collection ({len(companion_chunks)} chunks.)\"\n            )\n            docpoints: list[PointStruct] = await aupload(\n                client,\n                collection_name=doc_coll,\n                qdrant_model=QdrantEmbeddingModel.UUID,\n                embedding_settings=opts.embeddings,\n                chunks=companion_chunks,\n                logger=logger,\n            )\n        else:\n            docpoints = chunks_to_points(\n                chunks, QdrantEmbeddingModel.UUID, opts.embeddings\n            )\n        if not bool(docpoints):\n            logger.error(\"Could not upload documents to \" + doc_coll)\n            return []\n\n        return [\n            (\n                str(p.id),\n                (\n                    p.payload.get(GROUP_UUID_KEY, \"\")\n                    if p.payload\n                    else \"\"\n                ),\n            )\n            for p in points\n        ]\n\n    else:\n        return [(str(p.id),) for p in points]\n</code></pre>"},{"location":"API/ingest/#lmm_education.ingest.ainitialize_client","title":"<code>ainitialize_client(opts=None, logger=logger)</code>  <code>async</code>","text":"<p>Initialize QDRANT database. Will open or create the specified database and open or create the collection specified in opts, a settings object that reads the specifications in the configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>opts</code> <code>ConfigSettings | None</code> <p>a ConfigurationSettings object. If None, an object will be created from config.toml.</p> <code>None</code> <code>logger</code> <code>LoggerBase</code> <p>a logger object. Defaults to a console logger.</p> <code>logger</code> <p>Returns:</p> Type Description <code>AsyncQdrantClient | None</code> <p>an async Qdrant client object</p> Source code in <code>lmm_education/ingest.py</code> <pre><code>async def ainitialize_client(\n    opts: ConfigSettings | None = None,\n    logger: LoggerBase = logger,\n) -&gt; AsyncQdrantClient | None:\n    \"\"\"\n    Initialize QDRANT database. Will open or create the specified\n    database and open or create the collection specified in opts,\n    a settings object that reads the specifications in the\n    configuration file.\n\n    Args:\n        opts: a ConfigurationSettings object. If None, an object will\n            be created from config.toml.\n        logger: a logger object. Defaults to a console logger.\n\n    Returns:\n        an async Qdrant client object\n    \"\"\"\n    # Load and checks configuration settings.\n    if opts is None:\n        opts = load_settings(logger=logger)\n        if opts is None:\n            return None\n\n    dbOpts: DatabaseSettings = opts.database\n    collection_name: str = dbOpts.collection_name\n    if not collection_name:\n        return None\n    dbSource: DatabaseSource = opts.storage\n\n    # Obtain a QdrantClient object using the config file settings.\n    # We use the sync client here so we block during load\n    client: AsyncQdrantClient\n    try:\n        client = global_async_client_from_config(dbSource)\n    except Exception as e:\n        logger.error(f\"Could not load database: {e}\")\n        return None\n\n    # Initialize the database or check it for the presence of\n    # the collections used in LMM for education, as specified\n    # in the config file.\n    flag: bool = await ainitialize_collection_from_config(\n        client,\n        collection_name,\n        opts,\n        logger=logger,\n    )\n    if not flag:\n        return None\n\n    if bool(dbOpts.companion_collection):\n        flag = await ainitialize_collection(\n            client,\n            dbOpts.companion_collection,\n            QdrantEmbeddingModel.UUID,\n            opts.embeddings,\n            logger=logger,\n        )\n        if not flag:\n            return None\n\n    return client\n</code></pre>"},{"location":"API/ingest/#lmm_education.ingest.amarkdown_upload","title":"<code>amarkdown_upload(sources, *, config_opts=None, save_files=False, ingest=True, client=None, logger=logger)</code>  <code>async</code>","text":"<p>Upload a list of markdown files in the vector database (async version).</p> <p>Parameters:</p> Name Type Description Default <code>sources</code> <code>list[str] | list[Path] | str</code> <p>a list of file names containing the markdown files</p> required <code>config_opts</code> <code>ConfigSettings | None</code> <p>a ConfigSettings object declaring the schema; if None, will be read from config.toml</p> <code>None</code> <code>ingest</code> <code>bool</code> <p>Whether to ingest documents into the vector database (default: True)</p> <code>True</code> <code>save_files</code> <code>bool | TextIOBase</code> <p>Whether to save processed blocks to files for human inspection (default: False)</p> <code>False</code> <code>client</code> <code>AsyncQdrantClient | None</code> <p>if None (default) a QdrantClient will be initialized from the config_opts spec.</p> <code>None</code> <code>logger</code> <code>LoggerBase</code> <p>a logger object</p> <code>logger</code> <p>Returns:</p> Type Description <code>list[tuple[str] | tuple[str, str]]</code> <p>A list of strings, or string tuples, representing the processed content.</p> Note <p>This function makes use of the following other functions in this same module:     initialize_client: to get a QdrantClient object         initialized to a database with the collections for         the ingestion     blocklist_encode: transforms markdown files into the chunks         that will be ingested, creating the annotations with         a language model if they are still missing     blocklist_upload: takes the chunks, forms the embeddings,         and loads the whole lot into the database.</p> Source code in <code>lmm_education/ingest.py</code> <pre><code>async def amarkdown_upload(\n    sources: list[str] | list[Path] | str,\n    *,\n    config_opts: ConfigSettings | None = None,\n    save_files: bool | io.TextIOBase = False,\n    ingest: bool = True,\n    client: AsyncQdrantClient | None = None,\n    logger: LoggerBase = logger,\n) -&gt; list[tuple[str] | tuple[str, str]]:\n    \"\"\"\n    Upload a list of markdown files in the vector database (async\n    version).\n\n    Args:\n        sources: a list of file names containing the markdown files\n        config_opts: a ConfigSettings object declaring the schema;\n            if None, will be read from config.toml\n        ingest: Whether to ingest documents into the vector database\n            (default: True)\n        save_files: Whether to save processed blocks to files for\n            human inspection (default: False)\n        client: if None (default) a QdrantClient will be initialized\n            from the config_opts spec.\n        logger: a logger object\n\n    Returns:\n        A list of strings, or string tuples, representing the\n            processed content.\n\n    Note:\n        This function makes use of the following other functions in\n        this same module:\n            initialize_client: to get a QdrantClient object\n                initialized to a database with the collections for\n                the ingestion\n            blocklist_encode: transforms markdown files into the chunks\n                that will be ingested, creating the annotations with\n                a language model if they are still missing\n            blocklist_upload: takes the chunks, forms the embeddings,\n                and loads the whole lot into the database.\n    \"\"\"\n\n    if config_opts is None:\n        config_opts = load_settings(logger=logger)\n        if config_opts is None:\n            return []\n\n    if client is None:\n        client = await ainitialize_client(config_opts, logger)\n    if not client:\n        logger.error(\"Database could not be initialized.\")\n        return []\n\n    if not bool(sources):\n        logger.warning(\"No documents for ingestion in the database.\")\n        return []\n    if isinstance(sources, str):\n        sources = [sources]\n    error_sources: dict[str, list[ErrorBlock]] = {}\n    logger_level: int = logger.get_level()\n    for source in sources:\n        blocks: list[Block] = markdown_scan(\n            source, False, logger=logger\n        )\n        if not bool(blocks):\n            error_sources[str(source)] = [\n                ErrorBlock(\n                    content=f\"Empty or nonexistent file: {source}\"\n                )\n            ]\n        elif blocklist_haserrors(blocks):\n            error_sources[str(source)] = blocklist_errors(blocks)\n    if error_sources:\n        logger.error(\n            \"Problems in markdowns, fix before continuing:\\n\\t\"\n            + \"\\n\\t\".join(error_sources.keys())\n        )\n        return []\n    logger.set_level(logger_level)\n\n    ids: list[tuple[str] | tuple[str, str]] = []\n    for source in sources:\n        SAVE_FILE = False\n        blocks = markdown_scan(source, SAVE_FILE, logger=logger)\n        chunks, comp_chunks = blocklist_encode(\n            blocks, config_opts, logger\n        )\n        if not bool(chunks):\n            logger.warning(f\"{source} could not be encoded.\")\n            continue\n        # Ingestion.\n        idss: list[tuple[str] | tuple[str, str]] = (\n            await ablocklist_upload(\n                client,\n                chunks,\n                comp_chunks,\n                config_opts,\n                ingest=ingest,\n                logger=logger,\n            )\n        )\n        # Feedback and cumulate idss\n        if bool(idss):\n            if ingest:\n                storage_location: str = database_name(client)\n                logger.info(f\"{source} added to {storage_location}.\")\n            ids.extend(idss)\n        else:\n            logger.warning(f\"{source} could not be ingested.\")\n            continue\n\n        # this allows users to inspect the annotations that were\n        # used to encode the document\n        if bool(save_files) and bool(idss):\n            chunk_blocks: list[Block] = chunks_to_blocks(\n                chunks, '+++++'\n            )\n            comp_blocks: list[Block] = chunks_to_blocks(\n                comp_chunks, \".....\"\n            )\n            if isinstance(save_files, bool):\n                out_file: str = append_postfix_to_filename(\n                    str(source), \"_chunks\"\n                )\n                save_markdown(out_file, chunk_blocks, logger)\n                if bool(comp_blocks):\n                    out_file = append_postfix_to_filename(\n                        str(source), \"_documents\"\n                    )\n                    save_markdown(out_file, comp_blocks, logger)\n            else:  # it's a stream\n                from lmm.markdown.parse_markdown import TextBlock\n\n                if bool(comp_blocks):\n                    chunk_blocks.append(\n                        TextBlock(\n                            content=\"-----------------------------------------\"\n                        )\n                    )\n                save_markdown(\n                    save_files, chunk_blocks + comp_blocks, logger\n                )\n\n    return ids\n</code></pre>"},{"location":"API/ingest/#lmm_education.ingest.blocklist_encode","title":"<code>blocklist_encode(blocklist, opts, logger)</code>","text":"<p>Encode a list of markdown blocks. Preprocessing will be executed at this stage as specified in the ConfigSettings opts object.</p> <p>Parameters:</p> Name Type Description Default <code>blocklist</code> <code>list[Block]</code> <p>the list of markdown blocks</p> required <code>opts</code> <code>ConfigSettings</code> <p>the ConfigSettings object</p> required <code>logger</code> <code>LoggerBase</code> <p>a logger object</p> required <p>Returns:</p> Type Description <code>list[Chunk]</code> <p>A tuple of two lists of chunks. The first list contains the</p> <code>list[Chunk]</code> <p>chunks to be ingested into the main collection, the second</p> <code>tuple[list[Chunk], list[Chunk]]</code> <p>list contains the chunks to be ingested into the companion</p> <code>tuple[list[Chunk], list[Chunk]]</code> <p>collection. If no companion collection is specified in opts,</p> <code>tuple[list[Chunk], list[Chunk]]</code> <p>the second list will be empty.</p> Source code in <code>lmm_education/ingest.py</code> <pre><code>def blocklist_encode(\n    blocklist: list[Block],\n    opts: ConfigSettings,\n    logger: LoggerBase,\n) -&gt; tuple[list[Chunk], list[Chunk]]:\n    \"\"\"\n    Encode a list of markdown blocks. Preprocessing will be\n    executed at this stage as specified in the ConfigSettings opts\n    object.\n\n    Args:\n        blocklist: the list of markdown blocks\n        opts: the ConfigSettings object\n        logger: a logger object\n\n    Returns:\n        A tuple of two lists of chunks. The first list contains the\n        chunks to be ingested into the main collection, the second\n        list contains the chunks to be ingested into the companion\n        collection. If no companion collection is specified in opts,\n        the second list will be empty.\n    \"\"\"\n\n    if not blocklist:\n        return [], []\n    if blocklist_haserrors(blocklist):\n        report_error_blocks(blocklist, logger)\n        logger.error(\"Problems in markdown, fix before continuing\")\n        return [], []\n\n    # we get rid of all existing UUIDs preliminarly\n    blocklist = clear_metadata_properties(blocklist, [UUID_KEY])\n\n    # this contains the info if form a companion collection\n    # TODO: it would be more transparent for user to set this\n    # info in a RAG section of config.toml\n    dbOpts: DatabaseSettings = opts.database\n\n    # preprocessing for RAG. You tell scan_rag what annotations\n    # to make.\n    # * titles and questions are used in annotations\n    # * summaries are additional text blocks encoded on their own (see\n    #       raptor paper)\n    # * headingUUID are required to form groupUUID's\n    scan_opts = ScanOpts(\n        titles=bool(opts.RAG.titles),\n        questions=bool(opts.RAG.questions),\n        summaries=bool(opts.RAG.summaries),\n        # if computing a companion collection (group queries)\n        # we need the id's to link records between collections\n        headingid=bool(dbOpts.companion_collection),\n        headingUUID=bool(dbOpts.companion_collection),\n        language_model_settings=opts,\n    )\n    blocks: list[Block] = blocklist_rag(blocklist, scan_opts, logger)\n    if not blocks:\n        return [], []\n\n    # The companion collection is used to store the textblocks\n    # under headings together.\n    if dbOpts.companion_collection:\n        coll_blocks: list[Block] = blocklist_copy(blocks)\n        # in the companion collection, we merge textblocks\n        # under headings together prior to saving them in the\n        # documents collection with the UUID of the headings\n        coll_blocks = merge_textblocks(coll_blocks)\n        coll_root: MarkdownNode | None = blocks_to_tree(coll_blocks)\n        if coll_root is None:\n            return [], []\n\n        # this will also inherit the UUID\n        coll_root = inherit_metadata(\n            coll_root,\n            exclude=[\n                TXTHASH_KEY,\n                CHAT_KEY,\n                SUMMARY_KEY,\n                LAST_MODIFIED_KEY,\n            ],\n            inherit=True,\n            include_header=True,\n            filter_func=lambda n: n.is_text_node(),\n        )\n\n        # # copy summaries from immediate parent into the context\n        # summary key of text nodes, to pool with content at retrieval\n        if opts.RAG.summaries:\n            # inherit_parent_properties raises an exception only when\n            # the list of properties and destination_names are of\n            # differing length, which is obviously not the case here\n            coll_root = inherit_parent_properties(\n                coll_root,\n                properties=[SUMMARY_KEY],\n                destination_names=[CTXT_SUMMARY_KEY],\n                filter_func=lambda n: n.is_heading_node(),\n            )\n            coll_root = inherit_parent_properties(\n                coll_root,\n                properties=[CTXT_SUMMARY_KEY],\n                destination_names=[CTXT_SUMMARY_KEY],\n                filter_func=lambda n: n.is_text_node(),\n            )\n\n        # collect text and annotations into chunk objects\n        coll_chunks: list[Chunk] = blocks_to_chunks(\n            tree_to_blocks(coll_root),\n            encoding_model=EncodingModel.NONE,\n            annotation_model=AnnotationModel(),  # no annotations here\n            logger=logger,\n        )\n        if not coll_chunks:\n            return [], []\n\n    else:\n        coll_chunks = []\n\n    # chunking\n    splitter: TextSplitter = defaultSplitter\n    match opts.textSplitter.splitter:\n        case 'none':\n            splitter = NullTextSplitter()\n        case 'default':\n            splitter = RecursiveCharacterTextSplitter(\n                chunk_size=500,\n                chunk_overlap=200,\n                add_start_index=False,\n            )\n        case _:\n            raise ValueError(\n                f\"Invalid splitter: {opts.textSplitter.splitter}\"\n            )\n\n    splits: list[Block] = scan_split(blocks, splitter)\n\n    # now copy the UUID in the heading into the metadata of the\n    # splits under the name GROUP_UUID_KEY\n    root: MarkdownTree = blocks_to_tree(splits)\n    if not root:\n        return [], []\n\n    root = bequeath_properties(root, [UUID_KEY], [GROUP_UUID_KEY])\n    splits = tree_to_blocks(root)\n\n    # we avoid chunks that isolate or split equations or code to keep\n    # those together with textual context to improve the embedding\n    # TODO: handle text overlap\n    # splits = merge_code_blocks(splits)\n    # splits = merge_equation_blocks(splits)\n\n    # summaries. To add summaries as additional chunks, we transform\n    # the summary property into a child text block of the heading node\n    def _propagate_summaries(blocks: list[Block]) -&gt; list[Block]:\n        root: MarkdownTree = blocks_to_tree(\n            blocklist_copy(blocks), logger\n        )\n        if not root:\n            return []\n\n        # propagate_property acts on heading nodes moving the content\n        # of the metadata from the metadata to a child text node. If\n        # add_type_property is set to True, the child text node will\n        # have a metadata property marking its content type as summary\n        add_type_property: bool = True\n        root = propagate_property(\n            root, SUMMARY_KEY, add_key_info=add_type_property\n        )\n        return tree_to_blocks(root)\n\n    if bool(opts.RAG.summaries):\n        splits = _propagate_summaries(splits)\n\n    # the blocks_to_chunks function transforms the chunks into the\n    # format recognized by the vector database for ingestion, also\n    # collecting the annotations specified by the encoding model.\n    chunks: list[Chunk] = blocks_to_chunks(\n        splits,\n        encoding_model=opts.RAG.encoding_model,\n        annotation_model=opts.get_annotation_model(),\n        logger=logger,\n    )\n    logger.info(f\"Created {len(chunks)} chunks for ingestion.\")\n    if not chunks:\n        return [], []\n\n    return chunks, coll_chunks\n</code></pre>"},{"location":"API/ingest/#lmm_education.ingest.blocklist_upload","title":"<code>blocklist_upload(client, chunks, companion_chunks, opts, *, logger, ingest=True)</code>","text":"<p>Upload a list of preprocessed chunks (including document chunks) to the database. Uses the encoding strategy specified in the ConfigSettings object opts.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>QdrantClient</code> <p>the QdrantClient</p> required <code>chunks</code> <code>list[Chunk]</code> <p>list of chunks to be uploaded</p> required <code>companion_chunks</code> <code>list[Chunk]</code> <p>list of chunks for the companion collection</p> required <code>opts</code> <code>ConfigSettings</code> <p>the ConfigSettings object</p> required <code>ingest</code> <code>bool</code> <p>Whether to ingest documents into the vector database (default: True)</p> <code>True</code> <code>logger</code> <code>LoggerBase</code> <p>a logger object</p> required <p>Returns:</p> Type Description <code>list[tuple[str] | tuple[str, str]]</code> <p>A list of tuples containing the id's of the ingested objects,</p> <code>list[tuple[str] | tuple[str, str]]</code> <p>If document_collection is True, the second element of the</p> <code>list[tuple[str] | tuple[str, str]]</code> <p>tuples contains the id's of pooled text documents. The first</p> <code>list[tuple[str] | tuple[str, str]]</code> <p>element always contains the id of the chunk.</p> Source code in <code>lmm_education/ingest.py</code> <pre><code>def blocklist_upload(\n    client: QdrantClient,\n    chunks: list[Chunk],\n    companion_chunks: list[Chunk],\n    opts: ConfigSettings,\n    *,\n    logger: LoggerBase,\n    ingest: bool = True,\n) -&gt; list[tuple[str] | tuple[str, str]]:\n    \"\"\"\n    Upload a list of preprocessed chunks (including document chunks)\n    to the database. Uses the encoding strategy specified in the\n    ConfigSettings object opts.\n\n    Args:\n        client: the QdrantClient\n        chunks: list of chunks to be uploaded\n        companion_chunks: list of chunks for the companion collection\n        opts: the ConfigSettings object\n        ingest: Whether to ingest documents into the vector database\n            (default: True)\n        logger: a logger object\n\n    Returns:\n        A list of tuples containing the id's of the ingested objects,\n        If document_collection is True, the second element of the\n        tuples contains the id's of pooled text documents. The first\n        element always contains the id of the chunk.\n    \"\"\"\n\n    # ingestion of the chunks\n    dbOpts: DatabaseSettings = opts.database\n    model: QdrantEmbeddingModel = encoding_to_embedding_model(\n        opts.RAG.encoding_model\n    )\n\n    if ingest:\n        points: list[PointStruct] = upload(\n            client,\n            collection_name=dbOpts.collection_name,\n            qdrant_model=model,\n            embedding_settings=opts.embeddings,\n            chunks=chunks,\n            logger=logger,\n        )\n    else:\n        points = chunks_to_points(chunks, model, opts.embeddings)\n    if not bool(points):\n        logger.error(\"Could not upload documents\")\n        return []\n    else:\n        if ingest:\n            logger.info(\n                f\"Ingested {len(points)} chunks in main collection.\"\n            )\n\n    # ingestion of the document collection (if specified)\n    if bool(dbOpts.companion_collection):\n        if not (bool(companion_chunks)):\n            logger.error(\n                \"Companion collection specified, but no \"\n                + \"document chunks generated.\"\n            )\n            return []\n        doc_coll: str = dbOpts.companion_collection\n        if ingest:\n            logger.info(\n                f\"Ingesting companion collection ({len(companion_chunks)} chunks.)\"\n            )\n            docpoints: list[PointStruct] = upload(\n                client,\n                collection_name=doc_coll,\n                qdrant_model=QdrantEmbeddingModel.UUID,\n                embedding_settings=opts.embeddings,\n                chunks=companion_chunks,\n                logger=logger,\n            )\n        else:\n            docpoints = chunks_to_points(\n                chunks, QdrantEmbeddingModel.UUID, opts.embeddings\n            )\n        if not bool(docpoints):\n            logger.error(\"Could not upload documents to \" + doc_coll)\n            return []\n\n        return [\n            (\n                str(p.id),\n                (\n                    p.payload.get(GROUP_UUID_KEY, \"\")\n                    if p.payload\n                    else \"\"\n                ),\n            )\n            for p in points\n        ]\n\n    else:\n        return [(str(p.id),) for p in points]\n</code></pre>"},{"location":"API/ingest/#lmm_education.ingest.initialize_client","title":"<code>initialize_client(opts=None, logger=logger)</code>","text":"<p>Initialize QDRANT database. Will open or create the specified database and open or create the collection specified in opts, a settings object that reads the specifications in the configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>opts</code> <code>ConfigSettings | None</code> <p>a ConfigurationSettings object. If None, an object will be created from config.toml.</p> <code>None</code> <code>logger</code> <code>LoggerBase</code> <p>a logger object. Defaults to a console logger.</p> <code>logger</code> <p>Returns:</p> Type Description <code>QdrantClient | None</code> <p>a Qdrant client object</p> Source code in <code>lmm_education/ingest.py</code> <pre><code>@validate_call(config={'arbitrary_types_allowed': True})\ndef initialize_client(\n    opts: ConfigSettings | None = None,\n    logger: LoggerBase = logger,\n) -&gt; QdrantClient | None:\n    \"\"\"\n    Initialize QDRANT database. Will open or create the specified\n    database and open or create the collection specified in opts,\n    a settings object that reads the specifications in the\n    configuration file.\n\n    Args:\n        opts: a ConfigurationSettings object. If None, an object will\n            be created from config.toml.\n        logger: a logger object. Defaults to a console logger.\n\n    Returns:\n        a Qdrant client object\n    \"\"\"\n\n    # Load and checks configuration settings.\n    if opts is None:\n        opts = load_settings(logger=logger)\n        if opts is None:\n            return None\n\n    dbOpts: DatabaseSettings = opts.database\n    collection_name: str = dbOpts.collection_name\n    if not collection_name:\n        return None\n    dbSource: DatabaseSource = opts.storage\n\n    # Obtain a QdrantClient object using the config file settings.\n    # We use the sync client here so we block during load\n    client: QdrantClient\n    try:\n        client = global_client_from_config(dbSource)\n    except Exception as e:\n        logger.error(f\"Could not load database: {e}\")\n        return None\n\n    # Initialize the database or check it for the presence of\n    # the collections used in LMM for education, as specified\n    # in the config file.\n    flag: bool = initialize_collection_from_config(\n        client,\n        collection_name,\n        opts,\n        logger=logger,\n    )\n    if not flag:\n        return None\n\n    if bool(dbOpts.companion_collection):\n        flag = initialize_collection(\n            client,\n            dbOpts.companion_collection,\n            QdrantEmbeddingModel.UUID,\n            opts.embeddings,\n            logger=logger,\n        )\n        if not flag:\n            return None\n\n    return client\n</code></pre>"},{"location":"API/ingest/#lmm_education.ingest.markdown_upload","title":"<code>markdown_upload(sources, *, config_opts=None, save_files=False, ingest=True, client=None, logger=logger)</code>","text":"<p>Upload a list of markdown files in the vector database.</p> <p>Parameters:</p> Name Type Description Default <code>sources</code> <code>list[str] | list[Path] | str</code> <p>a list of file names containing the markdown files</p> required <code>config_opts</code> <code>ConfigSettings | None</code> <p>a ConfigSettings object declaring the schema; if None, will be read from config.toml</p> <code>None</code> <code>ingest</code> <code>bool</code> <p>Whether to ingest documents into the vector database (default: True)</p> <code>True</code> <code>save_files</code> <code>bool | TextIOBase</code> <p>Whether to save processed blocks to files for human inspection (default: False)</p> <code>False</code> <code>client</code> <code>QdrantClient | None</code> <p>if None (default) a QdrantClient will be initialized from the config_opts spec.</p> <code>None</code> <code>logger</code> <code>LoggerBase</code> <p>a logger object</p> <code>logger</code> <p>Returns:</p> Type Description <code>list[tuple[str] | tuple[str, str]]</code> <p>A list of strings, or string tuples, representing the processed content.</p> Note <p>This function makes use of the following other functions in this same module:     initialize_client: to get a QdrantClient object         initialized to a database with the collections for         the ingestion     blocklist_encode: transforms markdown files into the chunks         that will be ingested, creating the annotations with         a language model if they are still missing     blocklist_upload: takes the chunks, forms the embeddings,         and loads the whole lot into the database.</p> Source code in <code>lmm_education/ingest.py</code> <pre><code>@validate_call(config={'arbitrary_types_allowed': True})\ndef markdown_upload(\n    sources: list[str] | list[Path] | str,\n    *,\n    config_opts: ConfigSettings | None = None,\n    save_files: bool | io.TextIOBase = False,\n    ingest: bool = True,\n    client: QdrantClient | None = None,\n    logger: LoggerBase = logger,\n) -&gt; list[tuple[str] | tuple[str, str]]:\n    \"\"\"\n    Upload a list of markdown files in the vector database.\n\n    Args:\n        sources: a list of file names containing the markdown files\n        config_opts: a ConfigSettings object declaring the schema;\n            if None, will be read from config.toml\n        ingest: Whether to ingest documents into the vector database\n            (default: True)\n        save_files: Whether to save processed blocks to files for\n            human inspection (default: False)\n        client: if None (default) a QdrantClient will be initialized\n            from the config_opts spec.\n        logger: a logger object\n\n    Returns:\n        A list of strings, or string tuples, representing the\n            processed content.\n\n    Note:\n        This function makes use of the following other functions in\n        this same module:\n            initialize_client: to get a QdrantClient object\n                initialized to a database with the collections for\n                the ingestion\n            blocklist_encode: transforms markdown files into the chunks\n                that will be ingested, creating the annotations with\n                a language model if they are still missing\n            blocklist_upload: takes the chunks, forms the embeddings,\n                and loads the whole lot into the database.\n    \"\"\"\n\n    # Check the config file is ok. The ConfigSettings constructor\n    # will read the config.toml file if no arguments are provided\n    if config_opts is None:\n        config_opts = load_settings(logger=logger)\n        if config_opts is None:\n            return []\n\n    # initialize qdrant client. The config_opts object will be used\n    # to validate the database schema, i.e that it has a schema that\n    # corresponds to the encoding model applied to the documents. If\n    # the database does not exist, it will be created.\n    if client is None:\n        client = initialize_client(config_opts, logger)\n    if not client:\n        logger.error(\"Database could not be initialized.\")\n        return []\n\n    # The markdown parser responds to errors by creating a special\n    # type of block, the ErrorBlock. This blocks can be serialized\n    # back to document, giving the user a chance to address the error,\n    # but here we just stop the process if there are parse errors in\n    # any of the documents. Hence, we do a priminary scan of all\n    # documents to check for errors.\n    if not bool(sources):\n        logger.warning(\"No documents for ingestion in the database.\")\n        return []\n    if isinstance(sources, str):\n        sources = [sources]\n    error_sources: dict[str, list[ErrorBlock]] = {}\n    logger_level: int = logger.get_level()\n    for source in sources:\n        blocks: list[Block] = markdown_scan(\n            source, False, logger=logger\n        )\n        if not bool(blocks):\n            error_sources[str(source)] = [\n                ErrorBlock(\n                    content=f\"Empty or nonexistent file: {source}\"\n                )\n            ]\n        elif blocklist_haserrors(blocks):\n            error_sources[str(source)] = blocklist_errors(blocks)\n    if error_sources:\n        logger.error(\n            \"Problems in markdowns, fix before continuing:\\n\\t\"\n            + \"\\n\\t\".join(error_sources.keys())\n        )\n        return []\n    logger.set_level(logger_level)\n\n    # the key loop. Each file is encoded and uploaded. We go through\n    # the files again to avoid loading all files into memory at\n    # once.\n    ids: list[tuple[str] | tuple[str, str]] = []\n    for source in sources:\n        # Markdown documents are loaded from files with markdown_scan\n        # because this function can initialize default properties\n        # such as 'title' from the file name.\n        SAVE_FILE = False\n        blocks = markdown_scan(source, SAVE_FILE, logger=logger)\n        # Process the markdown documents prior to ingesting, and\n        # create the chunks.\n        chunks, comp_chunks = blocklist_encode(\n            blocks, config_opts, logger\n        )\n        if not bool(chunks):\n            logger.warning(f\"{source} could not be encoded.\")\n            continue\n        # Ingestion.\n        idss: list[tuple[str] | tuple[str, str]] = blocklist_upload(\n            client,\n            chunks,\n            comp_chunks,\n            config_opts,\n            ingest=ingest,\n            logger=logger,\n        )\n        # Feedback and cumulate idss\n        if bool(idss):\n            if ingest:\n                storage_location: str = database_name(client)\n                logger.info(f\"{source} added to {storage_location}.\")\n            ids.extend(idss)\n        else:\n            logger.warning(f\"{source} could not be ingested.\")\n            continue\n\n        # this allows users to inspect the annotations that were\n        # used to encode the document\n        if bool(save_files) and bool(idss):\n            chunk_blocks: list[Block] = chunks_to_blocks(\n                chunks, '+++++'\n            )\n            comp_blocks: list[Block] = chunks_to_blocks(\n                comp_chunks, \".....\"\n            )\n            if isinstance(save_files, bool):\n                out_file: str = append_postfix_to_filename(\n                    str(source), \"_chunks\"\n                )\n                save_markdown(out_file, chunk_blocks, logger)\n                if bool(comp_blocks):\n                    out_file = append_postfix_to_filename(\n                        str(source), \"_documents\"\n                    )\n                    save_markdown(out_file, comp_blocks, logger)\n            else:  # it's a stream\n                from lmm.markdown.parse_markdown import TextBlock\n\n                if bool(comp_blocks):\n                    chunk_blocks.append(\n                        TextBlock(\n                            content=\"----------COMPANION BLOCKS:--------------\"\n                        )\n                    )\n                save_markdown(\n                    save_files, chunk_blocks + comp_blocks, logger\n                )\n\n    return ids\n</code></pre>"},{"location":"API/logging_db/","title":"Logging DB","text":"<p>Chat logging database interface and CSV implementations.</p> <p>This module provides abstractions for logging chat interactions to persistent storage. It includes an abstract base class (ChatDatabaseInterface) that defines the logging contract, and concrete implementations for CSV file storage.</p> <p>Classes:</p> Name Description <code>ChatDatabaseInterface</code> <p>Abstract base class defining the logging interface for chat interactions, including basic messages and messages with retrieval context.</p> <code>CsvChatDatabase</code> <p>CSV-based implementation writing to provided text streams. Formats data for CSV storage and handles both async and scheduled logging.</p> <code>CsvFileChatDatabase</code> <p>File-based CSV database that manages file lifecycle, including automatic header creation and resource cleanup.</p> The module supports two logging modes <ul> <li>Basic logging: Records user queries, model responses, and performance metrics</li> <li>Context-aware logging: Additionally captures retrieval context, validation results,   and content classification</li> </ul> <p>All database implementations are context managers and support asynchronous logging through schedule_task() for non-blocking operation.</p> <p>options:     show_root_heading: false</p>"},{"location":"API/logging_db/#lmm_education.workflows.langchain.graph_logging.ChatDatabaseInterface","title":"<code>ChatDatabaseInterface</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for chat logging databases.</p> Source code in <code>lmm_education/workflows/langchain/graph_logging.py</code> <pre><code>class ChatDatabaseInterface(ABC):\n    \"\"\"Abstract base class for chat logging databases.\"\"\"\n\n    @abstractmethod\n    async def log_message(\n        self,\n        record_id: str,\n        client_host: str,\n        session_hash: str,\n        timestamp: datetime,\n        message_count: int,\n        model_name: str,\n        interaction_type: str,\n        query: str,\n        response: str,\n        time_to_FB: float | None = None,\n        time_to_response: float | None = None,\n    ) -&gt; None:\n        \"\"\"Log a basic chat interaction/message.\"\"\"\n        pass\n\n    @abstractmethod\n    async def log_message_with_context(\n        self,\n        record_id: str,\n        client_host: str,\n        session_hash: str,\n        timestamp: datetime,\n        message_count: int,\n        model_name: str,\n        interaction_type: str,\n        query: str,\n        response: str,\n        validation: str,\n        context: str,\n        classification: str,\n        time_to_context: float | None = None,\n        time_to_FB: float | None = None,\n        time_to_response: float | None = None,\n    ) -&gt; None:\n        \"\"\"Log a chat interaction including retrieval context details.\"\"\"\n        pass\n\n    @abstractmethod\n    def close(self) -&gt; None:\n        \"\"\"Close database and release resources.\"\"\"\n        pass\n\n    def __enter__(self) -&gt; Self:\n        \"\"\"Enter context manager.\"\"\"\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb) -&gt; None:  # type: ignore\n        \"\"\"Exit context manager and cleanup resources.\"\"\"\n        self.close()\n\n    def schedule_message(\n        self,\n        record_id: str,\n        client_host: str,\n        session_hash: str,\n        timestamp: datetime,\n        message_count: int,\n        model_name: str,\n        interaction_type: str,\n        query: str,\n        response: str,\n        time_to_FB: float | None = None,\n        time_to_response: float | None = None,\n    ) -&gt; None:\n        \"\"\"Log a basic chat interaction/message (async).\"\"\"\n        schedule_task(\n            self.log_message(\n                record_id=record_id,\n                client_host=client_host,\n                session_hash=session_hash,\n                timestamp=timestamp,\n                message_count=message_count,\n                model_name=model_name,\n                interaction_type=interaction_type,\n                query=query,\n                response=response,\n                time_to_FB=time_to_FB,\n                time_to_response=time_to_response,\n            ),\n            error_callback=lambda e: print(f\"schedule_message: {e}\"),\n        )\n\n    def schedule_message_with_context(\n        self,\n        record_id: str,\n        client_host: str,\n        session_hash: str,\n        timestamp: datetime,\n        message_count: int,\n        model_name: str,\n        interaction_type: str,\n        query: str,\n        response: str,\n        validation: str,\n        context: str,\n        classification: str,\n        time_to_context: float | None = None,\n        time_to_FB: float | None = None,\n        time_to_response: float | None = None,\n    ) -&gt; None:\n        \"\"\"Log a basic chat interaction/message (async).\"\"\"\n        schedule_task(\n            self.log_message_with_context(\n                record_id=record_id,\n                client_host=client_host,\n                session_hash=session_hash,\n                timestamp=timestamp,\n                message_count=message_count,\n                model_name=model_name,\n                interaction_type=interaction_type,\n                query=query,\n                response=response,\n                validation=validation,\n                context=context,\n                classification=classification,\n                time_to_context=time_to_context,\n                time_to_FB=time_to_FB,\n                time_to_response=time_to_response,\n            ),\n            error_callback=lambda e: print(\n                f\"schedule_message_with_context: {e}\"\n            ),\n        )\n\n    @classmethod\n    def from_config(cls) -&gt; 'CsvFileChatDatabase':\n        from lmm_education.config.appchat import (\n            ChatSettings,\n            load_settings,\n            ChatDatabase,\n        )\n\n        chat_settings: ChatSettings | None = load_settings()\n        if chat_settings is None:\n            raise ValueError(\"Could not load chat settings\")\n\n        db_settings: ChatDatabase = chat_settings.chat_database\n        return CsvFileChatDatabase(\n            db_settings.messages_database_file,\n            db_settings.context_database_file,\n        )\n</code></pre>"},{"location":"API/logging_db/#lmm_education.workflows.langchain.graph_logging.ChatDatabaseInterface.__enter__","title":"<code>__enter__()</code>","text":"<p>Enter context manager.</p> Source code in <code>lmm_education/workflows/langchain/graph_logging.py</code> <pre><code>def __enter__(self) -&gt; Self:\n    \"\"\"Enter context manager.\"\"\"\n    return self\n</code></pre>"},{"location":"API/logging_db/#lmm_education.workflows.langchain.graph_logging.ChatDatabaseInterface.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Exit context manager and cleanup resources.</p> Source code in <code>lmm_education/workflows/langchain/graph_logging.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb) -&gt; None:  # type: ignore\n    \"\"\"Exit context manager and cleanup resources.\"\"\"\n    self.close()\n</code></pre>"},{"location":"API/logging_db/#lmm_education.workflows.langchain.graph_logging.ChatDatabaseInterface.close","title":"<code>close()</code>  <code>abstractmethod</code>","text":"<p>Close database and release resources.</p> Source code in <code>lmm_education/workflows/langchain/graph_logging.py</code> <pre><code>@abstractmethod\ndef close(self) -&gt; None:\n    \"\"\"Close database and release resources.\"\"\"\n    pass\n</code></pre>"},{"location":"API/logging_db/#lmm_education.workflows.langchain.graph_logging.ChatDatabaseInterface.log_message","title":"<code>log_message(record_id, client_host, session_hash, timestamp, message_count, model_name, interaction_type, query, response, time_to_FB=None, time_to_response=None)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Log a basic chat interaction/message.</p> Source code in <code>lmm_education/workflows/langchain/graph_logging.py</code> <pre><code>@abstractmethod\nasync def log_message(\n    self,\n    record_id: str,\n    client_host: str,\n    session_hash: str,\n    timestamp: datetime,\n    message_count: int,\n    model_name: str,\n    interaction_type: str,\n    query: str,\n    response: str,\n    time_to_FB: float | None = None,\n    time_to_response: float | None = None,\n) -&gt; None:\n    \"\"\"Log a basic chat interaction/message.\"\"\"\n    pass\n</code></pre>"},{"location":"API/logging_db/#lmm_education.workflows.langchain.graph_logging.ChatDatabaseInterface.log_message_with_context","title":"<code>log_message_with_context(record_id, client_host, session_hash, timestamp, message_count, model_name, interaction_type, query, response, validation, context, classification, time_to_context=None, time_to_FB=None, time_to_response=None)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Log a chat interaction including retrieval context details.</p> Source code in <code>lmm_education/workflows/langchain/graph_logging.py</code> <pre><code>@abstractmethod\nasync def log_message_with_context(\n    self,\n    record_id: str,\n    client_host: str,\n    session_hash: str,\n    timestamp: datetime,\n    message_count: int,\n    model_name: str,\n    interaction_type: str,\n    query: str,\n    response: str,\n    validation: str,\n    context: str,\n    classification: str,\n    time_to_context: float | None = None,\n    time_to_FB: float | None = None,\n    time_to_response: float | None = None,\n) -&gt; None:\n    \"\"\"Log a chat interaction including retrieval context details.\"\"\"\n    pass\n</code></pre>"},{"location":"API/logging_db/#lmm_education.workflows.langchain.graph_logging.ChatDatabaseInterface.schedule_message","title":"<code>schedule_message(record_id, client_host, session_hash, timestamp, message_count, model_name, interaction_type, query, response, time_to_FB=None, time_to_response=None)</code>","text":"<p>Log a basic chat interaction/message (async).</p> Source code in <code>lmm_education/workflows/langchain/graph_logging.py</code> <pre><code>def schedule_message(\n    self,\n    record_id: str,\n    client_host: str,\n    session_hash: str,\n    timestamp: datetime,\n    message_count: int,\n    model_name: str,\n    interaction_type: str,\n    query: str,\n    response: str,\n    time_to_FB: float | None = None,\n    time_to_response: float | None = None,\n) -&gt; None:\n    \"\"\"Log a basic chat interaction/message (async).\"\"\"\n    schedule_task(\n        self.log_message(\n            record_id=record_id,\n            client_host=client_host,\n            session_hash=session_hash,\n            timestamp=timestamp,\n            message_count=message_count,\n            model_name=model_name,\n            interaction_type=interaction_type,\n            query=query,\n            response=response,\n            time_to_FB=time_to_FB,\n            time_to_response=time_to_response,\n        ),\n        error_callback=lambda e: print(f\"schedule_message: {e}\"),\n    )\n</code></pre>"},{"location":"API/logging_db/#lmm_education.workflows.langchain.graph_logging.ChatDatabaseInterface.schedule_message_with_context","title":"<code>schedule_message_with_context(record_id, client_host, session_hash, timestamp, message_count, model_name, interaction_type, query, response, validation, context, classification, time_to_context=None, time_to_FB=None, time_to_response=None)</code>","text":"<p>Log a basic chat interaction/message (async).</p> Source code in <code>lmm_education/workflows/langchain/graph_logging.py</code> <pre><code>def schedule_message_with_context(\n    self,\n    record_id: str,\n    client_host: str,\n    session_hash: str,\n    timestamp: datetime,\n    message_count: int,\n    model_name: str,\n    interaction_type: str,\n    query: str,\n    response: str,\n    validation: str,\n    context: str,\n    classification: str,\n    time_to_context: float | None = None,\n    time_to_FB: float | None = None,\n    time_to_response: float | None = None,\n) -&gt; None:\n    \"\"\"Log a basic chat interaction/message (async).\"\"\"\n    schedule_task(\n        self.log_message_with_context(\n            record_id=record_id,\n            client_host=client_host,\n            session_hash=session_hash,\n            timestamp=timestamp,\n            message_count=message_count,\n            model_name=model_name,\n            interaction_type=interaction_type,\n            query=query,\n            response=response,\n            validation=validation,\n            context=context,\n            classification=classification,\n            time_to_context=time_to_context,\n            time_to_FB=time_to_FB,\n            time_to_response=time_to_response,\n        ),\n        error_callback=lambda e: print(\n            f\"schedule_message_with_context: {e}\"\n        ),\n    )\n</code></pre>"},{"location":"API/logging_db/#lmm_education.workflows.langchain.graph_logging.CsvChatDatabase","title":"<code>CsvChatDatabase</code>","text":"<p>               Bases: <code>ChatDatabaseInterface</code></p> <p>CSV implementation of the chat database interface. Writes to provided text streams.</p> Source code in <code>lmm_education/workflows/langchain/graph_logging.py</code> <pre><code>class CsvChatDatabase(ChatDatabaseInterface):\n    \"\"\"\n    CSV implementation of the chat database interface.\n    Writes to provided text streams.\n    \"\"\"\n\n    def __init__(\n        self,\n        message_stream: TextIOBase,\n        context_stream: TextIOBase | None = None,\n    ):\n        self.message_stream = message_stream\n        self.context_stream = context_stream\n\n    def _fmat_for_csv(self, text: str) -&gt; str:\n        \"\"\"Format text for CSV storage by escaping quotes and newlines.\"\"\"\n        if not text:\n            return \"\"\n        # Replace double quotation marks with single quotation marks\n        modified_text = text.replace('\"', \"'\")\n        # Replace newline characters with \" | \"\n        modified_text = modified_text.replace(\"\\n\", \" | \")\n        return modified_text\n\n    def _write_message_sync(\n        self,\n        record_id: str,\n        client_host: str,\n        session_hash: str,\n        timestamp: datetime,\n        message_count: int,\n        model_name: str,\n        interaction_type: str,\n        query: str,\n        response: str,\n        time_to_FB: float | None,\n        time_to_response: float | None,\n    ) -&gt; None:\n        \"\"\"Synchronous implementation of message writing.\"\"\"\n        self.message_stream.write(\n            f\"{record_id},{client_host},{session_hash},\"\n            f\"{timestamp},{message_count},\"\n            f\"{model_name},{interaction_type},\"\n            f'\"{self._fmat_for_csv(query)}\",'\n            f'\"{self._fmat_for_csv(response)}\",'\n            f\"{time_to_FB or ''},{time_to_response or ''}\\n\"\n        )\n        self.message_stream.flush()\n\n    def _write_context_sync(\n        self,\n        record_id: str,\n        validation: str,\n        context: str,\n        classification: str,\n        time_to_context: float | None,\n    ) -&gt; None:\n        \"\"\"Synchronous implementation of context writing.\"\"\"\n        if self.context_stream:\n            self.context_stream.write(\n                f\"{record_id},{validation},\"\n                f'\"{self._fmat_for_csv(context)}\",'\n                f\"{classification},{time_to_context or ''}\\n\"\n            )\n            self.context_stream.flush()\n\n    async def log_message(\n        self,\n        record_id: str,\n        client_host: str,\n        session_hash: str,\n        timestamp: datetime,\n        message_count: int,\n        model_name: str,\n        interaction_type: str,\n        query: str,\n        response: str,\n        time_to_FB: float | None = None,\n        time_to_response: float | None = None,\n    ) -&gt; None:\n        loop = asyncio.get_running_loop()\n        await loop.run_in_executor(\n            None,\n            self._write_message_sync,\n            record_id,\n            client_host,\n            session_hash,\n            timestamp,\n            message_count,\n            model_name,\n            interaction_type,\n            query,\n            response,\n            time_to_FB,\n            time_to_response,\n        )\n\n    async def log_message_with_context(\n        self,\n        record_id: str,\n        client_host: str,\n        session_hash: str,\n        timestamp: datetime,\n        message_count: int,\n        model_name: str,\n        interaction_type: str,\n        query: str,\n        response: str,\n        validation: str,\n        context: str,\n        classification: str,\n        time_to_context: float | None = None,\n        time_to_FB: float | None = None,\n        time_to_response: float | None = None,\n    ) -&gt; None:\n        loop = asyncio.get_running_loop()\n\n        def _log_both() -&gt; None:\n            self._write_message_sync(\n                record_id,\n                client_host,\n                session_hash,\n                timestamp,\n                message_count,\n                model_name,\n                interaction_type,\n                query,\n                response,\n                time_to_FB,\n                time_to_response,\n            )\n            self._write_context_sync(\n                record_id,\n                validation,\n                context,\n                classification,\n                time_to_context,\n            )\n\n        await loop.run_in_executor(None, _log_both)\n\n    def close(self) -&gt; None:\n        \"\"\"Close database resources. No-op for stream-based implementation.\"\"\"\n        # This implementation doesn't own the streams, so it doesn't close them.\n        # The caller is responsible for managing stream lifecycle.\n        pass\n</code></pre>"},{"location":"API/logging_db/#lmm_education.workflows.langchain.graph_logging.CsvChatDatabase.close","title":"<code>close()</code>","text":"<p>Close database resources. No-op for stream-based implementation.</p> Source code in <code>lmm_education/workflows/langchain/graph_logging.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close database resources. No-op for stream-based implementation.\"\"\"\n    # This implementation doesn't own the streams, so it doesn't close them.\n    # The caller is responsible for managing stream lifecycle.\n    pass\n</code></pre>"},{"location":"API/logging_db/#lmm_education.workflows.langchain.graph_logging.CsvFileChatDatabase","title":"<code>CsvFileChatDatabase</code>","text":"<p>               Bases: <code>CsvChatDatabase</code></p> <p>File-based CSV chat database that manages file lifecycle. Handles file creation, header initialization, and proper cleanup. Can be used as a context manager or with explicit close() calls.</p> Source code in <code>lmm_education/workflows/langchain/graph_logging.py</code> <pre><code>class CsvFileChatDatabase(CsvChatDatabase):\n    \"\"\"\n    File-based CSV chat database that manages file lifecycle.\n    Handles file creation, header initialization, and proper cleanup.\n    Can be used as a context manager or with explicit close() calls.\n    \"\"\"\n\n    def __init__(\n        self,\n        database_file: str,\n        database_context_file: str | None = None,\n    ):\n        self.database_file = database_file\n        self.database_context_file = database_context_file\n\n        # Initialize headers immediately\n        self._ensure_headers()\n\n        # Open files immediately for use\n        self._message_file = open(\n            database_file, \"a\", encoding=\"utf-8\"\n        )\n        self._context_file = (\n            open(database_context_file, \"a\", encoding=\"utf-8\")\n            if database_context_file\n            else None\n        )\n\n        # Initialize parent class with opened streams\n        super().__init__(self._message_file, self._context_file)\n\n    def _ensure_headers(self) -&gt; None:\n        \"\"\"Creates the database files with the correct headers if they don't exist.\"\"\"\n        if not os.path.exists(self.database_file):\n            with open(self.database_file, \"w\", encoding=\"utf-8\") as f:\n                f.write(\n                    \"record_id,client_host,session_hash,timestamp,\"\n                    \"history_length,model_name,interaction_type,\"\n                    \"query,response,time_to_FB,time_to_response\\n\"\n                )\n\n        if self.database_context_file and not os.path.exists(\n            self.database_context_file\n        ):\n            with open(\n                self.database_context_file, \"w\", encoding=\"utf-8\"\n            ) as f:\n                f.write(\n                    \"record_id,evaluation,context,classification,time_to_context\\n\"\n                )\n\n    def close(self) -&gt; None:\n        \"\"\"Explicitly close database files and release resources.\"\"\"\n        if hasattr(self, '_message_file'):\n            try:\n                self._message_file.close()\n            except Exception as e:\n                print(f\"Error closing message log file: {e}\")\n\n        if hasattr(self, '_context_file') and self._context_file:\n            try:\n                self._context_file.close()\n            except Exception as e:\n                print(f\"Error closing context log file: {e}\")\n\n    def __enter__(self) -&gt; Self:\n        \"\"\"Enter context manager. Files are already open, just return self.\"\"\"\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb) -&gt; None:  # type: ignore\n        \"\"\"Exit context manager and cleanup resources.\"\"\"\n        self.close()\n\n    def __del__(self) -&gt; None:\n        \"\"\"Fallback cleanup - not guaranteed to be called.\"\"\"\n        try:\n            self.close()\n        except Exception:\n            # Suppress errors during cleanup in destructor\n            pass\n</code></pre>"},{"location":"API/logging_db/#lmm_education.workflows.langchain.graph_logging.CsvFileChatDatabase.__del__","title":"<code>__del__()</code>","text":"<p>Fallback cleanup - not guaranteed to be called.</p> Source code in <code>lmm_education/workflows/langchain/graph_logging.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Fallback cleanup - not guaranteed to be called.\"\"\"\n    try:\n        self.close()\n    except Exception:\n        # Suppress errors during cleanup in destructor\n        pass\n</code></pre>"},{"location":"API/logging_db/#lmm_education.workflows.langchain.graph_logging.CsvFileChatDatabase.__enter__","title":"<code>__enter__()</code>","text":"<p>Enter context manager. Files are already open, just return self.</p> Source code in <code>lmm_education/workflows/langchain/graph_logging.py</code> <pre><code>def __enter__(self) -&gt; Self:\n    \"\"\"Enter context manager. Files are already open, just return self.\"\"\"\n    return self\n</code></pre>"},{"location":"API/logging_db/#lmm_education.workflows.langchain.graph_logging.CsvFileChatDatabase.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Exit context manager and cleanup resources.</p> Source code in <code>lmm_education/workflows/langchain/graph_logging.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb) -&gt; None:  # type: ignore\n    \"\"\"Exit context manager and cleanup resources.\"\"\"\n    self.close()\n</code></pre>"},{"location":"API/logging_db/#lmm_education.workflows.langchain.graph_logging.CsvFileChatDatabase.close","title":"<code>close()</code>","text":"<p>Explicitly close database files and release resources.</p> Source code in <code>lmm_education/workflows/langchain/graph_logging.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Explicitly close database files and release resources.\"\"\"\n    if hasattr(self, '_message_file'):\n        try:\n            self._message_file.close()\n        except Exception as e:\n            print(f\"Error closing message log file: {e}\")\n\n    if hasattr(self, '_context_file') and self._context_file:\n        try:\n            self._context_file.close()\n        except Exception as e:\n            print(f\"Error closing context log file: {e}\")\n</code></pre>"},{"location":"API/query/","title":"Module query.py","text":"<p>This module allows interactively querying a language model, such that it can be tested using material ingested in the RAG database.</p> <p>A RAG database must have been previously created (for example, with the ingest module). In the following examples, we assume the existence of a database on basic statistical modelling.</p> <p>Examples:</p> <pre><code># Python called from the console\n\npython -m lmm_education.query 'What is logistic regression?'\n</code></pre> <pre><code># from python code\nfrom lmm_education.query import query\n\nresponse = query('what is logistic regression?')\nprint(response)\n</code></pre> <p>Because ingest replaces the content of the database when documents are edited, you can set up an ingest-evaluate loop:</p> <pre><code># Python called from the console\n\n# append True to ingest the file 'LogisticRegression.md'\npython -m lmm_education.ingest LogisticRegression.md True\npython -m lmm_education.query 'what is logistic regression?'\n\nMain functions:\n\n- `create_chat_stream` and `create_chat_stringstream`: these functions\nset up streams or LangGraph 'states' and of text, respectively, to\nstream from the graph. They take as arguments the input to the graph\n(i.e., the query text), a history of previous messages, a Workflow\ncontext object for the graph, and optional parameters to configure the\nfunction of the graph (validation of query) or the stream (logging\nthe exchange to a database).\n- `query` and `aquery`: they take the query text and execute the\nquery, streaming the result. The provide an interface to config.toml,\nbut the settings contained there can be overridden by arguments.\nInternally, they create a chat stream with a Workflow context\ninitialized from the config.toml parameters.\n\nThese two layers of functions handle construction of a streamable\ngraph to execute a query by taking care of the two layers of possible\nspecifications, i.e. from config.toml and for the components of the\ngraph.\n</code></pre> <p>options:     show_root_heading: false</p>"},{"location":"API/query/#lmm_education.query.aquery","title":"<code>aquery(querystr, *, model_settings=None, chat_settings=None, print_context=False, validate_content=False, allowed_content=None, client=None, logger=ConsoleLogger())</code>  <code>async</code>","text":"<p>Asynchronous generator that yields a text stream from the graph coding the RAG chatting workflow.</p> <p>Parameters:</p> Name Type Description Default <code>querystr</code> <code>str</code> <p>The query text to send to the language model</p> required <code>model_settings</code> <code>LanguageModelSettings | str | None</code> <p>Language model settings (or 'major', 'minor', 'aux'). If None (default) take settings from config.toml</p> <code>None</code> <code>chat_settings</code> <code>ChatSettings | None</code> <p>Chat settings for the query</p> <code>None</code> <code>print_context</code> <code>bool</code> <p>If True, streams the RAG context</p> <code>False</code> <code>validate_content</code> <code>bool</code> <p>If True, validate response content</p> <code>False</code> <code>allowed_content</code> <code>list[str] | None</code> <p>List of allowed content types for validation</p> <code>None</code> <code>client</code> <code>AsyncQdrantClient | None</code> <p>Optional pre-configured Qdrant client</p> <code>None</code> <code>logger</code> <code>LoggerBase</code> <p>Logger instance for error reporting</p> <code>ConsoleLogger()</code> <p>Yields:</p> Name Type Description <code>str</code> <code>AsyncIterator[str]</code> <p>Text chunks from the language model response</p> Behaviour <p>On successful execution, it will yield the text chunks from the language model response. If exceptions are raised, it will yield an error message. Errors may also be propagated through the logger. This behaviour is consistent with its use in a CLI.</p> Example <pre><code># assumes vector database is present\nasync for text in aquery(\"What is logistic regression?\"):\n    print(text, end=\"\", flush=True)\nprint()\n</code></pre> Source code in <code>lmm_education/query.py</code> <pre><code>async def aquery(\n    querystr: str,\n    *,\n    model_settings: LanguageModelSettings | str | None = None,\n    chat_settings: ChatSettings | None = None,\n    print_context: bool = False,\n    validate_content: bool = False,\n    allowed_content: list[str] | None = None,\n    client: AsyncQdrantClient | None = None,\n    logger: LoggerBase = ConsoleLogger(),\n) -&gt; AsyncIterator[str]:\n    \"\"\"\n    Asynchronous generator that yields a text stream from the graph\n    coding the RAG chatting workflow.\n\n    Args:\n        querystr: The query text to send to the language model\n        model_settings: Language model settings (or 'major', 'minor',\n            'aux'). If None (default) take settings from config.toml\n        chat_settings: Chat settings for the query\n        print_context: If True, streams the RAG context\n        validate_content: If True, validate response content\n        allowed_content: List of allowed content types for validation\n        client: Optional pre-configured Qdrant client\n        logger: Logger instance for error reporting\n\n    Yields:\n        str: Text chunks from the language model response\n\n    Behaviour:\n        On successful execution, it will yield the text chunks from\n        the language model response. If exceptions are raised, it\n        will yield an error message. Errors may also be propagated\n        through the logger. This behaviour is consistent with its\n        use in a CLI.\n\n    Example:\n        ```python\n        # assumes vector database is present\n        async for text in aquery(\"What is logistic regression?\"):\n            print(text, end=\"\", flush=True)\n        print()\n        ```\n    \"\"\"\n    if allowed_content is None:\n        allowed_content = []\n\n    config_settings: ConfigSettings | None = load_settings(\n        logger=logger\n    )\n    if config_settings is None:\n        logger.error(\"Could not load settings.\")\n        yield \"Could not load settings.\"\n        return\n\n    if model_settings is None:\n        model_settings = config_settings.major\n    elif isinstance(model_settings, str):\n        config = ConfigSettings()\n        if model_settings == \"major\":\n            model_settings = config.major\n        elif model_settings == \"minor\":\n            model_settings = config.minor\n        elif model_settings == \"aux\":\n            model_settings = config.aux\n        else:\n            errmsg: str = (\n                f\"Invalid language model settings: \"\n                f\"{model_settings}\\nShould\"\n                \" be one of 'major', 'minor', 'aux'\"\n            )\n            logger.error(errmsg)\n            yield errmsg\n            return\n\n    if chat_settings is None:\n        chat_settings = load_chat_settings(logger=logger)\n        if chat_settings is None:\n            logger.error(\"Could not load chat settings\")\n            yield \"Could not load chat settings.\"\n            return\n\n    if validate_content and not allowed_content:\n        allowed_content = chat_settings.check_response.allowed_content\n        if not allowed_content:\n            errmsg = (\n                \"A request to validate content was made, but there is\"\n                \" no allowed content value in the configuration file.\"\n                \"\\nAdd a list of allowed contents in the \"\n                \"[check_response] section of \" + DEFAULT_CONFIG_FILE\n            )\n            logger.error(errmsg)\n            yield errmsg\n            return\n\n    try:\n        retriever: BaseRetriever = (\n            AsyncQdrantRetriever.from_config_settings(client=client)\n        )\n    except Exception as e:\n        logger.error(f\"Could not load retriever: {e}\")\n        yield f\"Could not load retriever: {e}\"\n        return\n\n    # Create dependency injection object\n    response_settings = CheckResponse(\n        check_response=validate_content,\n        allowed_content=allowed_content,\n    )  # this for override config settings\n    context = ChatWorkflowContext(\n        retriever=retriever,\n        chat_settings=chat_settings.from_instance(\n            check_response=response_settings\n        ),\n        logger=logger,\n    )\n\n    # Get the iterator and consume it\n    try:\n        iterator: tier_3_iterator = create_chat_stringstream(\n            querystr,\n            None,\n            context,\n            print_context=print_context,\n            validate=response_settings,\n            logger=logger,\n        )\n    except Exception as e:\n        logger.error(f\"Could not load chat stream: {e}\")\n        yield f\"Could not load chat stream: {e}\"\n        return\n\n    try:\n        async for chunk in iterator:\n            yield chunk\n    except Exception as e:\n        errmsg: str = f\"Workflow streaming failed: {e}\"\n        logger.error(errmsg)\n        yield errmsg\n        return\n</code></pre>"},{"location":"API/query/#lmm_education.query.create_chat_stream","title":"<code>create_chat_stream(querytext, history, context, *, stream_updates=False, validate=None, database_log=False, logger=ConsoleLogger())</code>","text":"<p>Creates and configures the chat stream. Returns tuples of (mode, event) items as returned from the graph.</p> <p>This function retrieves a compiled LangGraph's graph, creates the initial state, and returns a configured stream to process a user query.</p> Note <p>The tier_1_iterator contains all information from the graph stream, and may be combined with other stream adaptors to customize behaviour. Use create_chat_stringstream to get a stream of strings.</p> <p>Example:</p> <pre><code>```python\ntry:\n    stream = create_chat_stream(...)\n    async for mode, event in stream:\n        ...\nexcept Exception as e:\n    ...\n```\n\n```python\ntry:\n    stream_raw = create_chat_stream(...)\n    stream = tier_3_adapter(stream_raw)\n    async for txt in stream:\n        ...\nexcept Exception as e:\n    ...\n```\n</code></pre> <p>Args:     querytext: The user's query text     history: List of previous message exchanges (Gradio format)     context: a ChatWorkflowContext object for dependencies to be         injected into the graph     stream_updates: streams an updates channel (default to False)     validate: if None, validates response using settings from         context object. If False, carries out no validation. If a         CheckReponse object, overrides the settings from         the context object.     database_log: if False (default), carries out no database         logging of the exchanges. If True, carries out database         logging with the settings defined in the context object,         i.e. the files where the database is located. If a tuple         of streams or file paths is provided, it uses those streams         for logging.     logger: Logger instance for info and error reporting</p> <p>Yields:</p> Type Description <code>tier_1_iterator</code> <p>(mode, event) tuples from the LLM stream</p> Behaviour <p>This function does not stream the LLM response, it only sets up the stream and returns it. Exceptions will be raised in case the stream fails to initialize (usually, due to invalid settings or failure to acquire resources).</p> Source code in <code>lmm_education/query.py</code> <pre><code>def create_chat_stream(\n    querytext: str,\n    history: list[dict[str, str]] | None,\n    context: ChatWorkflowContext,\n    *,\n    stream_updates: bool = False,\n    validate: CheckResponse | Literal[False] | None = None,\n    database_log: (\n        bool | tuple[TextIOBase, TextIOBase] | tuple[str, str]\n    ) = False,\n    logger: LoggerBase = ConsoleLogger(),\n) -&gt; tier_1_iterator:\n    \"\"\"\n    Creates and configures the chat stream. Returns tuples of\n    (mode, event) items as returned from the graph.\n\n    This function retrieves a compiled LangGraph's graph, creates the\n    initial state, and returns a configured stream to process a user\n    query.\n\n    Note:\n        The tier_1_iterator contains all information from the graph\n        stream, and may be combined with other stream adaptors to\n        customize behaviour.\n        Use create_chat_stringstream to get a stream of strings.\n\n    Example:\n\n        ```python\n        try:\n            stream = create_chat_stream(...)\n            async for mode, event in stream:\n                ...\n        except Exception as e:\n            ...\n        ```\n\n        ```python\n        try:\n            stream_raw = create_chat_stream(...)\n            stream = tier_3_adapter(stream_raw)\n            async for txt in stream:\n                ...\n        except Exception as e:\n            ...\n        ```\n    Args:\n        querytext: The user's query text\n        history: List of previous message exchanges (Gradio format)\n        context: a ChatWorkflowContext object for dependencies to be\n            injected into the graph\n        stream_updates: streams an updates channel (default to False)\n        validate: if None, validates response using settings from\n            context object. If False, carries out no validation. If a\n            CheckReponse object, overrides the settings from\n            the context object.\n        database_log: if False (default), carries out no database\n            logging of the exchanges. If True, carries out database\n            logging with the settings defined in the context object,\n            i.e. the files where the database is located. If a tuple\n            of streams or file paths is provided, it uses those streams\n            for logging.\n        logger: Logger instance for info and error reporting\n\n    Yields:\n        (mode, event) tuples from the LLM stream\n\n    Behaviour:\n        This function does not stream the LLM response, it only sets\n        up the stream and returns it. Exceptions will be raised in\n        case the stream fails to initialize (usually, due to invalid\n        settings or failure to acquire resources).\n    \"\"\"\n\n    # Load settings.\n    config_settings: ConfigSettings | None = load_settings(\n        logger=logger\n    )\n    if config_settings is None:\n        raise ValueError(\"Could not load settings\")\n\n    # Retrieve settings for validation.\n    response_settings: CheckResponse\n    match validate:\n        case None:\n            response_settings = context.chat_settings.check_response\n        case False:\n            response_settings = CheckResponse(check_response=False)\n        case CheckResponse():\n            response_settings = validate\n\n    # Settings for logging.\n    chat_database: ChatDatabaseInterface | None = None\n    match database_log:\n        case False:\n            chat_database = None\n        case True:\n            # create logger using config info.\n            db_settings: ChatDatabase = (\n                context.chat_settings.chat_database\n            )\n            chat_database = CsvFileChatDatabase(\n                db_settings.messages_database_file,\n                db_settings.context_database_file,\n            )\n        case (TextIOBase(), TextIOBase()):\n            # create logger using provided streams.\n            chat_database = CsvChatDatabase(\n                database_log[0], database_log[1]\n            )\n        case (str(), str()):\n            # create logger using provided file paths.\n            chat_database = CsvFileChatDatabase(\n                database_log[0], database_log[1]\n            )\n\n    # map dblogger to typed lambda (required for typing)\n    dblogger: Callable[[ChatState], Awaitable[None]] | None = None\n    if chat_database:\n\n        async def _log_state(state: ChatState) -&gt; None:\n            # chat_database is captured from closure,\n            # but we verify it is not None for type checker\n            if chat_database:\n                await graph_logger(\n                    database=chat_database,\n                    state=state,\n                    context=context,\n                    client_host=context.client_host,\n                    session_hash=context.session_hash,\n                )\n\n        dblogger = _log_state\n\n    # Fetch workflow graph from factory. The default is to\n    # use the 'workflow' graph, but also 'agent' is supported\n    # if set in appchat.toml.\n    wfname: Literal['workflow'] | Literal['agent'] = (\n        context.chat_settings.workflow\n    )\n    try:\n        workflow: ChatStateGraphType = workflow_factory(\n            wfname, context, config_settings\n        )\n    except Exception as e:\n        raise ValueError(\n            f\"Could not create workflow {wfname}:\\n{e}\"\n        ) from e\n\n    # Create initial state\n    initial_state: ChatState = initialize_state(\n        querytext,\n        messages=history_to_messages(history) if history else [],\n        timestamp=datetime.now(),\n    )\n\n    # Set up the stream\n    raw_stream: tier_1_iterator = (\n        stream_graph_updates(workflow, initial_state, context)\n        if stream_updates\n        else stream_graph_state(workflow, initial_state, context)\n    )\n\n    # Configure stream for validation requests\n    tier_1_stream: tier_1_iterator = raw_stream\n    if response_settings.check_response:\n        # Initialize the validation model\n        allowed_content = response_settings.allowed_content\n        try:\n            # the allowed content here introduces categories that\n            # the model uses to classify responses. The model will\n            # always add categories such as 'general knowledge' to\n            # this list.\n            validator_model: RunnableType = create_runnable(\n                \"allowed_content_validator\",\n                allowed_content=allowed_content,  # type: ignore\n            )\n        except Exception as e:\n            raise ValueError(\n                f\"Could not initialize validation model: {e}\"\n            ) from e\n\n        # the allowed content here lists the acceptable categories.\n        # It should be equal to the categories sent to the model above\n        # or a subset of it.\n        tier_1_stream = stateful_validation_adapter(\n            raw_stream,\n            validator_model=validator_model,\n            allowed_content=allowed_content,\n            source_nodes=[\n                \"generate\"\n            ],  # Only validate LLM-generated content\n            buffer_size=response_settings.initial_buffer_size,\n            error_message=context.chat_settings.MSG_WRONG_CONTENT,\n            continue_on_fail=response_settings.continue_on_fail,\n            logger=logger,\n        )\n\n    if dblogger:\n        tier_1_stream = terminal_tier1_adapter(\n            tier_1_stream, on_terminal_state=dblogger\n        )\n\n    if wfname == \"agent\":\n        # this eliminates streaming of context for the\n        # 'agent' workflow\n        tier_1_stream = tier_1_filter_messages_adapter(\n            tier_1_stream, exclude_nodes=[\"tool_caller\"]\n        )\n\n    return tier_1_stream\n</code></pre>"},{"location":"API/query/#lmm_education.query.create_chat_stringstream","title":"<code>create_chat_stringstream(querytext, history, context, *, print_context=False, validate=None, database_log=False, logger=ConsoleLogger())</code>","text":"<p>Creates and configures the chat stream.</p> <p>This function retrieves a compiled LangGraph's graph, creates the initial state, and returns a configured stream to process a user query.</p> <p>Example:</p> <pre><code>```python\ntry:\n    stream = create_chat_stream(...)\n    async for chunk in stream:\n        ...\nexcept Exception as e:\n    ...\n```\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>querytext</code> <code>str</code> <p>The user's query text</p> required <code>history</code> <code>list[dict[str, str]] | None</code> <p>List of previous message exchanges (Gradio format)</p> required <code>context</code> <code>ChatWorkflowContext</code> <p>a ChatWorkflowContext object for dependencies to be injected into the graph</p> required <code>print_context</code> <code>bool</code> <p>streams the retrieved context (default to False)</p> <code>False</code> <code>validate</code> <code>CheckResponse | Literal[False] | None</code> <p>if None, validates response using settings from context object. If False, carries out no validation. If a CheckReponse object, overrides the settings from the context object.</p> <code>None</code> <code>database_log</code> <code>bool | tuple[TextIOBase, TextIOBase] | tuple[str, str]</code> <p>if False (default), carries out no database logging of the exchanges. If True, carries out database logging with the settings defined in the context object, i.e. the files where the database is located. If a tuple of streams or file paths is provided, it uses those streams for logging.</p> <code>False</code> <code>logger</code> <code>LoggerBase</code> <p>Logger instance for info and error reporting</p> <code>ConsoleLogger()</code> <p>Yields:</p> Type Description <code>tier_3_iterator</code> <p>strings from the LLM stream</p> Behaviour <p>This function does not stream the LLM response, it only sets up the stream and returns it. Exceptions will be raised in case the stream fails to initialize (usually, due to invalid settings or failure to acquire resources).</p> Source code in <code>lmm_education/query.py</code> <pre><code>def create_chat_stringstream(\n    querytext: str,\n    history: list[dict[str, str]] | None,\n    context: ChatWorkflowContext,\n    *,\n    print_context: bool = False,\n    validate: CheckResponse | Literal[False] | None = None,\n    database_log: (\n        bool | tuple[TextIOBase, TextIOBase] | tuple[str, str]\n    ) = False,\n    logger: LoggerBase = ConsoleLogger(),\n) -&gt; tier_3_iterator:\n    \"\"\"\n    Creates and configures the chat stream.\n\n    This function retrieves a compiled LangGraph's graph, creates the\n    initial state, and returns a configured stream to process a user\n    query.\n\n    Example:\n\n        ```python\n        try:\n            stream = create_chat_stream(...)\n            async for chunk in stream:\n                ...\n        except Exception as e:\n            ...\n        ```\n\n    Args:\n        querytext: The user's query text\n        history: List of previous message exchanges (Gradio format)\n        context: a ChatWorkflowContext object for dependencies to be\n            injected into the graph\n        print_context: streams the retrieved context (default to False)\n        validate: if None, validates response using settings from\n            context object. If False, carries out no validation. If a\n            CheckReponse object, overrides the settings from\n            the context object.\n        database_log: if False (default), carries out no database\n            logging of the exchanges. If True, carries out database\n            logging with the settings defined in the context object,\n            i.e. the files where the database is located. If a tuple\n            of streams or file paths is provided, it uses those streams\n            for logging.\n        logger: Logger instance for info and error reporting\n\n    Yields:\n        strings from the LLM stream\n\n    Behaviour:\n        This function does not stream the LLM response, it only sets\n        up the stream and returns it. Exceptions will be raised in\n        case the stream fails to initialize (usually, due to invalid\n        settings or failure to acquire resources).\n    \"\"\"\n\n    stream: tier_1_iterator = create_chat_stream(\n        querytext=querytext,\n        history=history,\n        context=context,\n        stream_updates=print_context,\n        validate=validate,\n        database_log=database_log,\n        logger=logger,\n    )\n\n    if print_context:\n        return terminal_field_change_adapter(\n            stream,\n            on_field_change={\n                \"context\": lambda c: \"CONTEXT:\\n\"\n                + c\n                + \"\\nEND CONTEXT------\\n\\n\"\n            },\n        )\n    else:\n        return tier_1_to_3_adapter(\n            stream,\n        )\n</code></pre>"},{"location":"API/query/#lmm_education.query.history_to_messages","title":"<code>history_to_messages(history)</code>","text":"<p>Convert Gradio history format to LangChain messages.</p> <p>Parameters:</p> Name Type Description Default <code>history</code> <code>list[dict[str, str]]</code> <p>Gradio-format history list</p> required <p>Returns:</p> Type Description <code>list[HumanMessage | AIMessage | BaseMessage]</code> <p>List of LangChain message objects</p> Source code in <code>lmm_education/query.py</code> <pre><code>def history_to_messages(\n    history: list[dict[str, str]],\n) -&gt; list[HumanMessage | AIMessage | BaseMessage]:\n    \"\"\"\n    Convert Gradio history format to LangChain messages.\n\n    Args:\n        history: Gradio-format history list\n\n    Returns:\n        List of LangChain message objects\n    \"\"\"\n    messages: list[HumanMessage | AIMessage | BaseMessage] = []\n    for m in history:\n        role = m.get(\"role\", \"\")\n        content = m.get(\"content\", \"\")\n        if role == \"user\":\n            messages.append(HumanMessage(content=content))\n        elif role == \"assistant\":\n            messages.append(AIMessage(content=content))\n    return messages\n</code></pre>"},{"location":"API/query/#lmm_education.query.query","title":"<code>query(querystr, *, model_settings=None, chat_settings=None, print_context=False, validate_content=False, allowed_content=None, logger=ConsoleLogger())</code>","text":"<p>Synchronous generator that yields a text stream from the graph coding the RAG chatting workflow.</p> <p>This is a convenience wrapper that allows synchronous code to consume the streaming response from the language model. It creates an event loop internally to bridge the async/sync boundary.</p> <p>Parameters:</p> Name Type Description Default <code>querystr</code> <code>str</code> <p>The query text to send to the language model</p> required <code>model_settings</code> <code>LanguageModelSettings | str | None</code> <p>Language model settings (or 'major', 'minor', 'aux'). If None (default) take settings from config.toml</p> <code>None</code> <code>chat_settings</code> <code>ChatSettings | None</code> <p>Chat settings for the query</p> <code>None</code> <code>print_context</code> <code>bool</code> <p>If True, streams the RAG context</p> <code>False</code> <code>validate_content</code> <code>bool</code> <p>If True, validate response content</p> <code>False</code> <code>allowed_content</code> <code>list[str] | None</code> <p>List of allowed content types for validation</p> <code>None</code> <code>logger</code> <code>LoggerBase</code> <p>Logger instance for error reporting</p> <code>ConsoleLogger()</code> <p>Yields:</p> Name Type Description <code>str</code> <code>str</code> <p>Text chunks from the language model response</p> Behaviour <p>On successful execution, it will yield the text chunks from the language model response. If exceptions are raised, it will yield an error message. Errors may also be propagated through the logger. This behaviour is consistent with its use in a CLI.</p> Example <pre><code># assumes a RAG vector database has been created\nfor text in query(\"What is logistic regression?\"):\n    print(text, end=\"\", flush=True)\nprint()\n</code></pre> Source code in <code>lmm_education/query.py</code> <pre><code>def query(\n    querystr: str,\n    *,\n    model_settings: LanguageModelSettings | str | None = None,\n    chat_settings: ChatSettings | None = None,\n    print_context: bool = False,\n    validate_content: bool = False,\n    allowed_content: list[str] | None = None,\n    logger: LoggerBase = ConsoleLogger(),\n) -&gt; Iterator[str]:\n    \"\"\"\n    Synchronous generator that yields a text stream from the graph\n    coding the RAG chatting workflow.\n\n    This is a convenience wrapper that allows synchronous code to\n    consume the streaming response from the language model. It\n    creates an event loop internally to bridge the async/sync\n    boundary.\n\n    Args:\n        querystr: The query text to send to the language model\n        model_settings: Language model settings (or 'major', 'minor',\n            'aux'). If None (default) take settings from config.toml\n        chat_settings: Chat settings for the query\n        print_context: If True, streams the RAG context\n        validate_content: If True, validate response content\n        allowed_content: List of allowed content types for validation\n        logger: Logger instance for error reporting\n\n    Yields:\n        str: Text chunks from the language model response\n\n    Behaviour:\n        On successful execution, it will yield the text chunks from\n        the language model response. If exceptions are raised, it\n        will yield an error message. Errors may also be propagated\n        through the logger. This behaviour is consistent with its\n        use in a CLI.\n\n    Example:\n        ```python\n        # assumes a RAG vector database has been created\n        for text in query(\"What is logistic regression?\"):\n            print(text, end=\"\", flush=True)\n        print()\n        ```\n    \"\"\"\n    from .asyncutils import async_gen_to_sync_iter\n\n    # Create the async generator object\n    async_gen: AsyncIterator[str] = aquery(\n        querystr,\n        model_settings=model_settings,\n        chat_settings=chat_settings,\n        print_context=print_context,\n        validate_content=validate_content,\n        allowed_content=allowed_content or [],\n        logger=logger,\n    )\n\n    # Iterate synchronously and yield text from each chunk\n    for chunk in async_gen_to_sync_iter(async_gen):\n        yield chunk\n</code></pre>"},{"location":"API/querydb/","title":"Module querydb.py","text":"<p>This module allows retrieval of material from the vector database.</p> <p>Since retrieval is performed by the RAG code automatically (for example, in appChat), this module is meant to be used for test purposes.</p> <p>A database must have been created (for example, with the ingest module).</p> <p>Examples:</p> <pre><code># Python from the command line\n\npython -m lmm_education.querydb 'What is logistic regression?'\n</code></pre> <pre><code># from python code\nfrom lmm_education.querydb import querydb\n\nresponse = querydb('what is logistic regression?')\nprint(response)\n</code></pre> <p>Because ingest replaces the content of the database when documents are edited, you can set up an ingest-evaluate loop:</p> <pre><code># Python called from the command line\n\n# append True to ingest the file 'LogisticRegression.md'\npython -m lmm_education.ingest LogisticRegression.md True\npython -m lmm_education.querydb 'what is logistic regression?'\n</code></pre> this module is only provided for use from an <p>interactive envirnoment, such as the Python REPL. It opens the database exclusively.</p> <p>options:     show_root_heading: false</p>"},{"location":"API/querydb/#lmm_education.querydb.aquerydb","title":"<code>aquerydb(query_text, *, client=None, logger=ConsoleLogger())</code>  <code>async</code>","text":"<p>Execute a query on the database using the settings in config.toml.</p> <p>Parameters:</p> Name Type Description Default <code>query_text</code> <code>str</code> <p>a text to use as query.</p> required <code>client</code> <code>AsyncQdrantClient | None</code> <p>a QdrantClient object</p> <code>None</code> <code>logger</code> <code>LoggerBase</code> <p>a logger object. Defaults to the console.</p> <code>ConsoleLogger()</code> <p>Returns:</p> Type Description <code>str</code> <p>a string concatenating the results of the query.</p> Source code in <code>lmm_education/querydb.py</code> <pre><code>async def aquerydb(\n    query_text: str,\n    *,\n    client: AsyncQdrantClient | None = None,\n    logger: LoggerBase = ConsoleLogger(),\n) -&gt; str:\n    \"\"\"\n    Execute a query on the database using the settings in\n    config.toml.\n\n    Args:\n        query_text: a text to use as query.\n        client: a QdrantClient object\n        logger: a logger object. Defaults to the console.\n\n    Returns:\n        a string concatenating the results of the query.\n    \"\"\"\n    if not query_text:\n        logger.info(\"No query text provided\")\n        return \"\"\n\n    if len(query_text.split()) &lt; 3:\n        logger.info(\"Invalid query? \" + query_text)\n        return \"\"\n\n    settings: ConfigSettings | None = load_settings(logger=logger)\n    if not settings:\n        logger.error(\"Could not read settings\")\n        return \"\"\n\n    if client is None:\n        try:\n            client = global_async_client_from_config(settings.storage)\n        except Exception as e:\n            logger.error(f\"Could not create client: {e}\")\n            return \"\"\n\n    points: list[ScoredPoint] = []\n    retrieve_docs: bool = settings.RAG.retrieve_companion_docs\n    if retrieve_docs and not settings.database.companion_collection:\n        retrieve_docs = False\n        logger.warning(\n            \"Retrieve docs directive ignored, no companion collection\"\n        )\n    if retrieve_docs:\n        results: GroupsResult = await aquery_grouped(\n            client,\n            settings.database.collection_name,\n            settings.database.companion_collection,  # type: ignore\n            encoding_to_qdrantembedding_model(\n                settings.RAG.encoding_model\n            ),\n            settings.embeddings,\n            query_text,\n            logger=logger,\n        )\n        points = groups_to_points(results)\n    else:\n        points = await aquery(\n            client,\n            settings.database.collection_name,\n            encoding_to_qdrantembedding_model(\n                settings.RAG.encoding_model\n            ),\n            settings.embeddings,\n            query_text,\n            logger=logger,\n        )\n\n    if not points:\n        return \"No results, please check connection/database.\"\n\n    if retrieve_docs:\n        summaries = points_to_metadata(points, CTXT_SUMMARY_KEY)\n        texts = points_to_text(points)\n        whole = [\n            str(sm) + \"\\n\\n\" + txt\n            for sm, txt in zip(summaries, texts)\n        ]\n        return \"\\n-------\\n\".join(whole)\n    else:\n        return \"\\n-------\\n\".join(points_to_text(points))\n</code></pre>"},{"location":"API/querydb/#lmm_education.querydb.querydb","title":"<code>querydb(query_text, *, client=None, logger=ConsoleLogger())</code>","text":"<p>Execute a query on the database using the settings in config.toml.</p> <p>Parameters:</p> Name Type Description Default <code>query_text</code> <code>str</code> <p>a text to use as query.</p> required <code>client</code> <code>QdrantClient | None</code> <p>a QdrantClient object</p> <code>None</code> <code>logger</code> <code>LoggerBase</code> <p>a logger object. Defaults to the console.</p> <code>ConsoleLogger()</code> <p>Returns:</p> Type Description <code>str</code> <p>a string concatenating the results of the query.</p> Source code in <code>lmm_education/querydb.py</code> <pre><code>@validate_call(config={'arbitrary_types_allowed': True})\ndef querydb(\n    query_text: str,\n    *,\n    client: QdrantClient | None = None,\n    logger: LoggerBase = ConsoleLogger(),\n) -&gt; str:\n    \"\"\"\n    Execute a query on the database using the settings in\n    config.toml.\n\n    Args:\n        query_text: a text to use as query.\n        client: a QdrantClient object\n        logger: a logger object. Defaults to the console.\n\n    Returns:\n        a string concatenating the results of the query.\n    \"\"\"\n    if not query_text:\n        logger.info(\"No query text provided\")\n        return \"\"\n\n    if len(query_text.split()) &lt; 3:\n        logger.info(\"Invalid query? \" + query_text)\n        return \"\"\n\n    settings: ConfigSettings | None = load_settings(logger=logger)\n    if not settings:\n        logger.error(\"Could not read settings\")\n        return \"\"\n\n    if client is None:\n        try:\n            client = global_client_from_config(settings.storage)\n        except Exception as e:\n            logger.error(f\"Could not create client: {e}\")\n            return \"\"\n\n    points: list[ScoredPoint] = []\n    retrieve_docs: bool = settings.RAG.retrieve_companion_docs\n    if retrieve_docs and not settings.database.companion_collection:\n        logger.warning(\n            \"Retrieve docs directive ignores, no companion collection\"\n        )\n        retrieve_docs = False\n    if retrieve_docs:\n        results: GroupsResult = query_grouped(\n            client,\n            settings.database.collection_name,\n            settings.database.companion_collection,  # type: ignore\n            encoding_to_qdrantembedding_model(\n                settings.RAG.encoding_model\n            ),\n            settings.embeddings,\n            query_text,\n            logger=logger,\n        )\n        points = groups_to_points(results)\n    else:\n        points = query(\n            client,\n            settings.database.collection_name,\n            encoding_to_qdrantembedding_model(\n                settings.RAG.encoding_model\n            ),\n            settings.embeddings,\n            query_text,\n            logger=logger,\n        )\n\n    if not points:\n        return \"No results, please check connection/database.\"\n\n    if retrieve_docs:\n        summaries = points_to_metadata(points, CTXT_SUMMARY_KEY)\n        texts = points_to_text(points)\n        whole = [\n            str(sm) + \"\\n\\n\" + txt\n            for sm, txt in zip(summaries, texts)\n        ]\n        return \"\\n-------\\n\".join(whole)\n    else:\n        return \"\\n-------\\n\".join(points_to_text(points))\n</code></pre>"},{"location":"API/scan_rag/","title":"Module scan_rag.py","text":"<p>This module allows interactively scanning a markdown file to create a RAG database.</p> <p>Examples:</p> <pre><code># Python called from the console\n\npython -m lmm_education.scan_rag LogisticRegression.md\n</code></pre> <pre><code># from python code\nfrom lmm_education.scan_rag import markdown_rag\n\nmarkdown_rag('LogisticRegression.md')\n</code></pre> <p>These functions allow interactively scanning a markdown file to create a RAG database.</p> <p>Main functions:</p> <ul> <li><code>markdown_rag</code> and <code>amarkdown_rag</code>: these functions allow interactively scanning a markdown file to create a RAG database.</li> </ul> <p>options:     show_root_heading: false</p>"},{"location":"API/scan_rag/#lmm_education.scan_rag.amarkdown_rag","title":"<code>amarkdown_rag(sourcefile, *, config=None, logger=logger)</code>  <code>async</code>","text":"<p>Call scan_markdown_rag with default config settings.</p> Source code in <code>lmm_education/scan_rag.py</code> <pre><code>async def amarkdown_rag(\n    sourcefile: str | Path,\n    *,\n    config: ConfigSettings | None = None,\n    logger: LoggerBase = logger,\n) -&gt; None:\n    \"\"\"Call scan_markdown_rag with default config settings.\"\"\"\n\n    return markdown_rag(\n        sourcefile=sourcefile,\n        config=config,\n        logger=logger,\n    )\n</code></pre>"},{"location":"API/scan_rag/#lmm_education.scan_rag.markdown_rag","title":"<code>markdown_rag(sourcefile, *, config=None, logger=logger)</code>","text":"<p>Call scan_markdown_rag with default config settings.</p> Source code in <code>lmm_education/scan_rag.py</code> <pre><code>def markdown_rag(\n    sourcefile: str | Path,\n    *,\n    config: ConfigSettings | None = None,\n    logger: LoggerBase = logger,\n) -&gt; None:\n    \"\"\"Call scan_markdown_rag with default config settings.\"\"\"\n\n    if config is None:\n        config = load_settings(logger=logger)\n    if config is None:\n        logger.error(\"Could not load settings.\")\n        return\n\n    scan_markdown_rag(\n        sourcefile,\n        ScanOpts(\n            titles=config.RAG.titles,\n            questions=config.RAG.questions,\n            questions_threshold=15,\n            summaries=config.RAG.summaries,\n            summary_threshold=50,\n            language_model_settings=config.major,\n        ),\n        save=True,\n        logger=logger,\n    )\n</code></pre>"},{"location":"API/stream_adapters/","title":"Stream adapters","text":"<p>Stream adapters for processing LangGraph response streams.</p> <p>This module provides composable stream adapters that can wrap and transform async iterators of compiled LangGraph streams.</p> <p>Architecture: - Generic components work with any LangGraph workflow - Three-tier adapter system:   * Tier 1: Multi-mode adapters (operate on (mode, event) tuples)   * Tier 2: Message-only adapters (operate on (BaseMessageChunk,     metadata) tuples)   * Tier 3: string adapters (operate on str)</p> <p>Stream modes for tier 1: - \"messages\": (BaseMessageChunk, metadata) tuples for text display - \"values\": Complete state snapshot after each node execution - \"updates\": State changes {node_name: {field: value, ...}}</p> <p>Stream modes for tier 2: - \"messages\": (BaseMessageChunk, metadata) tuples for text display</p> <p>Stream mode for tier 3: - streams are limited to strings (any other information is lost).</p> <p>Tier 1 adapters work on graphs streamed via stream_mode = [\"messages\", \"values\"] or [\"messages\", \"values\", \"updates\"]. Any combination or two or three of these modes is valid, but the combination will usually include \"messages\" for streaming. Tier 2 adapters work on streams from stream_mode = \"messages\". Tier 3 streams arise from tier 1 or tier 2 streams, and stream simple strings. A stream may be filtered from tier 1 to tier 2 or tier 3, but not in the opposite direction, as information is lost during the conversion.</p> <p>A use case for tier 3 streams is to extract information from messages, metadata, or state and convey it into the string stream. A tier 3 stream will always be produced at some stage to stream content to the output.</p> <p>The stream output types are captured by type aliases:</p> <ul> <li><code>tier_1_iterator</code>: For tier 1 streams (multimodal)</li> <li><code>tier_2_iterator</code>: For tier 2 streams (messages)</li> <li><code>tier_3_iterator</code>: For tier 3 streams (strings)</li> </ul> <p>options:     show_root_heading: false</p>"},{"location":"API/stream_adapters/#lmm_education.workflows.langchain.stream_adapters--stream-setup","title":"Stream setup","text":"<p>Tier 1 stream setup:</p> <ul> <li><code>stream_graph_state</code>: Entry point with \"messages\" and \"values\" modes</li> <li><code>stream_graph_updates</code>: Entry point with \"messages\", \"values\",   and \"updates\" modes for change-reactive patterns</li> </ul> <p>Tier 1 streams may also be obtained by calling the graph .astream() function directly with the appropriate stream_modes (see the body of these functions for reference). Once initialized with the appropriate stream_mode, the stream type is preserved across all tier 1 adapters.</p> <p>Tier 2 stream setup:</p> <ul> <li><code>stream_graph_messages</code>: Entry point with \"messages\" streams.</li> </ul>"},{"location":"API/stream_adapters/#lmm_education.workflows.langchain.stream_adapters--adapters","title":"Adapters","text":"<p>Tier 1 adapters:</p> <ul> <li><code>field_change_tier_1_adapter</code>: Reacts to specific field changes via     \"updates\"</li> <li><code>terminal_tier_1_adapter</code>: De-multiplexes multi-mode stream to     messages and calls callback with terminal state. The callback is     given the final state as an argument.</li> </ul> <p>Tier 1 to tier 2 adapters:</p> <ul> <li><code>tier_1_to_2_adapter</code></li> </ul> <p>Tier 2 to tier 3 adapters:</p> <ul> <li><code>tier_2_to_3_adapter</code></li> </ul> <p>Tier 1 to tier 3 adapters:</p> <ul> <li><code>field_change_terminal_adapter</code>: Reacts to specific field changes     via \"updates\", and calls a callback to insert into the string     stream the result of the callback. It otherwise streams messages     by extracting their content. It also takes a terminal state     callback called after streaming, which is given the final state.</li> <li><code>tier_1_to_3_adapter</code></li> </ul>"},{"location":"API/stream_adapters/#lmm_education.workflows.langchain.stream_adapters.StreamableGraph","title":"<code>StreamableGraph</code>","text":"<p>               Bases: <code>Protocol[InputStateT, InputContextT]</code></p> <p>Minimal protocol for LangGraph compiled graphs.</p> <p>This protocol only specifies the astream() method that we actually use, avoiding tight coupling to LangGraph internals.</p> <p>The InputStateT and InputContextT are contravariant because they appear only in input positions (parameters), allowing a graph that accepts specific types (like ChatState, ChatWorkflowContext) to be compatible with a protocol expecting broader types (like Mapping[str, Any], BaseModel).</p> Source code in <code>lmm_education/workflows/langchain/stream_adapters.py</code> <pre><code>class StreamableGraph(Protocol[InputStateT, InputContextT]):\n    \"\"\"\n    Minimal protocol for LangGraph compiled graphs.\n\n    This protocol only specifies the astream() method that we\n    actually use, avoiding tight coupling to LangGraph internals.\n\n    The InputStateT and InputContextT are contravariant because they\n    appear only in input positions (parameters), allowing a graph\n    that accepts specific types (like ChatState, ChatWorkflowContext)\n    to be compatible with a protocol expecting broader types (like\n    Mapping[str, Any], BaseModel).\n    \"\"\"\n\n    def astream(\n        self,\n        input: InputStateT | Command[Any] | None,\n        *,\n        context: InputContextT | None = None,\n        stream_mode: StreamMode | Sequence[StreamMode] | None = None,\n        **kwargs: Any,\n    ) -&gt; AsyncIterator[Any]:\n        \"\"\"Stream graph execution with multiple output modes.\"\"\"\n        ...\n</code></pre>"},{"location":"API/stream_adapters/#lmm_education.workflows.langchain.stream_adapters.StreamableGraph.astream","title":"<code>astream(input, *, context=None, stream_mode=None, **kwargs)</code>","text":"<p>Stream graph execution with multiple output modes.</p> Source code in <code>lmm_education/workflows/langchain/stream_adapters.py</code> <pre><code>def astream(\n    self,\n    input: InputStateT | Command[Any] | None,\n    *,\n    context: InputContextT | None = None,\n    stream_mode: StreamMode | Sequence[StreamMode] | None = None,\n    **kwargs: Any,\n) -&gt; AsyncIterator[Any]:\n    \"\"\"Stream graph execution with multiple output modes.\"\"\"\n    ...\n</code></pre>"},{"location":"API/stream_adapters/#lmm_education.workflows.langchain.stream_adapters.field_change_tier_1_adapter","title":"<code>field_change_tier_1_adapter(multi_mode_stream, *, on_field_change=None, logger=ConsoleLogger())</code>  <code>async</code>","text":"<p>Tier 1 adapter that reacts to specific field changes via \"updates\" events.</p> <p>This adapter uses the \"updates\" stream mode to detect when specific state fields change, and invokes registered callbacks.</p> <p>This enables building reactive adapters that respond to specific state changes, such as: - React when \"status\" changes to \"valid\" - React when \"context\" becomes available</p> <p>Parameters:</p> Name Type Description Default <code>multi_mode_stream</code> <code>tier_1_iterator</code> <p>Source stream with (mode, event) tuples</p> required <code>on_field_change</code> <code>dict[str, Callable[[Any], Awaitable[str | None]]] | dict[str, Callable[[Any], str | None]] | None</code> <p>Dict mapping field names to async callbacks. Each callback receives the updated field value as its argument. If the callback returns a value, a deep copy of the event is created with the field value replaced by the callback's return value. If the callback returns None, the event is passed through unchanged. Callbacks are invoked when those fields are updated.</p> <code>None</code> <code>logger</code> <code>LoggerBase</code> <p>Logger to use for error logging</p> <code>ConsoleLogger()</code> <p>Yields:</p> Type Description <code>tier_1_iterator</code> <p>All (mode, event) tuples from the stream. Update events are</p> <code>tier_1_iterator</code> <p>deep-copied if any registered callback returns a non-None value.</p> Example <pre><code>async def on_context_ready(context: str):\n    logger.info(f\"Context retrieved: {len(context)} chars\")\n\nasync def on_status_changed(status: str):\n    logger.info(f\"Status changed to: {status}\")\n\nreactive_stream = field_change_tier_1_adapter(\n    multi_mode_stream,\n    on_field_change={\n        \"context\": on_context_ready,\n        \"status\": on_status_changed,\n    }\n)\n\nasync for mode, event in reactive_stream:\n    # Process events as usual\n    ...\n</code></pre> Source code in <code>lmm_education/workflows/langchain/stream_adapters.py</code> <pre><code>async def field_change_tier_1_adapter(\n    multi_mode_stream: tier_1_iterator,\n    *,\n    on_field_change: (\n        dict[str, Callable[[Any], Awaitable[str | None]]]\n        | dict[str, Callable[[Any], str | None]]\n        | None\n    ) = None,\n    logger: LoggerBase = ConsoleLogger(),\n) -&gt; tier_1_iterator:\n    \"\"\"\n    Tier 1 adapter that reacts to specific field changes via \"updates\"\n    events.\n\n    This adapter uses the \"updates\" stream mode to detect when\n    specific state fields change, and invokes registered callbacks.\n\n    This enables building reactive adapters that respond to specific\n    state changes, such as:\n    - React when \"status\" changes to \"valid\"\n    - React when \"context\" becomes available\n\n    Args:\n        multi_mode_stream: Source stream with (mode, event) tuples\n        on_field_change: Dict mapping field names to async callbacks.\n            Each callback receives the updated field value as its\n            argument. If the callback returns a value, a deep copy of\n            the event is created with the field value replaced by the\n            callback's return value. If the callback returns None, the\n            event is passed through unchanged.\n            Callbacks are invoked when those fields are updated.\n        logger: Logger to use for error logging\n\n    Yields:\n        All (mode, event) tuples from the stream. Update events are\n        deep-copied if any registered callback returns a non-None value.\n\n    Example:\n        ```python\n        async def on_context_ready(context: str):\n            logger.info(f\"Context retrieved: {len(context)} chars\")\n\n        async def on_status_changed(status: str):\n            logger.info(f\"Status changed to: {status}\")\n\n        reactive_stream = field_change_tier_1_adapter(\n            multi_mode_stream,\n            on_field_change={\n                \"context\": on_context_ready,\n                \"status\": on_status_changed,\n            }\n        )\n\n        async for mode, event in reactive_stream:\n            # Process events as usual\n            ...\n        ```\n    \"\"\"\n    if on_field_change is None:\n        async for mode, event in multi_mode_stream:\n            yield (mode, event)\n        return\n\n    async for mode, event in multi_mode_stream:\n        # React to field changes via \"updates\" events\n        if mode == \"updates\" and isinstance(event, dict):\n            # Deep copy to avoid modifying the original event\n            event_copy = copy.deepcopy(event)  # type: ignore\n            modified = False\n\n            # event format: {node_name: {field_name: value, ...}}\n            for node_name, changes in event_copy.items():  # type: ignore[union-attr]\n                if isinstance(changes, dict):\n                    for field, value in changes.items():  # type: ignore[union-attr]\n                        if field in on_field_change:\n                            callback_fun = on_field_change[field]\n                            try:\n                                content: str | None = None\n                                if inspect.iscoroutinefunction(\n                                    callback_fun\n                                ):\n                                    content = await callback_fun(\n                                        value\n                                    )\n                                else:\n                                    content = callback_fun(value)  # type: ignore\n                                if content is not None:\n                                    # Actually modify the field in the copy\n                                    changes[field] = content\n                                    modified = True\n                            except Exception as e:\n                                # Log the error but don't stop stream\n                                logger.error(\n                                    f\"Error in field_change_adapter: \"\n                                    f\"{e}\"\n                                )\n                                pass\n\n            # Yield the modified copy if we made changes, otherwise the original\n            yield (mode, event_copy if modified else event)\n        else:\n            # For non-update events, yield unchanged\n            yield (mode, event)\n</code></pre>"},{"location":"API/stream_adapters/#lmm_education.workflows.langchain.stream_adapters.stream_graph_messages","title":"<code>stream_graph_messages(graph, initial_state, context)</code>","text":"<p>Entry point for streaming a LangGraph workflow with messages output.</p> <p>This function configures the graph to stream only messages, enabling downstream adapters to:</p> <ul> <li>Extract messages for display</li> </ul> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>StreamableGraph[Any, Any]</code> <p>Compiled LangGraph workflow</p> required <code>initial_state</code> <code>Mapping[str, Any]</code> <p>Initial state to start execution</p> required <code>context</code> <code>BaseModel</code> <p>Dependency injection context for the workflow</p> required <p>Returns:</p> Type Description <code>tier_2_iterator</code> <p>AsyncIterator yielding 'event' tuples of (BaseMessageChunk,</p> <code>tier_2_iterator</code> <p>dict[str, Any]) for messages (tier 2 iterator)</p> Example <pre><code>raw_stream = stream_graph_messages(workflow, initial_state,\n    context)\n\n# Apply adapters\n...\n\n# Consume for display\nasync for chunk, _ in raw_stream:\n    print(chunk.text, end=\"\")\n</code></pre> Source code in <code>lmm_education/workflows/langchain/stream_adapters.py</code> <pre><code>def stream_graph_messages(\n    graph: StreamableGraph[Any, Any],  # types checked at other args\n    initial_state: Mapping[str, Any],\n    context: BaseModel,\n) -&gt; tier_2_iterator:\n    \"\"\"\n    Entry point for streaming a LangGraph workflow with messages\n    output.\n\n    This function configures the graph to stream only messages,\n    enabling downstream adapters to:\n\n    - Extract messages for display\n\n    Args:\n        graph: Compiled LangGraph workflow\n        initial_state: Initial state to start execution\n        context: Dependency injection context for the workflow\n\n    Returns:\n        AsyncIterator yielding 'event' tuples of (BaseMessageChunk,\n        dict[str, Any]) for messages (tier 2 iterator)\n\n    Example:\n        ```python\n        raw_stream = stream_graph_messages(workflow, initial_state,\n            context)\n\n        # Apply adapters\n        ...\n\n        # Consume for display\n        async for chunk, _ in raw_stream:\n            print(chunk.text, end=\"\")\n        ```\n    \"\"\"\n    return graph.astream(\n        initial_state,\n        stream_mode=\"messages\",\n        context=context,\n    )\n</code></pre>"},{"location":"API/stream_adapters/#lmm_education.workflows.langchain.stream_adapters.stream_graph_state","title":"<code>stream_graph_state(graph, initial_state, context)</code>","text":"<p>Entry point for streaming a LangGraph workflow with multi-mode output.</p> <p>This function configures the graph to stream both messages and state updates, enabling downstream adapters to: - Access and modify state (Tier 1 adapters) - Capture terminal state for logging - Extract messages for display</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>StreamableGraph[Any, Any]</code> <p>Compiled LangGraph workflow</p> required <code>initial_state</code> <code>Mapping[str, Any]</code> <p>Initial state to start execution</p> required <code>context</code> <code>BaseModel</code> <p>Dependency injection context for the workflow</p> required <p>Returns:</p> Name Type Description <code>tier_1_iterator</code> <p>AsyncIterator yielding (mode, event) tuples (tier 1 iterator)</p> <code>where</code> <code>tier_1_iterator</code> <code>tier_1_iterator</code> <ul> <li>mode is \"messages\" or \"values\"</li> </ul> <code>tier_1_iterator</code> <ul> <li>event is (BaseMessageChunk, metadata) for messages</li> </ul> <code>tier_1_iterator</code> <ul> <li>event is StateT for values (a typed dictionary)</li> </ul> Example <pre><code>raw_stream = stream_graph_state(workflow, initial_state,\n                                context)\n\n# Apply adapters\nvalidated_stream = stateful_validation_adapter(raw_stream,\n    ...)\nterminal_stream = terminal_tier_1_adapter(validated_stream,\n    on_terminal_state=log_fn)\ntext_stream = tier_1_to_3_adapter(terminal_stream)\n\n# Consume for display\nasync for chunk in text_stream:\n    print(chunk.text, end=\"\")\n</code></pre> Source code in <code>lmm_education/workflows/langchain/stream_adapters.py</code> <pre><code>def stream_graph_state(\n    graph: StreamableGraph[Any, Any],  # types checked at other args\n    initial_state: Mapping[str, Any],\n    context: BaseModel,\n) -&gt; tier_1_iterator:\n    \"\"\"\n    Entry point for streaming a LangGraph workflow with multi-mode\n    output.\n\n    This function configures the graph to stream both messages and\n    state updates, enabling downstream adapters to:\n    - Access and modify state (Tier 1 adapters)\n    - Capture terminal state for logging\n    - Extract messages for display\n\n    Args:\n        graph: Compiled LangGraph workflow\n        initial_state: Initial state to start execution\n        context: Dependency injection context for the workflow\n\n    Returns:\n        AsyncIterator yielding (mode, event) tuples (tier 1 iterator)\n        where:\n        - mode is \"messages\" or \"values\"\n        - event is (BaseMessageChunk, metadata) for messages\n        - event is StateT for values (a typed dictionary)\n\n    Example:\n        ```python\n        raw_stream = stream_graph_state(workflow, initial_state,\n                                        context)\n\n        # Apply adapters\n        validated_stream = stateful_validation_adapter(raw_stream,\n            ...)\n        terminal_stream = terminal_tier_1_adapter(validated_stream,\n            on_terminal_state=log_fn)\n        text_stream = tier_1_to_3_adapter(terminal_stream)\n\n        # Consume for display\n        async for chunk in text_stream:\n            print(chunk.text, end=\"\")\n        ```\n    \"\"\"\n    return graph.astream(\n        initial_state,\n        stream_mode=[\"messages\", \"values\"],\n        context=context,\n    )\n</code></pre>"},{"location":"API/stream_adapters/#lmm_education.workflows.langchain.stream_adapters.stream_graph_updates","title":"<code>stream_graph_updates(graph, initial_state, context)</code>","text":"<p>Entry point for streaming a LangGraph workflow with state updates.</p> <p>This function configures the graph to stream messages, full state values, and differential state updates. This enables: - Tier 1 adapters to access and modify state - Change-reactive adapters to respond to specific field updates - Terminal state capture for logging - Message extraction for display</p> <p>The \"updates\" stream mode provides differential updates in the format:     (\"updates\", {node_name: {changed_field: new_value, ...}})</p> <p>This is more granular than \"values\" (which provides the complete state) and enables building adapters that react to specific state changes.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>StreamableGraph[Any, Any]</code> <p>Compiled LangGraph workflow</p> required <code>initial_state</code> <code>Mapping[str, Any]</code> <p>Initial state to start execution</p> required <code>context</code> <code>BaseModel</code> <p>Dependency injection context for the workflow</p> required <p>Returns:</p> Name Type Description <code>tier_1_iterator</code> <p>AsyncIterator yielding (mode, event) tuples (tier 1 iterator)</p> <code>where</code> <code>tier_1_iterator</code> <code>tier_1_iterator</code> <ul> <li>mode is \"messages\", \"values\", or \"updates\"</li> </ul> <code>tier_1_iterator</code> <ul> <li>event is (BaseMessageChunk, metadata) for messages</li> </ul> <code>tier_1_iterator</code> <ul> <li>event is full StateT for values</li> </ul> <code>tier_1_iterator</code> <ul> <li>event is UpdatesEvent for updates</li> </ul> Example <pre><code>raw_stream = stream_graph_updates(workflow, initial_state,\n    context)\n\n# Apply change-reactive adapters\nreactive_stream = field_change_tier_1_adapter(\n    raw_stream,\n    on_field_change={\n        \"status\": handle_status_change,\n        \"context\": handle_context_ready,\n    }\n)\n\n# Apply other adapters\nterminal_stream = terminal_tier_1_adapter(reactive_stream,\n    on_terminal_state=log_fn)\ntext_stream = tier_1_to_3_adapter(terminal_stream)\n\n# Consume for display\nasync for chunk in text_stream:\n    print(chunk, end=\"\")\n</code></pre> Source code in <code>lmm_education/workflows/langchain/stream_adapters.py</code> <pre><code>def stream_graph_updates(\n    graph: StreamableGraph[Any, Any],  # types checked at other args\n    initial_state: Mapping[str, Any],\n    context: BaseModel,\n) -&gt; tier_1_iterator:\n    \"\"\"\n    Entry point for streaming a LangGraph workflow with state updates.\n\n    This function configures the graph to stream messages, full state\n    values, and differential state updates. This enables:\n    - Tier 1 adapters to access and modify state\n    - Change-reactive adapters to respond to specific field updates\n    - Terminal state capture for logging\n    - Message extraction for display\n\n    The \"updates\" stream mode provides differential updates in the\n    format:\n        (\"updates\", {node_name: {changed_field: new_value, ...}})\n\n    This is more granular than \"values\" (which provides the complete\n    state) and enables building adapters that react to specific state\n    changes.\n\n    Args:\n        graph: Compiled LangGraph workflow\n        initial_state: Initial state to start execution\n        context: Dependency injection context for the workflow\n\n    Returns:\n        AsyncIterator yielding (mode, event) tuples (tier 1 iterator)\n        where:\n        - mode is \"messages\", \"values\", or \"updates\"\n        - event is (BaseMessageChunk, metadata) for messages\n        - event is full StateT for values\n        - event is UpdatesEvent for updates\n\n    Example:\n        ```python\n        raw_stream = stream_graph_updates(workflow, initial_state,\n            context)\n\n        # Apply change-reactive adapters\n        reactive_stream = field_change_tier_1_adapter(\n            raw_stream,\n            on_field_change={\n                \"status\": handle_status_change,\n                \"context\": handle_context_ready,\n            }\n        )\n\n        # Apply other adapters\n        terminal_stream = terminal_tier_1_adapter(reactive_stream,\n            on_terminal_state=log_fn)\n        text_stream = tier_1_to_3_adapter(terminal_stream)\n\n        # Consume for display\n        async for chunk in text_stream:\n            print(chunk, end=\"\")\n        ```\n    \"\"\"\n    return graph.astream(\n        initial_state,\n        stream_mode=[\"messages\", \"values\", \"updates\"],\n        context=context,\n    )\n</code></pre>"},{"location":"API/stream_adapters/#lmm_education.workflows.langchain.stream_adapters.terminal_field_change_adapter","title":"<code>terminal_field_change_adapter(multi_mode_stream, source_nodes=None, exclude_nodes=None, *, on_field_change=None, on_terminal_state=None, logger=ConsoleLogger())</code>  <code>async</code>","text":"<p>This adapter captures the 'messages' stream and converts it to a tier 3 iterator (a stream of strings). It also captures the 'updates' stream and invokes callbacks registered in on_field_change, if provided. These callbacks return strings that are injected into the tier 3 stream.</p> <p>The callback given to on_terminal_state is called after streaming (may be used, for example, for logging the exchange). The output of this callback is not injected into the tier 3 stream. The stream must have been created with the 'values' mode for this callback to be called.</p> <p>This adapter: - Extracts and yields only the text of messages chunks and     the return of the on_field_change callback. Any other     information from the 'updates' stream is discarded. - Captures the terminal state (last \"values\" event), and     invokes an optional callback with the terminal state. - State information is discarded in the output stream.</p> <p>Parameters:</p> Name Type Description Default <code>multi_mode_stream</code> <code>tier_1_iterator</code> <p>Source stream with (mode, event) tuples</p> required <code>source_nodes</code> <code>list[str] | None</code> <p>the source nodes one wants to stream. If omitted or None, all nodes will be streamed (only concerns the 'messages' stream).</p> <code>None</code> <code>exclude_nodes</code> <code>list[str] | None</code> <p>the source nodes one wants to exclude from streaming. If specified, overrides source_nodes.</p> <code>None</code> <code>on_field_change</code> <code>dict[str, Callable[[Any], Awaitable[str | None]]] | dict[str, Callable[[Any], str | None]] | None</code> <p>Dict mapping field names to async callbacks. Each callback receives the updated field value as its argument, and returns a string that is injected into the tier 3 stream. If the callback returns None, no string is injected. Callbacks are invoked when those fields are updated.</p> <code>None</code> <code>on_terminal_state</code> <code>Callable[[StateT], Any] | Callable[[StateT], Awaitable[Any]] | None</code> <p>Optional callback for terminal state. The callback receives Mapping[str, Any] type.</p> <code>None</code> <code>logger</code> <code>LoggerBase</code> <p>Logger to use for error logging</p> <code>ConsoleLogger()</code> <p>Yields:</p> Type Description <code>tier_3_iterator</code> <p>strings (tier 3 iterator)</p> Example <pre><code>async def on_context_ready(context: str) -&gt; str:\n    return f\"Context retrieved: {len(context)} chars\"\n\nasync def on_status_changed(status: str) -&gt; str:\n    return f\"Status changed to: {status}\"\n\ntexts = terminal_field_change_adapter(\n    multi_mode_stream,\n    on_field_change={\n        \"context\": on_context_ready,\n        \"status\": on_status_changed,\n    },\n    on_terminal_state=lambda s: logger.log(s),\n)\n\nasync for text in texts:\n    yield text\n</code></pre> Source code in <code>lmm_education/workflows/langchain/stream_adapters.py</code> <pre><code>async def terminal_field_change_adapter(\n    multi_mode_stream: tier_1_iterator,\n    source_nodes: list[str] | None = None,\n    exclude_nodes: list[str] | None = None,\n    *,\n    on_field_change: (\n        dict[str, Callable[[Any], Awaitable[str | None]]]\n        | dict[str, Callable[[Any], str | None]]\n        | None\n    ) = None,\n    on_terminal_state: (\n        Callable[[StateT], Any]\n        | Callable[[StateT], Awaitable[Any]]\n        | None\n    ) = None,\n    logger: LoggerBase = ConsoleLogger(),\n) -&gt; tier_3_iterator:\n    \"\"\"\n    This adapter captures the 'messages' stream and converts it to a\n    tier 3 iterator (a stream of strings). It also captures the\n    'updates' stream and invokes callbacks registered in\n    on_field_change, if provided. These callbacks return strings\n    that are injected into the tier 3 stream.\n\n    The callback given to on_terminal_state is called after\n    streaming (may be used, for example, for logging the exchange).\n    The output of this callback is not injected into the tier 3\n    stream. The stream must have been created with the 'values' mode\n    for this callback to be called.\n\n    This adapter:\n    - Extracts and yields only the text of messages chunks and\n        the return of the on_field_change callback. Any other\n        information from the 'updates' stream is discarded.\n    - Captures the terminal state (last \"values\" event), and\n        invokes an optional callback with the terminal state.\n    - State information is discarded in the output stream.\n\n    Args:\n        multi_mode_stream: Source stream with (mode, event) tuples\n        source_nodes: the source nodes one wants to stream. If\n            omitted or None, all nodes will be streamed (only\n            concerns the 'messages' stream).\n        exclude_nodes: the source nodes one wants to exclude from\n            streaming. If specified, overrides source_nodes.\n        on_field_change: Dict mapping field names to async callbacks.\n            Each callback receives the updated field value as its\n            argument, and returns a string that is injected into the\n            tier 3 stream. If the callback returns None, no string is\n            injected.\n            Callbacks are invoked when those fields are updated.\n        on_terminal_state: Optional callback for terminal state.\n            The callback receives Mapping[str, Any] type.\n        logger: Logger to use for error logging\n\n    Yields:\n        strings (tier 3 iterator)\n\n    Example:\n        ```python\n        async def on_context_ready(context: str) -&gt; str:\n            return f\"Context retrieved: {len(context)} chars\"\n\n        async def on_status_changed(status: str) -&gt; str:\n            return f\"Status changed to: {status}\"\n\n        texts = terminal_field_change_adapter(\n            multi_mode_stream,\n            on_field_change={\n                \"context\": on_context_ready,\n                \"status\": on_status_changed,\n            },\n            on_terminal_state=lambda s: logger.log(s),\n        )\n\n        async for text in texts:\n            yield text\n        ```\n    \"\"\"\n\n    final_state: StateT | None = None\n    async for mode, event in multi_mode_stream:\n        if mode == \"messages\":\n            if exclude_nodes:\n                chunk, metadata = event  # Extract chunk and metadata\n                try:\n                    if metadata[\"langgraph_node\"] in exclude_nodes:\n                        continue\n                except Exception as e:\n                    logger.error(\n                        f\"Could not retrieve langgraph_node property\"\n                        f\" in terminal_field_change_adapter:\\n{e}\"\n                    )\n                    yield chunk.text\n            elif source_nodes:\n                chunk, metadata = event  # Extract chunk and metadata\n                try:\n                    if metadata[\"langgraph_node\"] in source_nodes:\n                        yield chunk.text\n                except Exception as e:\n                    logger.error(\n                        f\"Could not retrieve langgraph_node property\"\n                        f\" in terminal_field_change_adapter:\\n{e}\"\n                    )\n                    yield chunk.text\n            else:\n                chunk, _ = event\n                yield chunk.text\n        elif (\n            mode == \"updates\"\n            and isinstance(event, dict)\n            and on_field_change\n        ):\n            # event format: {node_name: {field_name: value, ...}}\n            for node_name, changes in event.items():  # type: ignore[union-attr]\n                if isinstance(changes, dict):\n                    for field, value in changes.items():  # type: ignore[union-attr]\n                        if field in on_field_change:\n                            callback_fun = on_field_change[field]\n                            try:\n                                content: str | None = None\n                                if inspect.iscoroutinefunction(\n                                    callback_fun\n                                ):\n                                    content = await callback_fun(\n                                        value\n                                    )\n                                else:\n                                    content = callback_fun(value)  # type: ignore\n                                # Only yield if callback returned a value\n                                if content is not None:\n                                    yield content\n                            except Exception as e:\n                                # Log the error but don't stop stream\n                                logger.error(\n                                    f\"Error in field_change_adapter: \"\n                                    f\"{e}\"\n                                )\n                                pass\n        elif mode == \"values\":\n            final_state = event  # type: ignore\n        else:\n            pass\n\n    if on_terminal_state and final_state:\n        if inspect.iscoroutinefunction(on_terminal_state):\n            schedule_task(\n                on_terminal_state(final_state),\n                error_callback=lambda e: logger.error(\n                    f\"Error in on_terminal_state: {e}\"\n                ),\n            )\n        else:\n            try:\n                on_terminal_state(final_state)\n            except Exception as e:\n                # log error but do not stop stream\n                logger.error(f\"Error in on_terminal_state: {e}\")\n</code></pre>"},{"location":"API/stream_adapters/#lmm_education.workflows.langchain.stream_adapters.terminal_tier1_adapter","title":"<code>terminal_tier1_adapter(multi_mode_stream, *, on_terminal_state=None, logger=ConsoleLogger())</code>  <code>async</code>","text":"<p>Tier 1 terminal adapter that calls a callback with the terminal state. The tier 1 iterator must have been created with the \"values\" stream mode for the on_terminal_state callback to be called.</p> <p>This adapter: - Captures the terminal state (last \"values\" event) - Invokes an optional callback with the terminal state</p> <p>Parameters:</p> Name Type Description Default <code>multi_mode_stream</code> <code>tier_1_iterator</code> <p>Source stream with (mode, event) tuples</p> required <code>on_terminal_state</code> <code>Callable[[StateT], Any] | Callable[[StateT], Awaitable[Any]] | None</code> <p>Optional callback for terminal state. The callback receives Mapping[str, Any] type.</p> <code>None</code> <p>Yields:</p> Type Description <code>tier_1_iterator</code> <p>(mode, event) tuples from the stream</p> Example <pre><code>messages = terminal_tier1_adapter(\n    multi_mode_stream,\n    on_terminal_state=lambda s: logger.log(s),\n)\nasync for mode, event in messages:\n    if mode == 'messages':\n        ...\n</code></pre> Source code in <code>lmm_education/workflows/langchain/stream_adapters.py</code> <pre><code>async def terminal_tier1_adapter(\n    multi_mode_stream: tier_1_iterator,\n    *,\n    on_terminal_state: (\n        Callable[[StateT], Any]\n        | Callable[[StateT], Awaitable[Any]]\n        | None\n    ) = None,\n    logger: LoggerBase = ConsoleLogger(),\n) -&gt; tier_1_iterator:\n    \"\"\"\n    Tier 1 terminal adapter that calls a callback with the terminal\n    state. The tier 1 iterator must have been created with the\n    \"values\" stream mode for the on_terminal_state callback to be\n    called.\n\n    This adapter:\n    - Captures the terminal state (last \"values\" event)\n    - Invokes an optional callback with the terminal state\n\n    Args:\n        multi_mode_stream: Source stream with (mode, event) tuples\n        on_terminal_state: Optional callback for terminal state.\n            The callback receives Mapping[str, Any] type.\n\n    Yields:\n        (mode, event) tuples from the stream\n\n    Example:\n        ```python\n        messages = terminal_tier1_adapter(\n            multi_mode_stream,\n            on_terminal_state=lambda s: logger.log(s),\n        )\n        async for mode, event in messages:\n            if mode == 'messages':\n                ...\n        ```\n    \"\"\"\n\n    final_state: StateT | None = None\n    async for mode, event in multi_mode_stream:\n        if mode == \"values\":\n            final_state = event\n\n        yield (mode, event)\n\n    if on_terminal_state and final_state:\n        if inspect.iscoroutinefunction(on_terminal_state):\n            schedule_task(\n                on_terminal_state(final_state),\n                error_callback=lambda e: logger.error(\n                    f\"Error in on_terminal_state: {e}\"\n                ),\n            )\n        else:\n            try:\n                on_terminal_state(final_state)\n            except Exception as e:\n                # log error but do not stop stream\n                logger.error(f\"Error in on_terminal_state: {e}\")\n</code></pre>"},{"location":"API/stream_adapters/#lmm_education.workflows.langchain.stream_adapters.tier_1_filter_messages_adapter","title":"<code>tier_1_filter_messages_adapter(stream, exclude_nodes)</code>  <code>async</code>","text":"<p>Filter out message chunks from the stream that come from the excluded nodes (for example, to exclude tool messages).</p> <p>Parameters:</p> Name Type Description Default <code>stream</code> <code>tier_1_iterator</code> <p>a multimodal tier 1 iterator</p> required <code>exclude_nodes</code> <code>list[str]</code> <p>a string list with the names of the nodes that should be excluded from the stream (only messages stream)</p> required Source code in <code>lmm_education/workflows/langchain/stream_adapters.py</code> <pre><code>async def tier_1_filter_messages_adapter(\n    stream: tier_1_iterator, exclude_nodes: list[str]\n) -&gt; tier_1_iterator:\n    \"\"\"\n    Filter out message chunks from the stream that come\n    from the excluded nodes (for example, to exclude tool messages).\n\n    Args:\n        stream: a multimodal tier 1 iterator\n        exclude_nodes: a string list with the names of the nodes\n            that should be excluded from the stream (only messages\n            stream)\n    \"\"\"\n    async for mode, event in stream:\n        if mode == \"messages\":\n            _, metadata = event\n            if metadata.get(\"langgraph_node\") in exclude_nodes:\n                continue\n\n        yield (mode, event)\n</code></pre>"},{"location":"API/stream_adapters/#lmm_education.workflows.langchain.stream_adapters.tier_1_to_2_adapter","title":"<code>tier_1_to_2_adapter(multi_mode_stream, source_nodes=None, *, logger=ConsoleLogger())</code>  <code>async</code>","text":"<p>Terminal adapter: filters multi-mode stream to messages only. Non-message events are discarded.</p> <p>Parameters:</p> Name Type Description Default <code>multi_mode_stream</code> <code>tier_1_iterator</code> <p>Source stream with (mode, event) tuples</p> required <code>source_nodes</code> <code>list[str] | None</code> <p>the source nodes one wants to stream. If omitted or None, all nodes will be streamed</p> <code>None</code> <code>logger</code> <code>LoggerBase</code> <p>logger to use for error logging</p> <code>ConsoleLogger()</code> <p>Yields:</p> Type Description <code>tier_2_iterator</code> <p>BaseMessageChunk, metadata tuples (tier 2 iterator)</p> Example <pre><code>messages = tier_1_to_2_adapter(multi_mode_stream)\n\nasync for chunk, _ in messages:\n    yield chunk.text\n</code></pre> Source code in <code>lmm_education/workflows/langchain/stream_adapters.py</code> <pre><code>async def tier_1_to_2_adapter(\n    multi_mode_stream: tier_1_iterator,\n    source_nodes: list[str] | None = None,\n    *,\n    logger: LoggerBase = ConsoleLogger(),\n) -&gt; tier_2_iterator:\n    \"\"\"\n    Terminal adapter: filters multi-mode stream to messages only.\n    Non-message events are discarded.\n\n    Args:\n        multi_mode_stream: Source stream with (mode, event) tuples\n        source_nodes: the source nodes one wants to stream. If\n            omitted or None, all nodes will be streamed\n        logger: logger to use for error logging\n\n    Yields:\n        BaseMessageChunk, metadata tuples (tier 2 iterator)\n\n    Example:\n        ```python\n        messages = tier_1_to_2_adapter(multi_mode_stream)\n\n        async for chunk, _ in messages:\n            yield chunk.text\n        ```\n    \"\"\"\n    if source_nodes:\n        async for mode, event in multi_mode_stream:\n            if mode == \"messages\":\n                try:\n                    _, metadata = event\n                    if metadata[\"langgraph_node\"] in source_nodes:\n                        yield event\n                except Exception as e:\n                    logger.error(\n                        f\"Could not retrieve langgraph_node property\"\n                        f\" in tier_1_to_2_adapter:\\n{e}\"\n                    )\n                    yield event\n            else:\n                pass\n    else:\n        async for mode, event in multi_mode_stream:\n            if mode == \"messages\":\n                yield event\n            else:\n                pass\n</code></pre>"},{"location":"API/stream_adapters/#lmm_education.workflows.langchain.stream_adapters.tier_1_to_3_adapter","title":"<code>tier_1_to_3_adapter(multi_mode_stream, source_nodes=None, exclude_nodes=None, *, logger=ConsoleLogger())</code>  <code>async</code>","text":"<p>Transforms a tier 1 iterator into a tier 3 iterator. All information from the 'updates' and 'values' streams is discarded.</p> <p>Parameters:</p> Name Type Description Default <code>multi_mode_stream</code> <code>tier_1_iterator</code> <p>Source stream with (mode, event) tuples</p> required <code>source_nodes</code> <code>list[str] | None</code> <p>the source nodes one wants to stream. If omitted or None, all nodes will be streamed (only concerns the 'messages' stream).</p> <code>None</code> <code>exclude_nodes</code> <code>list[str] | None</code> <p>the source nodes one wants to exclude from streaming. If specified, overrides source_nodes.</p> <code>None</code> <code>logger</code> <code>LoggerBase</code> <p>Logger to use for error logging</p> <code>ConsoleLogger()</code> <p>Yields:</p> Type Description <code>tier_3_iterator</code> <p>strings (tier 3 iterator)</p> Example <pre><code>messages = tier_1_to_3_adapter(multi_mode_stream)\n\nasync for chunk in messages:\n    yield chunk\n</code></pre> Source code in <code>lmm_education/workflows/langchain/stream_adapters.py</code> <pre><code>async def tier_1_to_3_adapter(\n    multi_mode_stream: tier_1_iterator,\n    source_nodes: list[str] | None = None,\n    exclude_nodes: list[str] | None = None,\n    *,\n    logger: LoggerBase = ConsoleLogger(),\n) -&gt; tier_3_iterator:\n    \"\"\"\n    Transforms a tier 1 iterator into a tier 3 iterator. All\n    information from the 'updates' and 'values' streams is discarded.\n\n    Args:\n        multi_mode_stream: Source stream with (mode, event) tuples\n        source_nodes: the source nodes one wants to stream. If\n            omitted or None, all nodes will be streamed (only concerns\n            the 'messages' stream).\n        exclude_nodes: the source nodes one wants to exclude from\n            streaming. If specified, overrides source_nodes.\n        logger: Logger to use for error logging\n\n    Yields:\n        strings (tier 3 iterator)\n\n    Example:\n        ```python\n        messages = tier_1_to_3_adapter(multi_mode_stream)\n\n        async for chunk in messages:\n            yield chunk\n        ```\n    \"\"\"\n    async for mode, event in multi_mode_stream:\n        if mode == \"messages\":\n            if exclude_nodes:\n                chunk, metadata = event  # Extract chunk and metadata\n                try:\n                    if metadata[\"langgraph_node\"] in exclude_nodes:\n                        continue\n                except Exception as e:\n                    logger.error(\n                        f\"Could not retrieve langgraph_node property\"\n                        f\" in terminal_field_change_adapter:\\n{e}\"\n                    )\n                    yield chunk.text\n            elif source_nodes:\n                chunk, metadata = event  # Extract chunk and metadata\n                try:\n                    if metadata[\"langgraph_node\"] in source_nodes:\n                        yield chunk.text\n                except Exception as e:\n                    logger.error(\n                        f\"Could not retrieve langgraph_node property\"\n                        f\" in terminal_field_change_adapter:\\n{e}\"\n                    )\n                    yield chunk.text\n            else:\n                chunk, _ = event\n                yield chunk.text\n        else:\n            pass\n</code></pre>"},{"location":"API/stream_adapters/#lmm_education.workflows.langchain.stream_adapters.tier_2_filter_messages_adapter","title":"<code>tier_2_filter_messages_adapter(stream, exclude_nodes)</code>  <code>async</code>","text":"<p>Filter out message chunks from the stream that come from the excluded nodes (for example, to exclude tool messages).</p> <p>Parameters:</p> Name Type Description Default <code>stream</code> <code>tier_2_iterator</code> <p>a chunk, metadata tier 2 iterator</p> required <code>exclude_nodes</code> <code>list[str]</code> <p>a string list with the names of the nodes that should be excluded from the stream</p> required Source code in <code>lmm_education/workflows/langchain/stream_adapters.py</code> <pre><code>async def tier_2_filter_messages_adapter(\n    stream: tier_2_iterator, exclude_nodes: list[str]\n) -&gt; tier_2_iterator:\n    \"\"\"\n    Filter out message chunks from the stream that come\n    from the excluded nodes (for example, to exclude tool messages).\n\n    Args:\n        stream: a chunk, metadata tier 2 iterator\n        exclude_nodes: a string list with the names of the nodes\n            that should be excluded from the stream\n    \"\"\"\n    async for chunk, metadata in stream:\n        if metadata.get(\"langgraph_node\") in exclude_nodes:\n            continue\n        yield chunk, metadata\n</code></pre>"},{"location":"API/stream_adapters/#lmm_education.workflows.langchain.stream_adapters.tier_2_to_3_adapter","title":"<code>tier_2_to_3_adapter(messages_stream, source_nodes=None, exclude_nodes=None, *, logger=ConsoleLogger())</code>  <code>async</code>","text":"<p>Transforms a tier 2 iterator into a tier 3 iterator. 'metadata' information from the tier 2 iterator is discarded.</p> <p>Parameters:</p> Name Type Description Default <code>messages_stream</code> <code>tier_2_iterator</code> <p>Source stream with (chunk, metadata) tuples</p> required <code>source_nodes</code> <code>list[str] | None</code> <p>the source nodes one wants to stream. If omitted or None, all nodes will be streamed.</p> <code>None</code> <code>exclude_nodes</code> <code>list[str] | None</code> <p>the source nodes one wants to exclude from streaming. If specified, overrides source_nodes.</p> <code>None</code> <code>logger</code> <code>LoggerBase</code> <p>Logger to use for logging</p> <code>ConsoleLogger()</code> <p>Yields:</p> Type Description <code>tier_3_iterator</code> <p>strings (tier 3 iterator)</p> Example <p>```python messages = tier_2_to_3_adapter(messages_stream)</p> <p>async for chunk in messages:     yield chunk</p> Source code in <code>lmm_education/workflows/langchain/stream_adapters.py</code> <pre><code>async def tier_2_to_3_adapter(\n    messages_stream: tier_2_iterator,\n    source_nodes: list[str] | None = None,\n    exclude_nodes: list[str] | None = None,\n    *,\n    logger: LoggerBase = ConsoleLogger(),\n) -&gt; tier_3_iterator:\n    \"\"\"\n    Transforms a tier 2 iterator into a tier 3 iterator. 'metadata'\n    information from the tier 2 iterator is discarded.\n\n    Args:\n        messages_stream: Source stream with (chunk, metadata) tuples\n        source_nodes: the source nodes one wants to stream. If\n            omitted or None, all nodes will be streamed.\n        exclude_nodes: the source nodes one wants to exclude from\n            streaming. If specified, overrides source_nodes.\n        logger: Logger to use for logging\n\n    Yields:\n        strings (tier 3 iterator)\n\n    Example:\n        ```python\n        messages = tier_2_to_3_adapter(messages_stream)\n\n        async for chunk in messages:\n            yield chunk\n    \"\"\"\n    async for chunk, metadata in messages_stream:\n        if exclude_nodes:\n            try:\n                if metadata[\"langgraph_node\"] in exclude_nodes:\n                    continue\n            except Exception as e:\n                logger.error(\n                    f\"Could not retrieve langgraph_node property\"\n                    f\" in tier_2_to_3_adapter:\\n{e}\"\n                )\n                continue\n        elif source_nodes:\n            try:\n                if metadata[\"langgraph_node\"] in source_nodes:\n                    yield chunk.text\n            except Exception as e:\n                logger.error(\n                    f\"Could not retrieve langgraph_node property\"\n                    f\" in tier_2_to_3_adapter:\\n{e}\"\n                )\n                yield chunk.text\n        else:\n            yield chunk.text\n</code></pre>"},{"location":"API/vector_store_qdrant/","title":"Module vector_store_qdrant.py","text":"<p>Computes the embeddings and handles uploading and saving the data to the vector database.</p> <p>This is a low-level module that provides the interface to the qdrant database, while also bringing it together with the settings informa- tion in config.toml and an embeddings provider. Exceptions are relayed to a logger object for use in a REPL interface. A connection to the database is representend by a <code>QdrantClient</code> object of the Qdrant API, which may be initialized directly through the constructor, or through the <code>client_from_config</code> function which reads the database options from config.toml:</p> <pre><code>    from lmm_education.stores.vector_store_qdrant import client_from_config\n\n    client: QdrantClient | None = client_from_config()\n    # check client is not None\n</code></pre> <p>It is also possible to intialize a config object explicitly for the properties that override config.toml.</p> <pre><code>    from lmm_education.stores.vector_store_qdrant import client_from_config\n    from lmm_education.config.config import ConfigSettings\n\n    # read settings from config.toml, but override 'storage':\n    settings = ConfigSettings(storage=\":memory:\")\n    client: QdrantClient | None = client_from_config(settings)\n</code></pre> <p>(Please also see the vector_store_qdrant_context module to create a global client object that is automatically closed).</p> <p>The functions in this module use a logger to communicate errors, so that the way exceptions are handled depends on the logger type. If no logger is specified, an error message is printed on the console.</p> <pre><code>    from lmm_education.stores.vector_store_qdrant import client_from_config\n    from lmm.utils.logging import LogfileLogger\n\n    logger = LogfileLogger()\n    client: QdrantClient | None = client_from_config(None, logger)\n    if client is None:\n        # read causes from logger\n</code></pre> <p>The role of client_from_config is to bind the creation of a QdrantClient object with the relevant settings in config.toml, and channel possible exceptions through the logger. Note, however, that the QdrantClient can also be created with the qdrant API directly. Note that in both cases the client needs be closed before exiting.</p> <p>A slightly more higher-lever alternative is obtaining the client through a context manager by calling <code>qdrant_client_context</code>:</p> <pre><code>try:\n    with qdrant_client_context() as client:\n        result_docs = upload(\n            client, \"documents\", model, settings, doc_chunks\n        )\n        result_imgs = upload(\n            client, \"images\", model, settings, img_chunks\n        )\nexcept Exception as e:\n    .... error handling\n</code></pre> <p>Please note that at present you have to capture errors when using the context manager.</p> <p>The remaining functions of the module take the client object to read and write to the database. All calls go through initialize_collection, which takes the name of the collection and an embedding model to specify how the data should be embedded (what type of dense and sparse vector, or any hybrid combination of those, should be used):</p> <pre><code>    # ... client creation not shown\n    from lmm_education.stores.vector_store_qdrant import (\n        initialize_collection,\n        QdrantEmbeddingModel,\n    )\n\n    embedding_model = QdrantEmbeddingModel.DENSE\n    flag: bool = initialize_collection(\n        client,\n        \"documents\",\n        embedding_model,\n        ConfigSettings().embeddings,\n        logger=logger)\n</code></pre> <pre><code>embedding_model = QdrantEmbeddingModel.DENSE\nopts: DatabaseSource = \":memory:\"\ntry:\n    with qdrant_client_context(opts) as client:\n        result = initialize_collection(\n            client,\n            \"Main\",\n            embedding_model,\n            ConfigSettings().embeddings,\n        )\nexcept Exception as e:\n    .... handle exceptions\n</code></pre> <p>In every call to the functions of the module, the client, the collection name, the embedding model, and the embedding provider settings are given as arguments.</p> <p>Data are ingested in the database in the form of lists of <code>Chunk</code> objects (see the <code>lmm_education.stores.chunks</code> module).</p> <pre><code>points: list[Point] = upload(\n    client,\n    \"documents\",\n    embedding_model,\n    ConfigSettings().embeddings,\n    chunks,\n    logger=logger,\n)\n</code></pre> <p>The <code>Point</code> objects are the representations of records used by Qdrant. This function converts each <code>Chunk</code> object to a <code>Point</code> object prior to ingesting. This conversion includes the formation of dense and sparse vectors, as specified by the embedding model. The points are returned if the upload is successful.</p> <p>The database may then be queried:</p> <pre><code>points: list[ScoredPoint] = query(\n    client,\n    \"documents\",\n    embedding_model,\n    ConfigSettings().embeddings,\n    \"What are the main uses of logistic regression?\",\n    limit=12,  # max number retrieved points\n    payload=True,  # all payload fields\n    logger=logger,\n)\n\n# retrieve text\nfor pt in points:\n    print(f\"{pt.score}\n{pt.payload['page_content']}\n\n\")\n</code></pre> <p><code>ScoredPoint</code> is the Qdrant class to return the payload of the retrieved points (which includes the text).</p> Responsibilities <p>definition of Qdrant embedding model compute embeddings upload data and query</p> Main functions <p>client_from_config / async_client_from_config initialize_collection / ainitialize_collection initialize_collection_from_config / ainitialize_collection_from_config upload / aupload query / aquery query_grouped / aquery_grouped</p> Behaviour <p>Errors communicated through logger, with the exception of client object context managers: qdrant_client_context and async_qdrant_client_context</p> <p>options:     show_root_heading: false</p>"},{"location":"API/vector_store_qdrant/#lmm_education.stores.vector_store_qdrant.QdrantEmbeddingModel","title":"<code>QdrantEmbeddingModel</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for embedding strategies</p> Source code in <code>lmm_education/stores/vector_store_qdrant.py</code> <pre><code>class QdrantEmbeddingModel(Enum):\n    \"\"\"Enum for embedding strategies\"\"\"\n\n    DENSE = \"dense\"\n    MULTIVECTOR = \"multivector\"\n    SPARSE = \"sparse\"\n    HYBRID_DENSE = \"hybrid_dense\"\n    HYBRID_MULTIVECTOR = \"hybrid_multivector\"\n    UUID = \"UUID\"  # no embedding\n</code></pre>"},{"location":"API/vector_store_qdrant/#lmm_education.stores.vector_store_qdrant.ainitialize_collection","title":"<code>ainitialize_collection(client, collection_name, qdrant_model, embedding_settings, *, logger=default_logger)</code>  <code>async</code>","text":"<p>Check that the collection supports the embedding model, if already in the database. If not, create a collection supporting the embedding model</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncQdrantClient</code> <p>QdrantClient object encapsulating the conn to the db</p> required <code>collection_name</code> <code>str</code> <p>the collection</p> required <code>qdrant_model</code> <code>QdrantEmbeddingModel</code> <p>the qdrant embedding model</p> required <code>embedding_settings</code> <code>EmbeddingSettings</code> <p>the embedding settings</p> required <p>Returns:</p> Type Description <code>bool</code> <p>a boolean signaling successful initialization and that the client may be used with these parameters.</p> Note <p>the embedding_settings are required to create an embedding to     obtain the dense embedding vector length, and record the     embedding vector length in the schema.</p> Source code in <code>lmm_education/stores/vector_store_qdrant.py</code> <pre><code>async def ainitialize_collection(\n    client: AsyncQdrantClient,\n    collection_name: str,\n    qdrant_model: QdrantEmbeddingModel,\n    embedding_settings: EmbeddingSettings,\n    *,\n    logger: LoggerBase = default_logger,\n) -&gt; bool:\n    \"\"\"\n    Check that the collection supports the embedding model, if\n    already in the database. If not, create a collection supporting\n    the embedding model\n\n    Args:\n        client: QdrantClient object encapsulating the conn to the db\n        collection_name: the collection\n        qdrant_model: the qdrant embedding model\n        embedding_settings: the embedding settings\n\n    Returns:\n        a boolean signaling successful initialization and that the\n            client may be used with these parameters.\n\n    Note:\n        the embedding_settings are required to create an embedding to\n            obtain the dense embedding vector length, and record the\n            embedding vector length in the schema.\n    \"\"\"\n\n    from requests.exceptions import ConnectionError\n\n    try:\n        from lmm.models.langchain.runnables import (\n            create_embeddings,\n        )\n\n        encoder: Embeddings = create_embeddings(embedding_settings)\n    except ConnectionError:\n        logger.error(\n            \"Could not connect to language models.\\n\"\n            + \"Check the internet connection.\"\n        )\n        return False\n    except ImportError:\n        logger.error(\n            \"Could not create langchain kernel. Check that langchain \"\n            \"is installed, and an internet connection is available.\"\n        )\n        return False\n    except Exception as e:\n        logger.error(\n            \"Could not initialize language model engine:\\n\" + str(e)\n        )\n        return False\n\n    try:\n        if await client.collection_exists(collection_name):\n            info = await client.get_collection(collection_name)\n            params = info.config.params  # noqa  # type: ignore\n\n            # checks that the opts and the collection\n            # are compatible\n            return await acheck_schema(\n                client,\n                collection_name,\n                qdrant_model,\n                embedding_settings,\n            )\n\n        # determine embedding size\n        if not (\n            qdrant_model == QdrantEmbeddingModel.UUID\n            or qdrant_model == QdrantEmbeddingModel.SPARSE\n        ):\n            data: list[float] = await encoder.aembed_query(\n                \"Test query\"\n            )\n            embedding_size: int = len(data)\n        else:\n            embedding_size: int = 0\n\n        match qdrant_model:\n            case QdrantEmbeddingModel.UUID:\n                await client.create_collection(\n                    collection_name=collection_name, vectors_config={}\n                )\n            case QdrantEmbeddingModel.DENSE:\n                await client.create_collection(\n                    collection_name=collection_name,\n                    vectors_config={\n                        DENSE_VECTOR_NAME: models.VectorParams(\n                            size=embedding_size,\n                            distance=models.Distance.COSINE,\n                        )\n                    },\n                )\n            case QdrantEmbeddingModel.MULTIVECTOR:\n                await client.create_collection(\n                    collection_name=collection_name,\n                    vectors_config={\n                        DENSE_VECTOR_NAME: models.VectorParams(\n                            size=embedding_size,\n                            distance=models.Distance.COSINE,\n                            multivector_config=models.MultiVectorConfig(\n                                comparator=models.MultiVectorComparator.MAX_SIM\n                            ),\n                        )\n                    },\n                )\n            case QdrantEmbeddingModel.SPARSE:\n                # TO DO: indices\n                await client.create_collection(\n                    collection_name=collection_name,\n                    vectors_config={},\n                    sparse_vectors_config={\n                        SPARSE_VECTOR_NAME: models.SparseVectorParams()\n                    },\n                )\n            case QdrantEmbeddingModel.HYBRID_DENSE:\n                # TO DO: indices\n                await client.create_collection(\n                    collection_name=collection_name,\n                    vectors_config={\n                        DENSE_VECTOR_NAME: models.VectorParams(\n                            size=embedding_size,\n                            distance=models.Distance.COSINE,\n                        )\n                    },\n                    sparse_vectors_config={\n                        SPARSE_VECTOR_NAME: models.SparseVectorParams()\n                    },\n                )\n            case QdrantEmbeddingModel.HYBRID_MULTIVECTOR:\n                # TO DO: indices\n                await client.create_collection(\n                    collection_name=collection_name,\n                    vectors_config={\n                        DENSE_VECTOR_NAME: models.VectorParams(\n                            size=embedding_size,\n                            distance=models.Distance.COSINE,\n                            multivector_config=models.MultiVectorConfig(\n                                comparator=models.MultiVectorComparator.MAX_SIM\n                            ),\n                        )\n                    },\n                    sparse_vectors_config={\n                        SPARSE_VECTOR_NAME: models.SparseVectorParams()\n                    },\n                )\n\n            case _:\n                raise RuntimeError(\n                    \"Unreachable code reached due to invalid \"\n                    + \"embedding model: \"\n                    + str(qdrant_model)\n                )\n\n    except ConnectionError:\n        logger.error(\n            \"Could not connect to the qdrant server, which may be down.\"\n        )\n        return False\n    except UnexpectedResponse as e:\n        logger.error(f\"Could not initialize {collection_name}: {e}\")\n        return False\n    except ApiException as e:\n        logger.error(\n            f\"Could not initialize {collection_name} due to API error: {e}\"\n        )\n        return False\n    except Exception as e:\n        logger.error(f\"Could not initialize {collection_name}: {e}\")\n        return False\n\n    # register the schema\n    try:\n        await acheck_schema(\n            client,\n            collection_name,\n            qdrant_model,\n            embedding_settings,\n            logger=logger,\n        )\n    except Exception as e:\n        logger.error(\n            f\"Unepected error: {collection_name} was\"\n            \"initialized, but the schema could not be\"\n            \"registered.\\nCall initialize_collection again to\"\n            f\"attempt to record schema. Reason for the failure:\\n{e}\"\n        )\n        # return True nevertheless\n\n    return True\n</code></pre>"},{"location":"API/vector_store_qdrant/#lmm_education.stores.vector_store_qdrant.ainitialize_collection_from_config","title":"<code>ainitialize_collection_from_config(client, collection_name=None, opts=None, *, logger=default_logger)</code>  <code>async</code>","text":"<p>See ainitialize_collection</p> Source code in <code>lmm_education/stores/vector_store_qdrant.py</code> <pre><code>async def ainitialize_collection_from_config(\n    client: AsyncQdrantClient,\n    collection_name: str | None = None,\n    opts: ConfigSettings | None = None,\n    *,\n    logger: LoggerBase = default_logger,\n) -&gt; bool:\n    \"\"\"See ainitialize_collection\"\"\"\n    opts = opts or load_settings(logger=logger)\n    if opts is None:\n        logger.error(\n            \"Could not initialize client due to invalid \" \"settings.\"\n        )\n        return False\n    if not collection_name:\n        collection_name = opts.database.collection_name\n\n    return await ainitialize_collection(\n        client,\n        collection_name,\n        encoding_to_qdrantembedding_model(opts.RAG.encoding_model),\n        opts.embeddings,\n        logger=logger,\n    )\n</code></pre>"},{"location":"API/vector_store_qdrant/#lmm_education.stores.vector_store_qdrant.aquery","title":"<code>aquery(client, collection_name, qdrant_model, embedding_settings, querytext, *, limit=12, payload=['page_content'], logger=default_logger)</code>  <code>async</code>","text":"<p>Executes a query on the client asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncQdrantClient</code> <p>a QdrantClient object</p> required <code>collection_name</code> <code>str</code> <p>the collection to query</p> required <code>qdrant_model</code> <code>QdrantEmbeddingModel</code> <p>the qdrant embedding model</p> required <code>embedding_settings</code> <code>EmbeddingSettings</code> <p>the embedding settings</p> required <code>querytext</code> <code>str</code> <p>the target text</p> required <code>limit</code> <code>int</code> <p>max number of chunks retrieved</p> <code>12</code> <code>payload</code> <code>list[str] | bool</code> <p>what properties to be retrieved; defaults to the text retrieved for similarity to the querytext</p> <code>['page_content']</code> <code>logger</code> <code>LoggerBase</code> <p>a logger object</p> <code>default_logger</code> <p>Returns:</p> Type Description <code>list[ScoredPoint]</code> <p>a list of ScoredPoint objects.</p> Source code in <code>lmm_education/stores/vector_store_qdrant.py</code> <pre><code>async def aquery(\n    client: AsyncQdrantClient,\n    collection_name: str,\n    qdrant_model: QdrantEmbeddingModel,\n    embedding_settings: EmbeddingSettings,\n    querytext: str,\n    *,\n    limit: int = 12,\n    payload: list[str] | bool = ['page_content'],\n    logger: LoggerBase = default_logger,\n) -&gt; list[ScoredPoint]:\n    \"\"\"\n    Executes a query on the client asynchronously.\n\n    Args:\n        client: a QdrantClient object\n        collection_name: the collection to query\n        qdrant_model: the qdrant embedding model\n        embedding_settings: the embedding settings\n        querytext: the target text\n        limit: max number of chunks retrieved\n        payload: what properties to be retrieved; defaults to the text\n            retrieved for similarity to the querytext\n        logger: a logger object\n\n    Returns:\n        a list of ScoredPoint objects.\n    \"\"\"\n\n    # load language model\n    from requests.exceptions import ConnectionError\n\n    try:\n        from lmm.models.langchain.runnables import (\n            create_embeddings,\n        )\n\n        encoder: Embeddings = create_embeddings(embedding_settings)\n    except ConnectionError:\n        logger.error(\n            \"Could not connect to language models.\\n\"\n            + \"Check the internet connection.\",\n        )\n        return []\n    except ImportError:\n        logger.error(\n            \"Could not create langchain kernel. Check that langchain \"\n            \"is installed, and an internet connection is available.\"\n        )\n        return []\n    except Exception as e:\n        logger.error(\n            \"Could not initialize language model engine:\\n\" + str(e)\n        )\n        return []\n\n    response: QdrantResponse = QdrantResponse(points=[])\n    try:\n        match qdrant_model:\n            case QdrantEmbeddingModel.UUID:\n                records: list[Record] = await client.retrieve(\n                    collection_name=collection_name,\n                    ids=[querytext],\n                    with_payload=payload,\n                )\n                points: list[ScoredPoint] = [\n                    ScoredPoint(\n                        id=r.id, version=1, payload=r.payload, score=1\n                    )\n                    for r in records\n                ]\n                response = QdrantResponse(points=points)\n\n            case QdrantEmbeddingModel.DENSE:\n                vect: list[float] = await encoder.aembed_query(\n                    querytext\n                )\n                query_dict: dict[str, Any] = {\n                    'collection_name': collection_name,\n                    'query': vect,\n                    'using': DENSE_VECTOR_NAME,\n                    'with_payload': payload,\n                    'limit': limit,\n                }\n                response = await client.query_points(**query_dict)\n\n            case QdrantEmbeddingModel.MULTIVECTOR:\n                vect: list[float] = await encoder.aembed_query(\n                    querytext\n                )\n                multi_vect: list[list[float]] = [vect, vect]\n                query_dict: dict[str, Any] = {\n                    'collection_name': collection_name,\n                    'query': multi_vect,\n                    'using': DENSE_VECTOR_NAME,\n                    'with_payload': payload,\n                    'limit': limit,\n                }\n                response = await client.query_points(**query_dict)\n\n            case QdrantEmbeddingModel.SPARSE:\n                sparse_model = _get_sparse_model(embedding_settings)\n                sparse_embeddings = list(\n                    sparse_model.embed(querytext)\n                )\n                query_dict: dict[str, Any] = {\n                    'collection_name': collection_name,\n                    'query': models.SparseVector(\n                        indices=list(sparse_embeddings[0].indices),\n                        values=list(sparse_embeddings[0].values),\n                    ),\n                    'using': SPARSE_VECTOR_NAME,\n                    'with_payload': payload,\n                    'limit': limit,\n                }\n                response = await client.query_points(**query_dict)\n\n            case QdrantEmbeddingModel.HYBRID_DENSE:\n                try:\n                    vect: list[float] = await encoder.aembed_query(\n                        querytext\n                    )\n                    sparse_model = _get_sparse_model(\n                        embedding_settings\n                    )\n                    sparse_embeddings = list(\n                        sparse_model.embed(querytext)\n                    )\n                except Exception:\n                    logger.error(\"Could not create encoding\")\n                    return []\n                dense_dict: dict[str, Any] = {\n                    'query': vect,\n                    'using': DENSE_VECTOR_NAME,\n                    'limit': 25,\n                }\n                sparse_dict: dict[str, Any] = {\n                    'query': models.SparseVector(\n                        indices=list(sparse_embeddings[0].indices),\n                        values=list(sparse_embeddings[0].values),\n                    ),\n                    'using': SPARSE_VECTOR_NAME,\n                    'limit': limit,\n                }\n                query_dict: dict[str, Any] = {\n                    'collection_name': collection_name,\n                    'prefetch': [\n                        models.Prefetch(**dense_dict),\n                        models.Prefetch(**sparse_dict),\n                    ],\n                    'query': models.FusionQuery(\n                        fusion=models.Fusion.RRF\n                    ),\n                    'with_payload': payload,\n                }\n                response = await client.query_points(**query_dict)\n\n            case QdrantEmbeddingModel.HYBRID_MULTIVECTOR:\n                try:\n                    vect: list[float] = await encoder.aembed_query(\n                        querytext\n                    )\n                    sparse_model = _get_sparse_model(\n                        embedding_settings\n                    )\n                    sparse_embeddings = list(\n                        sparse_model.embed(querytext)\n                    )\n                except Exception:\n                    logger.error(\"Could not create encoding\")\n                    return []\n                dense_dict: dict[str, Any] = {\n                    'query': [vect, vect],\n                    'using': DENSE_VECTOR_NAME,\n                    'limit': 25,\n                }\n                sparse_dict: dict[str, Any] = {\n                    'query': models.SparseVector(\n                        indices=list(sparse_embeddings[0].indices),\n                        values=list(sparse_embeddings[0].values),\n                    ),\n                    'using': SPARSE_VECTOR_NAME,\n                    'limit': limit,\n                }\n                query_dict: dict[str, Any] = {\n                    'collection_name': collection_name,\n                    'prefetch': [\n                        models.Prefetch(**dense_dict),\n                        models.Prefetch(**sparse_dict),\n                    ],\n                    'query': models.FusionQuery(\n                        fusion=models.Fusion.RRF\n                    ),\n                    'with_payload': payload,\n                }\n                response = await client.query_points(**query_dict)\n\n            case _:\n                raise RuntimeError(\n                    \"Unreachable code reached due to invalid \"\n                    + \"embedding model: \"\n                    + str(qdrant_model)\n                )\n\n    except ConnectionError:\n        logger.error(\n            \"Could not connect to the qdrant server, which may be down.\"\n        )\n        return []\n    except UnexpectedResponse as e:\n        logger.error(f\"Could not read from the vector database: {e}\")\n        return []\n    except ApiException as e:\n        logger.error(\n            f\"Could not read from the database due to API error: {e}\"\n        )\n        return []\n    except Exception as e:\n        logger.error(f\"Could not read from the database: {e}\")\n        return []\n\n    return response.points\n</code></pre>"},{"location":"API/vector_store_qdrant/#lmm_education.stores.vector_store_qdrant.aquery_grouped","title":"<code>aquery_grouped(client, collection_name, group_collection, qdrant_model, embedding_settings, querytext, *, limit=4, payload=True, group_size=1, group_field=GROUP_UUID_KEY, logger=default_logger)</code>  <code>async</code>","text":"<p>Executes a grouped query on the client asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncQdrantClient</code> <p>a QdrantClient object</p> required <code>collection_name</code> <code>str</code> <p>the collection to query</p> required <code>group_collection</code> <code>str</code> <p>the companion collection that will provide the output of the query</p> required <code>qdrant_model</code> <code>QdrantEmbeddingModel</code> <p>the qdrant embedding model</p> required <code>embedding_settings</code> <code>EmbeddingSettings</code> <p>the embedding settings</p> required <code>group_field</code> <code>str</code> <p>the filed to group on</p> <code>GROUP_UUID_KEY</code> <code>querytext</code> <code>str</code> <p>the target text</p> required <code>limit</code> <code>int</code> <p>max number of chunks retrieved</p> <code>4</code> <code>payload</code> <code>list[str] | bool</code> <p>what properties to be retrieved; defaults to all metadata properties. Note that text is 'page_content', and this will be included unless payload = False If payload = True, all propertes are included.</p> <code>True</code> <code>group_size</code> <code>int</code> <p>max retrieved output from the group collection</p> <code>1</code> <code>group_field</code> <code>str</code> <p>the filed to group on</p> <code>GROUP_UUID_KEY</code> <code>logger</code> <code>LoggerBase</code> <p>a logger object</p> <code>default_logger</code> <p>Returns:</p> Type Description <code>GroupsResult</code> <p>a list of ScoredPoint objects.</p> Source code in <code>lmm_education/stores/vector_store_qdrant.py</code> <pre><code>async def aquery_grouped(\n    client: AsyncQdrantClient,\n    collection_name: str,\n    group_collection: str,\n    qdrant_model: QdrantEmbeddingModel,\n    embedding_settings: EmbeddingSettings,\n    querytext: str,\n    *,\n    limit: int = 4,\n    payload: list[str] | bool = True,\n    group_size: int = 1,\n    group_field: str = GROUP_UUID_KEY,\n    logger: LoggerBase = default_logger,\n) -&gt; GroupsResult:\n    \"\"\"\n    Executes a grouped query on the client asynchronously.\n\n    Args:\n        client: a QdrantClient object\n        collection_name: the collection to query\n        group_collection: the companion collection that will provide\n            the output of the query\n        qdrant_model: the qdrant embedding model\n        embedding_settings: the embedding settings\n        group_field: the filed to group on\n        querytext: the target text\n        limit: max number of chunks retrieved\n        payload: what properties to be retrieved; defaults to all\n            metadata properties. Note that text is 'page_content', and\n            this will be included unless payload = False\n            If payload = True, all propertes are included.\n        group_size: max retrieved output from the group collection\n        group_field: the filed to group on\n        logger: a logger object\n\n    Returns:\n        a list of ScoredPoint objects.\n    \"\"\"\n\n    # make sure if payload is given, it contains `page_content`\n    if isinstance(payload, list):\n        if 'page_content' not in payload:\n            payload.append('page_content')\n\n    # Essentially, the qdrant API declares different\n    # types for any search API.\n    NullResult: GroupsResult = GroupsResult(groups=[])\n    # load language model\n    from requests.exceptions import ConnectionError\n\n    try:\n        from lmm.models.langchain.runnables import (\n            create_embeddings,\n        )\n\n        encoder: Embeddings = create_embeddings(embedding_settings)\n    except ConnectionError:\n        logger.error(\n            \"Could not connect to language models.\\n\"\n            + \"Check the internet connection.\",\n        )\n        return NullResult\n    except ImportError:\n        logger.error(\n            \"Could not create langchain kernel. Check that langchain \"\n            \"is installed, and an internet connection is available.\"\n        )\n        return NullResult\n    except Exception as e:\n        logger.error(\n            \"Could not initialize language model engine:\\n\" + str(e)\n        )\n        return NullResult\n\n    response: GroupsResult = NullResult\n    try:\n        match qdrant_model:\n            case QdrantEmbeddingModel.UUID:\n                # ignore grouping\n                hits: list[ScoredPoint] = await aquery(\n                    client,\n                    collection_name,\n                    qdrant_model,\n                    embedding_settings,\n                    querytext,\n                    limit=limit,\n                    payload=payload,\n                    logger=logger,\n                )\n                return GroupsResult(\n                    groups=[PointGroup(hits=hits, id=querytext)]\n                )\n\n            case QdrantEmbeddingModel.DENSE:\n                vect = await encoder.aembed_query(querytext)\n                query_dict: dict[str, Any] = {\n                    'collection_name': collection_name,\n                    'query': vect,\n                    'using': DENSE_VECTOR_NAME,\n                    'group_by': group_field,\n                    'limit': limit,\n                    'group_size': group_size,\n                    'with_lookup': models.WithLookup(\n                        collection=group_collection,\n                        with_payload=payload,\n                        with_vectors=False,\n                    ),\n                }\n                response = await client.query_points_groups(\n                    **query_dict\n                )\n\n            case QdrantEmbeddingModel.MULTIVECTOR:\n                vect = await encoder.aembed_query(querytext)\n                query_dict: dict[str, Any] = {\n                    'collection_name': collection_name,\n                    'query': [vect, vect],\n                    'using': DENSE_VECTOR_NAME,\n                    'group_by': group_field,\n                    'limit': limit,\n                    'group_size': group_size,\n                    'with_lookup': models.WithLookup(\n                        collection=group_collection,\n                        with_payload=payload,\n                        with_vectors=False,\n                    ),\n                }\n                response = await client.query_points_groups(\n                    **query_dict\n                )\n\n            case QdrantEmbeddingModel.SPARSE:\n                sparse_model: SparseTextEmbedding = _get_sparse_model(\n                    embedding_settings\n                )\n                sparse_embeddings: list[SparseEmbedding] = list(\n                    sparse_model.embed(querytext)\n                )\n                query_dict: dict[str, Any] = {\n                    'collection_name': collection_name,\n                    'query': models.SparseVector(\n                        indices=list(sparse_embeddings[0].indices),\n                        values=list(sparse_embeddings[0].values),\n                    ),\n                    'using': SPARSE_VECTOR_NAME,\n                    'group_by': group_field,\n                    'limit': limit,\n                    'group_size': group_size,\n                    'with_lookup': models.WithLookup(\n                        collection=group_collection,\n                        with_payload=payload,\n                        with_vectors=False,\n                    ),\n                }\n                response = await client.query_points_groups(\n                    **query_dict\n                )\n\n            case QdrantEmbeddingModel.HYBRID_DENSE:\n                try:\n                    vect: list[float] = await encoder.aembed_query(\n                        querytext\n                    )\n                    sparse_model: SparseTextEmbedding = (\n                        _get_sparse_model(embedding_settings)\n                    )\n                    sparse_embeddings: list[SparseEmbedding] = list(\n                        sparse_model.embed(querytext)\n                    )\n                except Exception:\n                    logger.error(\"Could not create encoding\")\n                    return NullResult\n                dense_dict: dict[str, Any] = {\n                    'query': vect,\n                    'using': DENSE_VECTOR_NAME,\n                    'limit': 25,\n                }\n                sparse_dict: dict[str, Any] = {\n                    'query': models.SparseVector(\n                        indices=list(sparse_embeddings[0].indices),\n                        values=list(sparse_embeddings[0].values),\n                    ),\n                    'using': SPARSE_VECTOR_NAME,\n                    'limit': limit,\n                }\n                query_dict: dict[str, Any] = {\n                    'collection_name': collection_name,\n                    'prefetch': [\n                        models.Prefetch(**dense_dict),\n                        models.Prefetch(**sparse_dict),\n                    ],\n                    'query': models.FusionQuery(\n                        fusion=models.Fusion.RRF\n                    ),\n                    'group_by': group_field,\n                    'group_size': group_size,\n                    'with_lookup': models.WithLookup(\n                        collection=group_collection,\n                        with_payload=payload,\n                        with_vectors=False,\n                    ),\n                }\n                response = await client.query_points_groups(\n                    **query_dict\n                )\n\n            case QdrantEmbeddingModel.HYBRID_MULTIVECTOR:\n                try:\n                    vect: list[float] = await encoder.aembed_query(\n                        querytext\n                    )\n                    sparse_model: SparseTextEmbedding = (\n                        _get_sparse_model(embedding_settings)\n                    )\n                    sparse_embeddings: list[SparseEmbedding] = list(\n                        sparse_model.embed(querytext)\n                    )\n                except Exception:\n                    logger.error(\"Could not create encoding\")\n                    return NullResult\n                dense_dict: dict[str, Any] = {\n                    'query': [vect, vect],\n                    'using': DENSE_VECTOR_NAME,\n                    'limit': 25,\n                }\n                sparse_dict: dict[str, Any] = {\n                    'query': models.SparseVector(\n                        indices=list(sparse_embeddings[0].indices),\n                        values=list(sparse_embeddings[0].values),\n                    ),\n                    'using': SPARSE_VECTOR_NAME,\n                    'limit': limit,\n                }\n                query_dict: dict[str, Any] = {\n                    'collection_name': collection_name,\n                    'prefetch': [\n                        models.Prefetch(**dense_dict),\n                        models.Prefetch(**sparse_dict),\n                    ],\n                    'query': models.FusionQuery(\n                        fusion=models.Fusion.RRF\n                    ),\n                    'group_by': group_field,\n                    'group_size': group_size,\n                    'with_lookup': models.WithLookup(\n                        collection=group_collection,\n                        with_payload=payload,\n                        with_vectors=False,\n                    ),\n                }\n                response = await client.query_points_groups(\n                    **query_dict\n                )\n\n            case _:\n                raise RuntimeError(\n                    \"Unreachable code reached due to invalid \"\n                    + \"embedding model: \"\n                    + str(qdrant_model)\n                )\n\n    except ConnectionError:\n        logger.error(\n            \"Could not connect to the qdrant server, which may be down.\"\n        )\n        return NullResult\n    except UnexpectedResponse as e:\n        logger.error(f\"Could not read from the vector database: {e}\")\n        return NullResult\n    except ApiException as e:\n        logger.error(\n            f\"Could not read from the database due to API error: {e}\"\n        )\n        return NullResult\n    except Exception as e:\n        logger.error(f\"Could not read from the database: {e}\")\n        return NullResult\n\n    return response\n</code></pre>"},{"location":"API/vector_store_qdrant/#lmm_education.stores.vector_store_qdrant.async_client_from_config","title":"<code>async_client_from_config(opts=None, logger=default_logger)</code>","text":"<p>Create a qdrant clients from config settings. Reads from config toml file settings if none given.</p> <p>Parameters:</p> Name Type Description Default <code>opts</code> <code>DatabaseSource | ConfigSettings | None</code> <p>the config settings</p> <code>None</code> <code>logger</code> <code>LoggerBase</code> <p>a logger object</p> <code>default_logger</code> <p>Returns:</p> Type Description <code>AsyncQdrantClient | None</code> <p>a QdrantClient object</p> Source code in <code>lmm_education/stores/vector_store_qdrant.py</code> <pre><code>def async_client_from_config(\n    opts: DatabaseSource | ConfigSettings | None = None,\n    logger: LoggerBase = default_logger,\n) -&gt; AsyncQdrantClient | None:\n    \"\"\"\n    Create a qdrant clients from config settings. Reads from config\n    toml file settings if none given.\n\n    Args:\n        opts: the config settings\n        logger: a logger object\n\n    Returns:\n        a QdrantClient object\n    \"\"\"\n    from lmm_education.config.config import (\n        LocalStorage,\n        RemoteSource,\n    )\n\n    opts = opts or load_settings(logger=logger)\n    if opts is None:\n        logger.error(\n            \"Could not initialize client due to invalid settings.\"\n        )\n        return None\n\n    try:\n        opts_storage: DatabaseSource = (\n            opts.storage if isinstance(opts, ConfigSettings) else opts\n        )\n\n        client: AsyncQdrantClient\n        match opts_storage:\n            case ':memory:':\n                client = AsyncQdrantClient(':memory:')\n            case LocalStorage(folder=folder):\n                client = AsyncQdrantClient(path=folder)\n            case RemoteSource(url=url, port=port):\n                client = AsyncQdrantClient(url=str(url), port=port)\n            case _:\n                logger.error(\"Invalid database source\")\n                return None\n    except ConnectionError:\n        logger.error(\n            \"Could not connect to the qdrant server, which may be down.\"\n        )\n        return None\n    except UnexpectedResponse as e:\n        logger.error(f\"Could not initialize qdrant client: {e}\")\n        return None\n    except ApiException as e:\n        logger.error(\n            f\"Could not initialize qdrant client due to API error: {e}\"\n        )\n        return None\n    except RuntimeError as e:\n        # This often caused by accessing qdrant with async client after\n        # prior acces with sync client\n        if \"aready accessed\" in str(e):\n            logger.error(\n                f\"Could not initialize qdrant client due to \"\n                f\"previous sync initialization?\\n{e}\"\n            )\n        else:\n            logger.error(\n                f\"Could not initialize qdrant client due to \"\n                f\"runtime error:\\n{e}\"\n            )\n        return None\n    except Exception as e:\n        logger.error(f\"Could not initialize qdrant client:\\n{e}\")\n        return None\n\n    return client\n</code></pre>"},{"location":"API/vector_store_qdrant/#lmm_education.stores.vector_store_qdrant.async_qdrant_client_context","title":"<code>async_qdrant_client_context(config=None, logger=default_logger)</code>  <code>async</code>","text":"<p>Contructs an asynchronous client object within a context manager.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DatabaseSource | ConfigSettings | None</code> <p>specification of the database. If none, settings from config.toml will be used.</p> <code>None</code> <code>logger</code> <code>LoggerBase</code> <p>a logger object.</p> <code>default_logger</code> <p>Returns:</p> Type Description <code>AsyncGenerator[AsyncQdrantClient]</code> <p>a client object context.</p> Behaviour <p>Raises ConnectionError if client initialization fails. This is standard behavior for context managers.</p> Source code in <code>lmm_education/stores/vector_store_qdrant.py</code> <pre><code>@asynccontextmanager\nasync def async_qdrant_client_context(\n    config: DatabaseSource | ConfigSettings | None = None,\n    logger: LoggerBase = default_logger,\n) -&gt; AsyncGenerator[AsyncQdrantClient]:\n    \"\"\"Contructs an asynchronous client object within a context\n    manager.\n\n    Args:\n        config: specification of the database. If none, settings\n            from config.toml will be used.\n        logger: a logger object.\n\n    Returns:\n        a client object context.\n\n    Behaviour:\n        Raises ConnectionError if client initialization fails.\n        This is standard behavior for context managers.\n    \"\"\"\n    client = None\n    try:\n        client = async_client_from_config(config, logger)\n        if client is None:\n            logger.error(\"Failed to create Qdrant client\")\n            raise ConnectionError(\"Failed to create Qdrant client\")\n        yield client\n    finally:\n        if client is not None:\n            try:\n                await client.close()\n            except Exception as e:\n                logger.warning(f\"Error closing client: {e}\")\n</code></pre>"},{"location":"API/vector_store_qdrant/#lmm_education.stores.vector_store_qdrant.aupload","title":"<code>aupload(client, collection_name, qdrant_model, embedding_settings, chunks, *, logger=default_logger)</code>  <code>async</code>","text":"<p>Upload a list of chunks into the Qdrant database</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncQdrantClient</code> <p>the connection to the database</p> required <code>collection_name</code> <code>str</code> <p>the collection</p> required <code>qdrant_model</code> <code>QdrantEmbeddingModel</code> <p>the qdrant embedding model</p> required <code>embedding_settings</code> <code>EmbeddingSettings</code> <p>the embedding settings</p> required <code>chunks</code> <code>list[Chunk]</code> <p>the chunk list</p> required <p>Returns:</p> Type Description <code>list[PointStruct]</code> <p>a list of Point objects</p> Source code in <code>lmm_education/stores/vector_store_qdrant.py</code> <pre><code>async def aupload(\n    client: AsyncQdrantClient,\n    collection_name: str,\n    qdrant_model: QdrantEmbeddingModel,\n    embedding_settings: EmbeddingSettings,\n    chunks: list[Chunk],\n    *,\n    logger: LoggerBase = default_logger,\n) -&gt; list[Point]:\n    \"\"\"\n    Upload a list of chunks into the Qdrant database\n\n    Args:\n        client: the connection to the database\n        collection_name: the collection\n        qdrant_model: the qdrant embedding model\n        embedding_settings: the embedding settings\n        chunks: the chunk list\n\n    Returns:\n        a list of Point objects\n    \"\"\"\n\n    if not await ainitialize_collection(\n        client,\n        collection_name,\n        qdrant_model,\n        embedding_settings,\n        logger=logger,\n    ):\n        logger.error(\"Could not initialize collection\")\n        return []\n\n    points: list[Point] = chunks_to_points(\n        chunks, qdrant_model, embedding_settings, logger=logger\n    )\n    if not points:\n        return []\n\n    try:\n        await client.upsert(\n            collection_name=collection_name, points=points\n        )\n    except ConnectionError:\n        logger.error(\n            \"Could not connect to the qdrant server, which may be down.\"\n        )\n        return []\n    except UnexpectedResponse as e:\n        logger.error(f\"Could not write to vector database: {e}\")\n        return []\n    except ApiException as e:\n        logger.error(\n            f\"Could not write to database due to API error: {e}\"\n        )\n        return []\n    except Exception as e:\n        logger.error(f\"Could not upload chunks to database:\\n{e}\")\n        return []\n\n    return points\n</code></pre>"},{"location":"API/vector_store_qdrant/#lmm_education.stores.vector_store_qdrant.chunks_to_points","title":"<code>chunks_to_points(chunks, qdrant_model, embedding_settings, *, logger=default_logger, chunk_to_payload=_chunk_to_payload_langchain)</code>","text":"<p>Converts a list of chunks into a list of the record objects understood by the Qdrant database, using an embedding model for the conversion.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list[Chunk]</code> <p>the chunks list</p> required <code>qdrant_model</code> <code>QdrantEmbeddingModel</code> <p>the embedding model, or a ConfigSettings object from which the model may be deduced</p> required <code>embedding_settings</code> <code>EmbeddingSettings</code> <p>the embedding settings</p> required <code>logger</code> <code>LoggerBase</code> <p>a logger object</p> <code>default_logger</code> <code>chunk_to_payload</code> <code>Callable[[Chunk], dict[str, Any]]</code> <p>a function to map chunks to the Langchain representation (internal use)</p> <code>_chunk_to_payload_langchain</code> <p>Returns:</p> Type Description <code>list[PointStruct]</code> <p>a list of Point objects (PointStruct)</p> Source code in <code>lmm_education/stores/vector_store_qdrant.py</code> <pre><code>def chunks_to_points(\n    chunks: list[Chunk],\n    qdrant_model: QdrantEmbeddingModel,\n    embedding_settings: EmbeddingSettings,\n    *,\n    logger: LoggerBase = default_logger,\n    chunk_to_payload: Callable[\n        [Chunk], dict[str, Any]\n    ] = _chunk_to_payload_langchain,\n) -&gt; list[Point]:\n    \"\"\"\n    Converts a list of chunks into a list of the record objects\n    understood by the Qdrant database, using an embedding model\n    for the conversion.\n\n    Args:\n        chunks: the chunks list\n        qdrant_model: the embedding model, or a ConfigSettings object from\n            which the model may be deduced\n        embedding_settings: the embedding settings\n        logger: a logger object\n        chunk_to_payload: a function to map chunks to the Langchain\n            representation (internal use)\n\n    Returns:\n        a list of Point objects (PointStruct)\n    \"\"\"\n\n    # Note: no async version as there is no I/O\n\n    if not chunks:\n        return []\n\n    # load embedding model\n    from requests.exceptions import ConnectionError\n\n    try:\n        from lmm.models.langchain.runnables import (\n            create_embeddings,\n        )\n\n        # if embeddings_model is None, read from config.toml\n        encoder: Embeddings = create_embeddings(embedding_settings)\n    except ConnectionError:\n        logger.error(\n            \"Could not connect to language models.\\n\"\n            + \"Check the internet connection.\",\n        )\n        return []\n    except ImportError:\n        logger.error(\n            \"Could not create langchain kernel. Check that langchain \"\n            \"is installed, and an internet connection is available.\"\n        )\n        return []\n    except Exception as e:\n        logger.error(\n            \"Could not initialize language model engine:\\n\" + str(e)\n        )\n        return []\n\n    # the payload saved in the database is given by chunk_to_payload,\n    # the emmbedding by the embedding model\n    points: list[Point] = []\n    match qdrant_model:\n        case QdrantEmbeddingModel.UUID:\n            points = [\n                Point(\n                    id=d.uuid, vector={}, payload=chunk_to_payload(d)\n                )\n                for d in chunks\n            ]\n\n        case QdrantEmbeddingModel.DENSE:\n            try:\n                vect = encoder.embed_documents(\n                    [t.dense_encoding for t in chunks]\n                )\n            except Exception:\n                logger.error(\"Could not create encoding\")\n                return []\n            points = [\n                Point(\n                    id=d.uuid,\n                    vector={DENSE_VECTOR_NAME: v},\n                    payload=chunk_to_payload(d),\n                )\n                for d, v in zip(chunks, vect)\n            ]\n\n        case QdrantEmbeddingModel.MULTIVECTOR:\n            try:\n                vect = [\n                    encoder.embed_documents(\n                        [t.annotations for t in chunks]\n                    ),\n                    encoder.embed_documents(\n                        [t.dense_encoding for t in chunks]\n                    ),\n                ]\n            except Exception:\n                logger.error(\"Could not create encoding\")\n                return []\n            points = [\n                Point(\n                    id=d.uuid,\n                    vector={DENSE_VECTOR_NAME: [v1, v2]},\n                    payload=chunk_to_payload(d),\n                )\n                for d, v1, v2 in zip(chunks, vect[0], vect[1])\n            ]\n\n        case QdrantEmbeddingModel.SPARSE:\n            try:\n                sparse_model = _get_sparse_model(embedding_settings)\n                sparse_embeddings = list(\n                    sparse_model.embed(\n                        [d.sparse_encoding for d in chunks]\n                    )\n                )\n            except Exception:\n                logger.error(\"Could not create encoding\")\n                return []\n            points = [\n                Point(\n                    id=d.uuid,\n                    vector={\n                        SPARSE_VECTOR_NAME: models.SparseVector(\n                            indices=s.indices.tolist(),\n                            values=s.values.tolist(),\n                        )\n                    },\n                    payload=chunk_to_payload(d),\n                )\n                for d, s in zip(chunks, sparse_embeddings)\n            ]\n\n        case QdrantEmbeddingModel.HYBRID_DENSE:\n            try:\n                vect = encoder.embed_documents(\n                    [t.dense_encoding for t in chunks]\n                )\n                sparse_model = _get_sparse_model(embedding_settings)\n                sparse_embeddings = list(\n                    sparse_model.embed(\n                        [d.sparse_encoding for d in chunks]\n                    )\n                )\n            except Exception:\n                logger.error(\"Could not create encoding\")\n                return []\n            points = [\n                Point(\n                    id=d.uuid,\n                    vector={\n                        DENSE_VECTOR_NAME: v,\n                        SPARSE_VECTOR_NAME: models.SparseVector(\n                            indices=s.indices.tolist(),\n                            values=s.values.tolist(),\n                        ),\n                    },\n                    payload=chunk_to_payload(d),\n                )\n                for d, v, s in zip(chunks, vect, sparse_embeddings)\n            ]\n\n        case QdrantEmbeddingModel.HYBRID_MULTIVECTOR:\n            try:\n                vect = [\n                    encoder.embed_documents(\n                        [t.annotations for t in chunks]\n                    ),\n                    encoder.embed_documents(\n                        [t.dense_encoding for t in chunks]\n                    ),\n                ]\n                sparse_model = _get_sparse_model(embedding_settings)\n                sparse_embeddings = list(\n                    sparse_model.embed(\n                        [d.sparse_encoding for d in chunks]\n                    )\n                )\n            except Exception:\n                logger.error(\"Could not create encoding\")\n                return []\n            points = [\n                Point(\n                    id=d.uuid,\n                    vector={\n                        DENSE_VECTOR_NAME: [v1, v2],\n                        SPARSE_VECTOR_NAME: models.SparseVector(\n                            indices=s.indices.tolist(),\n                            values=s.values.tolist(),\n                        ),\n                    },\n                    payload=chunk_to_payload(d),\n                )\n                for d, v1, v2, s in zip(\n                    chunks, vect[0], vect[1], sparse_embeddings\n                )\n            ]\n\n        case _:\n            raise RuntimeError(\n                \"Unreachable code reached due to invalid \"\n                + \"embedding model: \"\n                + str(qdrant_model)\n            )\n\n    # This should not happen, but it is important to check\n    # as there will be no way to detect the origin of problems\n    # at query time.\n    if not len(points) == len(chunks):\n        raise SystemError(\n            f\"Unexpected failure in generating \"\n            f\"Qdrant points: {len(points)} points for \"\n            f\"{len(chunks)} chunks \"\n            f\"(qdrant embedding model: {qdrant_model})\"\n        )\n\n    return points\n</code></pre>"},{"location":"API/vector_store_qdrant/#lmm_education.stores.vector_store_qdrant.client_from_config","title":"<code>client_from_config(opts=None, logger=default_logger)</code>","text":"<p>Create a qdrant clients from config settings. Reads from config toml file settings if none given.</p> <p>Please note that a client should be closed before exiting.</p> <p>Parameters:</p> Name Type Description Default <code>opts</code> <code>DatabaseSource | ConfigSettings | None</code> <p>the config settings</p> <code>None</code> <code>logger</code> <code>LoggerBase</code> <p>a logger object</p> <code>default_logger</code> <p>Returns:</p> Type Description <code>QdrantClient | None</code> <p>a QdrantClient object</p> Source code in <code>lmm_education/stores/vector_store_qdrant.py</code> <pre><code>def client_from_config(\n    opts: DatabaseSource | ConfigSettings | None = None,\n    logger: LoggerBase = default_logger,\n) -&gt; QdrantClient | None:\n    \"\"\"\n    Create a qdrant clients from config settings. Reads from config\n    toml file settings if none given.\n\n    Please note that a client should be closed before exiting.\n\n    Args:\n        opts: the config settings\n        logger: a logger object\n\n    Returns:\n        a QdrantClient object\n    \"\"\"\n    from lmm_education.config.config import (\n        LocalStorage,\n        RemoteSource,\n    )\n\n    opts = opts or load_settings(logger=logger)\n    if opts is None:\n        logger.error(\n            \"Could not initialize client due to invalid settings.\"\n        )\n        return None\n\n    try:\n        opts_storage: DatabaseSource = (\n            opts.storage if isinstance(opts, ConfigSettings) else opts\n        )\n        client: QdrantClient\n        match opts_storage:\n            case ':memory:':\n                client = QdrantClient(':memory:')\n            case LocalStorage(folder=folder):\n                client = QdrantClient(path=folder)\n            case RemoteSource(url=url, port=port):\n                client = QdrantClient(url=str(url), port=port)\n            case _:\n                logger.error(\"Invalid database source\")\n                return None\n    except ConnectionError:\n        logger.error(\n            \"Could not connect to the qdrant server, which may be down.\"\n        )\n        return None\n    except UnexpectedResponse as e:\n        logger.error(f\"Could not initialize qdrant client: {e}\")\n        return None\n    except ApiException as e:\n        logger.error(\n            f\"Could not initialize qdrant client due to API error: {e}\"\n        )\n        return None\n    except RuntimeError as e:\n        # This often caused by accessing qdrant with sync client after\n        # prior acces with async client\n        if \"already accessed\" in str(e):\n            logger.error(\n                f\"Could not initialize qdrant client due to \"\n                f\"previous async initialization?\\n{e}\"\n            )\n        else:\n            logger.error(\n                f\"Could not initialize qdrant client due to \"\n                f\"runtime error:\\n{e}\"\n            )\n        return None\n    except Exception as e:\n        logger.error(f\"Could not initialize qdrant client:\\n{e}\")\n        return None\n\n    return client\n</code></pre>"},{"location":"API/vector_store_qdrant/#lmm_education.stores.vector_store_qdrant.encoding_to_qdrantembedding_model","title":"<code>encoding_to_qdrantembedding_model(em)</code>","text":"<p>Embedding model required for encoding</p> Source code in <code>lmm_education/stores/vector_store_qdrant.py</code> <pre><code>def encoding_to_qdrantembedding_model(\n    em: EncodingModel,\n) -&gt; QdrantEmbeddingModel:\n    \"\"\"Embedding model required for encoding\"\"\"\n    match em:\n        case EncodingModel.NONE:\n            return QdrantEmbeddingModel.UUID\n        case EncodingModel.CONTENT | EncodingModel.MERGED:\n            return QdrantEmbeddingModel.DENSE\n        case EncodingModel.MULTIVECTOR:\n            return QdrantEmbeddingModel.MULTIVECTOR\n        case EncodingModel.SPARSE:\n            return QdrantEmbeddingModel.SPARSE\n        case (\n            EncodingModel.SPARSE_CONTENT | EncodingModel.SPARSE_MERGED\n        ):\n            return QdrantEmbeddingModel.HYBRID_DENSE\n        case EncodingModel.SPARSE_MULTIVECTOR:\n            return QdrantEmbeddingModel.HYBRID_MULTIVECTOR\n        case _:\n            raise Exception(\n                \"Unreachable code reached due to \"\n                + \"invalid encoding model: \"\n                + str(em)\n            )\n</code></pre>"},{"location":"API/vector_store_qdrant/#lmm_education.stores.vector_store_qdrant.groups_to_points","title":"<code>groups_to_points(groups)</code>","text":"<p>transform a GroupsResult object into a list of ScoredPoint objects of the group lookup list</p> Source code in <code>lmm_education/stores/vector_store_qdrant.py</code> <pre><code>def groups_to_points(groups: GroupsResult) -&gt; list[ScoredPoint]:\n    \"\"\"transform a GroupsResult object into a list of ScoredPoint\n    objects of the group lookup list\"\"\"\n    # assumes groups is structured as it should\n    records: list[Record] = [\n        g.lookup for g in groups.groups if g.lookup\n    ]\n    _max: Callable[[list[ScoredPoint]], float] = lambda hits: max(\n        [h.score for h in hits]\n    )  # noqa: E731\n    scores: list[float] = [\n        _max(g.hits) for g in groups.groups if g.lookup\n    ]\n    points: list[ScoredPoint] = [\n        ScoredPoint(id=r.id, version=1, payload=r.payload, score=s)\n        for r, s in zip(records, scores)\n    ]\n    return points\n</code></pre>"},{"location":"API/vector_store_qdrant/#lmm_education.stores.vector_store_qdrant.initialize_collection","title":"<code>initialize_collection(client, collection_name, qdrant_model, embedding_settings, *, logger=default_logger)</code>","text":"<p>Check that the collection supports the embedding model, if already in the database. If not, create a collection supporting the embedding model</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>QdrantClient</code> <p>QdrantClient object encapsulating the conn to the db</p> required <code>collection_name</code> <code>str</code> <p>the collection.</p> required <code>qdrant_model</code> <code>QdrantEmbeddingModel</code> <p>the qdrant embedding model</p> required <code>embedding_settings</code> <code>EmbeddingSettings</code> <p>the embedding settings</p> required <p>Returns:</p> Type Description <code>bool</code> <p>a boolean flag indicating that the client may be used with these parameters.</p> Note <p>the embedding_settings are required to create an embedding to     obtain the dense embedding vector length, and record in     the schema their dimension.</p> Source code in <code>lmm_education/stores/vector_store_qdrant.py</code> <pre><code>def initialize_collection(\n    client: QdrantClient,\n    collection_name: str,\n    qdrant_model: QdrantEmbeddingModel,\n    embedding_settings: EmbeddingSettings,\n    *,\n    logger: LoggerBase = default_logger,\n) -&gt; bool:\n    \"\"\"\n    Check that the collection supports the embedding model, if\n    already in the database. If not, create a collection supporting\n    the embedding model\n\n    Args:\n        client: QdrantClient object encapsulating the conn to the db\n        collection_name: the collection.\n        qdrant_model: the qdrant embedding model\n        embedding_settings: the embedding settings\n\n    Returns:\n        a boolean flag indicating that the client may be used with\n            these parameters.\n\n    Note:\n        the embedding_settings are required to create an embedding to\n            obtain the dense embedding vector length, and record in\n            the schema their dimension.\n    \"\"\"\n\n    from requests.exceptions import ConnectionError\n\n    try:\n        from lmm.models.langchain.runnables import (\n            create_embeddings,\n        )\n\n        encoder: Embeddings = create_embeddings(embedding_settings)\n    except ConnectionError:\n        logger.error(\n            \"Could not connect to the language model.\\n\"\n            + \"Check the internet connection.\"\n        )\n        return False\n    except ImportError:\n        logger.error(\n            \"Could not create langchain kernel. Check that langchain \"\n            \"is installed, and an internet connection is available.\"\n        )\n        return False\n    except Exception as e:\n        logger.error(\n            \"Could not initialize language model engine:\\n\" + str(e)\n        )\n        return False\n\n    try:\n        if client.collection_exists(collection_name):\n            info = client.get_collection(collection_name)\n            params = info.config.params  # noqa  #type: ignore\n\n            # checks that the opts and the collection\n            # are compatible\n            return check_schema(\n                client,\n                collection_name,\n                qdrant_model,\n                embedding_settings,\n                logger=logger,\n            )\n\n        # determine embedding size\n        if not (\n            qdrant_model == QdrantEmbeddingModel.UUID\n            or qdrant_model == QdrantEmbeddingModel.SPARSE\n        ):\n            try:\n                data: list[float] = encoder.embed_query(\"Test query\")\n            except Exception as e:\n                logger.error(\n                    \"Could not determine embedding vector size, \"\n                    + f\"cannot embed.\\n{e}\"\n                )\n                return False\n\n            embedding_size: int = len(data)\n        else:\n            embedding_size: int = 0\n\n        match qdrant_model:\n            case QdrantEmbeddingModel.UUID:\n                client.create_collection(\n                    collection_name=collection_name, vectors_config={}\n                )\n            case QdrantEmbeddingModel.DENSE:\n                client.create_collection(\n                    collection_name=collection_name,\n                    vectors_config={\n                        DENSE_VECTOR_NAME: models.VectorParams(\n                            size=embedding_size,\n                            distance=models.Distance.COSINE,\n                        )\n                    },\n                )\n            case QdrantEmbeddingModel.MULTIVECTOR:\n                client.create_collection(\n                    collection_name=collection_name,\n                    vectors_config={\n                        DENSE_VECTOR_NAME: models.VectorParams(\n                            size=embedding_size,\n                            distance=models.Distance.COSINE,\n                            multivector_config=models.MultiVectorConfig(\n                                comparator=models.MultiVectorComparator.MAX_SIM\n                            ),\n                        )\n                    },\n                )\n            case QdrantEmbeddingModel.SPARSE:\n                # TO DO: indices\n                client.create_collection(\n                    collection_name=collection_name,\n                    vectors_config={},\n                    sparse_vectors_config={\n                        SPARSE_VECTOR_NAME: models.SparseVectorParams()\n                    },\n                )\n            case QdrantEmbeddingModel.HYBRID_DENSE:\n                # TO DO: indices\n                client.create_collection(\n                    collection_name=collection_name,\n                    vectors_config={\n                        DENSE_VECTOR_NAME: models.VectorParams(\n                            size=embedding_size,\n                            distance=models.Distance.COSINE,\n                        )\n                    },\n                    sparse_vectors_config={\n                        SPARSE_VECTOR_NAME: models.SparseVectorParams()\n                    },\n                )\n            case QdrantEmbeddingModel.HYBRID_MULTIVECTOR:\n                # TO DO: indices\n                client.create_collection(\n                    collection_name=collection_name,\n                    vectors_config={\n                        DENSE_VECTOR_NAME: models.VectorParams(\n                            size=embedding_size,\n                            distance=models.Distance.COSINE,\n                            multivector_config=models.MultiVectorConfig(\n                                comparator=models.MultiVectorComparator.MAX_SIM\n                            ),\n                        )\n                    },\n                    sparse_vectors_config={\n                        SPARSE_VECTOR_NAME: models.SparseVectorParams()\n                    },\n                )\n            case _:\n                raise RuntimeError(\n                    \"Unreachable code reached due to invalid \"\n                    + \"embedding model: \"\n                    + str(qdrant_model)\n                )\n    except ConnectionError:\n        logger.error(\n            \"Could not connect to the qdrant server, which may be down.\"\n        )\n        return False\n    except UnexpectedResponse as e:\n        logger.error(f\"Could not initialize {collection_name}: {e}\")\n        return False\n    except ApiException as e:\n        logger.error(\n            f\"Could not initialize {collection_name} due to API error: {e}\"\n        )\n        return False\n    except Exception as e:\n        logger.error(f\"Could not initialize {collection_name}: {e}\")\n        return False\n\n    # register the schema\n    try:\n        check_schema(\n            client,\n            collection_name,\n            qdrant_model,\n            embedding_settings,\n            logger=logger,\n        )\n    except Exception as e:\n        logger.error(\n            f\"Unepected error: {collection_name} was\"\n            \"initialized, but the schema could not be\"\n            \"registered.\\nCall initialize_collection again to\"\n            f\"attempt to record schema. Reason for the failure:\\n{e}\"\n        )\n        # return True nevertheless\n\n    return True\n</code></pre>"},{"location":"API/vector_store_qdrant/#lmm_education.stores.vector_store_qdrant.initialize_collection_from_config","title":"<code>initialize_collection_from_config(client, collection_name=None, opts=None, *, logger=default_logger)</code>","text":"<p>See initialize_collection. If collection_name is provided, it will override the collection_name in the config settings object.</p> Source code in <code>lmm_education/stores/vector_store_qdrant.py</code> <pre><code>def initialize_collection_from_config(\n    client: QdrantClient,\n    collection_name: str | None = None,\n    opts: ConfigSettings | None = None,\n    *,\n    logger: LoggerBase = default_logger,\n) -&gt; bool:\n    \"\"\"\n    See initialize_collection. If collection_name is provided, it\n    will override the collection_name in the config settings object.\n    \"\"\"\n    opts = opts or load_settings(logger=logger)\n    if opts is None:\n        logger.error(\n            \"Could not initialize client due to invalid \" \"settings.\"\n        )\n        return False\n    if not collection_name:\n        collection_name = opts.database.collection_name\n\n    return initialize_collection(\n        client,\n        collection_name,\n        encoding_to_qdrantembedding_model(opts.RAG.encoding_model),\n        opts.embeddings,\n        logger=logger,\n    )\n</code></pre>"},{"location":"API/vector_store_qdrant/#lmm_education.stores.vector_store_qdrant.points_to_blocks","title":"<code>points_to_blocks(points)</code>","text":"<p>transform a list of ingestion points into a block list (to visualize what one ingested)</p> Source code in <code>lmm_education/stores/vector_store_qdrant.py</code> <pre><code>def points_to_blocks(points: list[Point]) -&gt; list[Block]:\n    \"\"\"transform a list of ingestion points into a block list (to\n    visualize what one ingested)\"\"\"\n    blocks: list[Block] = []\n    for p in points:\n        blocks.append(\n            MetadataBlock(\n                content=(\n                    p.payload['metadata']\n                    if p.payload and 'metadata' in p.payload\n                    else {}\n                )\n            )\n        )\n        blocks.append(\n            TextBlock(\n                content=(\n                    p.payload['page_content']\n                    if p.payload and 'page_content' in p.payload\n                    else \"\"\n                )\n            )\n        )\n    return blocks\n</code></pre>"},{"location":"API/vector_store_qdrant/#lmm_education.stores.vector_store_qdrant.points_to_ids","title":"<code>points_to_ids(points)</code>","text":"<p>transform a list of points into a list of their id's</p> Source code in <code>lmm_education/stores/vector_store_qdrant.py</code> <pre><code>def points_to_ids(\n    points: list[Point] | list[ScoredPoint],\n) -&gt; list[str]:\n    \"\"\"transform a list of points into a list of their id's\"\"\"\n    return [str(p.id) for p in points]\n</code></pre>"},{"location":"API/vector_store_qdrant/#lmm_education.stores.vector_store_qdrant.points_to_payload","title":"<code>points_to_payload(points, payload_key=None)</code>","text":"<p>transform a list of points into a list of their payload</p> Source code in <code>lmm_education/stores/vector_store_qdrant.py</code> <pre><code>def points_to_payload(\n    points: list[Point] | list[ScoredPoint],\n    payload_key: str | None = None,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"transform a list of points into a list of their payload\"\"\"\n    if payload_key is None:\n        return [p.payload for p in points if p.payload]\n    else:\n        return [\n            p.payload.get(payload_key, \"\")\n            for p in points\n            if p.payload\n        ]\n</code></pre>"},{"location":"API/vector_store_qdrant/#lmm_education.stores.vector_store_qdrant.points_to_text","title":"<code>points_to_text(points)</code>","text":"<p>transform a list of points into a list of their textual content</p> Source code in <code>lmm_education/stores/vector_store_qdrant.py</code> <pre><code>def points_to_text(\n    points: list[Point] | list[ScoredPoint],\n) -&gt; list[str]:\n    \"\"\"transform a list of points into a list of their textual\n    content\"\"\"\n    return [\n        str(p.payload['page_content'])\n        for p in points\n        if p.payload and 'page_content' in p.payload\n    ]\n</code></pre>"},{"location":"API/vector_store_qdrant/#lmm_education.stores.vector_store_qdrant.qdrant_client_context","title":"<code>qdrant_client_context(config=None, logger=default_logger)</code>","text":"<p>Contructs a client object within a context manager.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DatabaseSource | ConfigSettings | None</code> <p>specification of the database. If none, settings from config.toml will be used.</p> <code>None</code> <code>logger</code> <code>LoggerBase</code> <p>a logger object.</p> <code>default_logger</code> <p>Returns:</p> Type Description <code>Generator[QdrantClient]</code> <p>a client object context.</p> Behaviour <p>Raises ConnectionError if client initialization fails. This is standard behavior for context managers.</p> Source code in <code>lmm_education/stores/vector_store_qdrant.py</code> <pre><code>@contextmanager\ndef qdrant_client_context(\n    config: DatabaseSource | ConfigSettings | None = None,\n    logger: LoggerBase = default_logger,\n) -&gt; Generator[QdrantClient]:\n    \"\"\"Contructs a client object within a context manager.\n\n    Args:\n        config: specification of the database. If none, settings\n            from config.toml will be used.\n        logger: a logger object.\n\n    Returns:\n        a client object context.\n\n    Behaviour:\n        Raises ConnectionError if client initialization fails.\n        This is standard behavior for context managers.\n    \"\"\"\n    client = None\n    try:\n        client = client_from_config(config, logger)\n        if client is None:\n            logger.error(\"Failed to create Qdrant client\")\n            raise ConnectionError(\"Failed to create Qdrant client\")\n        yield client\n    finally:\n        if client is not None:\n            try:\n                client.close()\n            except Exception as e:\n                logger.warning(f\"Error closing client: {e}\")\n</code></pre>"},{"location":"API/vector_store_qdrant/#lmm_education.stores.vector_store_qdrant.query","title":"<code>query(client, collection_name, qdrant_model, embedding_settings, querytext, *, limit=12, payload=['page_content'], logger=default_logger)</code>","text":"<p>Executes a query on the client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>QdrantClient</code> <p>a QdrantClient object</p> required <code>collection_name</code> <code>str</code> <p>the collection to query</p> required <code>qdrant_model</code> <code>QdrantEmbeddingModel</code> <p>the qdrant embedding model</p> required <code>embedding_settings</code> <code>EmbeddingSettings</code> <p>the embedding settings</p> required <code>querytext</code> <code>str</code> <p>the target text</p> required <code>limit</code> <code>int</code> <p>max number of chunks retrieved</p> <code>12</code> <code>payload</code> <code>list[str] | bool</code> <p>what properties to be retrieved; defaults to the text retrieved for similarity to the querytext</p> <code>['page_content']</code> <code>logger</code> <code>LoggerBase</code> <p>a logger object</p> <code>default_logger</code> <p>Returns:</p> Type Description <code>list[ScoredPoint]</code> <p>a list of ScoredPoint objects.</p> Source code in <code>lmm_education/stores/vector_store_qdrant.py</code> <pre><code>def query(\n    client: QdrantClient,\n    collection_name: str,\n    qdrant_model: QdrantEmbeddingModel,\n    embedding_settings: EmbeddingSettings,\n    querytext: str,\n    *,\n    limit: int = 12,\n    payload: list[str] | bool = ['page_content'],\n    logger: LoggerBase = default_logger,\n) -&gt; list[ScoredPoint]:\n    \"\"\"\n    Executes a query on the client.\n\n    Args:\n        client: a QdrantClient object\n        collection_name: the collection to query\n        qdrant_model: the qdrant embedding model\n        embedding_settings: the embedding settings\n        querytext: the target text\n        limit: max number of chunks retrieved\n        payload: what properties to be retrieved; defaults to the text\n            retrieved for similarity to the querytext\n        logger: a logger object\n\n    Returns:\n        a list of ScoredPoint objects.\n    \"\"\"\n\n    # load language model\n    from requests.exceptions import ConnectionError\n\n    try:\n        from lmm.models.langchain.runnables import (\n            create_embeddings,\n        )\n\n        encoder: Embeddings = create_embeddings(embedding_settings)\n    except ConnectionError:\n        logger.error(\n            \"Could not connect to language models.\\n\"\n            + \"Check the internet connection.\",\n        )\n        return []\n    except ImportError:\n        logger.error(\n            \"Could not create langchain kernel. Check that langchain \"\n            \"is installed, and an internet connection is available.\"\n        )\n        return []\n    except Exception as e:\n        logger.error(\n            \"Could not initialize language model engine:\\n\" + str(e)\n        )\n        return []\n\n    response: QdrantResponse = QdrantResponse(points=[])\n    try:\n        match qdrant_model:\n            case QdrantEmbeddingModel.UUID:\n                records: list[Record] = client.retrieve(\n                    collection_name=collection_name,\n                    ids=[querytext],\n                    with_payload=payload,\n                )\n                points: list[ScoredPoint] = [\n                    ScoredPoint(\n                        id=r.id, version=1, payload=r.payload, score=1\n                    )\n                    for r in records\n                ]\n                response = QdrantResponse(points=points)\n\n            case QdrantEmbeddingModel.DENSE:\n                vect: list[float] = encoder.embed_query(querytext)\n                query_dict: dict[str, Any] = {\n                    'collection_name': collection_name,\n                    'query': vect,\n                    'using': DENSE_VECTOR_NAME,\n                    'with_payload': payload,\n                    'limit': limit,\n                }\n                response = client.query_points(**query_dict)\n\n            case QdrantEmbeddingModel.MULTIVECTOR:\n                vect: list[float] = encoder.embed_query(querytext)\n                query_dict: dict[str, Any] = {\n                    'collection_name': collection_name,\n                    'query': [vect, vect],\n                    'using': DENSE_VECTOR_NAME,\n                    'with_payload': payload,\n                    'limit': limit,\n                }\n                response = client.query_points(**query_dict)\n\n            case QdrantEmbeddingModel.SPARSE:\n                sparse_model = _get_sparse_model(embedding_settings)\n                sparse_embeddings = list(\n                    sparse_model.embed(querytext)\n                )\n                query_dict: dict[str, Any] = {\n                    'collection_name': collection_name,\n                    'query': models.SparseVector(\n                        indices=list(sparse_embeddings[0].indices),\n                        values=list(sparse_embeddings[0].values),\n                    ),\n                    'using': SPARSE_VECTOR_NAME,\n                    'with_payload': payload,\n                    'limit': limit,\n                }\n                response = client.query_points(**query_dict)\n\n            case QdrantEmbeddingModel.HYBRID_DENSE:\n                try:\n                    vect: list[float] = encoder.embed_query(querytext)\n                    sparse_model = _get_sparse_model(\n                        embedding_settings\n                    )\n                    sparse_embeddings = list(\n                        sparse_model.embed(querytext)\n                    )\n                except Exception:\n                    logger.error(\"Could not create encoding\")\n                    return []\n                dense_dict: dict[str, Any] = {\n                    'query': vect,\n                    'using': DENSE_VECTOR_NAME,\n                    'limit': 25,\n                }\n                sparse_dict: dict[str, Any] = {\n                    'query': models.SparseVector(\n                        indices=list(sparse_embeddings[0].indices),\n                        values=list(sparse_embeddings[0].values),\n                    ),\n                    'using': SPARSE_VECTOR_NAME,\n                    'limit': limit,\n                }\n                query_dict: dict[str, Any] = {\n                    'collection_name': collection_name,\n                    'prefetch': [\n                        models.Prefetch(**dense_dict),\n                        models.Prefetch(**sparse_dict),\n                    ],\n                    'query': models.FusionQuery(\n                        fusion=models.Fusion.RRF\n                    ),\n                    'with_payload': payload,\n                }\n                response = client.query_points(**query_dict)\n\n            case QdrantEmbeddingModel.HYBRID_MULTIVECTOR:\n                try:\n                    vect: list[float] = encoder.embed_query(querytext)\n                    sparse_model = _get_sparse_model(\n                        embedding_settings\n                    )\n                    sparse_embeddings = list(\n                        sparse_model.embed(querytext)\n                    )\n                except Exception:\n                    logger.error(\"Could not create encoding\")\n                    return []\n                dense_dict: dict[str, Any] = {\n                    'query': [vect, vect],\n                    'using': DENSE_VECTOR_NAME,\n                    'limit': 25,\n                }\n                sparse_dict: dict[str, Any] = {\n                    'query': models.SparseVector(\n                        indices=list(sparse_embeddings[0].indices),\n                        values=list(sparse_embeddings[0].values),\n                    ),\n                    'using': SPARSE_VECTOR_NAME,\n                    'limit': limit,\n                }\n                query_dict: dict[str, Any] = {\n                    'collection_name': collection_name,\n                    'prefetch': [\n                        models.Prefetch(**dense_dict),\n                        models.Prefetch(**sparse_dict),\n                    ],\n                    'query': models.FusionQuery(\n                        fusion=models.Fusion.RRF\n                    ),\n                    'with_payload': payload,\n                }\n                response = client.query_points(**query_dict)\n\n            case _:\n                raise RuntimeError(\n                    \"Unreachable code reached due to invalid \"\n                    + \"embedding model: \"\n                    + str(qdrant_model)\n                )\n\n    except ConnectionError:\n        logger.error(\n            \"Could not connect to the qdrant server, which may be down.\"\n        )\n        return []\n    except UnexpectedResponse as e:\n        logger.error(f\"Could not read from the vector database: {e}\")\n        return []\n    except ApiException as e:\n        logger.error(\n            f\"Could not read from the database due to API error: {e}\"\n        )\n        return []\n    except Exception as e:\n        logger.error(f\"Could not read from the database: {e}\")\n        return []\n\n    return response.points\n</code></pre>"},{"location":"API/vector_store_qdrant/#lmm_education.stores.vector_store_qdrant.query_grouped","title":"<code>query_grouped(client, collection_name, group_collection, qdrant_model, embedding_settings, querytext, *, limit=4, payload=True, group_size=1, group_field=GROUP_UUID_KEY, logger=default_logger)</code>","text":"<p>Executes a grouped query on the client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>QdrantClient</code> <p>a QdrantClient object</p> required <code>collection_name</code> <code>str</code> <p>the collection to query</p> required <code>group_collection</code> <code>str</code> <p>the companion collection that will provide the output of the query</p> required <code>qdrant_model</code> <code>QdrantEmbeddingModel</code> <p>the qdrant embedding model</p> required <code>embedding_settings</code> <code>EmbeddingSettings</code> <p>the embedding settings</p> required <code>querytext</code> <code>str</code> <p>the target text</p> required <code>limit</code> <code>int</code> <p>max number of chunks retrieved</p> <code>4</code> <code>payload</code> <code>list[str] | bool</code> <p>what properties to be retrieved; defaults to all metadata propoerties. Note that the text is 'page_content', and this will be included unless payload = False. If payload = True, all propertes are included.</p> <code>True</code> <code>group_size</code> <code>int</code> <p>max retrieved output from the group collection</p> <code>1</code> <code>group_field</code> <code>str</code> <p>the filed to group on</p> <code>GROUP_UUID_KEY</code> <code>logger</code> <code>LoggerBase</code> <p>a logger object</p> <code>default_logger</code> <p>Returns:</p> Type Description <code>GroupsResult</code> <p>a list of ScoredPoint objects.</p> Source code in <code>lmm_education/stores/vector_store_qdrant.py</code> <pre><code>def query_grouped(\n    client: QdrantClient,\n    collection_name: str,\n    group_collection: str,\n    qdrant_model: QdrantEmbeddingModel,\n    embedding_settings: EmbeddingSettings,\n    querytext: str,\n    *,\n    limit: int = 4,\n    payload: list[str] | bool = True,\n    group_size: int = 1,\n    group_field: str = GROUP_UUID_KEY,\n    logger: LoggerBase = default_logger,\n) -&gt; GroupsResult:\n    \"\"\"\n    Executes a grouped query on the client.\n\n    Args:\n        client: a QdrantClient object\n        collection_name: the collection to query\n        group_collection: the companion collection that will provide\n            the output of the query\n        qdrant_model: the qdrant embedding model\n        embedding_settings: the embedding settings\n        querytext: the target text\n        limit: max number of chunks retrieved\n        payload: what properties to be retrieved; defaults to all\n            metadata propoerties. Note that the text is 'page_content',\n            and this will be included unless payload = False.\n            If payload = True, all propertes are included.\n        group_size: max retrieved output from the group collection\n        group_field: the filed to group on\n        logger: a logger object\n\n    Returns:\n        a list of ScoredPoint objects.\n    \"\"\"\n\n    # make sure if payload is given, it contains `page_content`\n    if isinstance(payload, list):\n        if 'page_content' not in payload:\n            payload.append('page_content')\n\n    # Essentially, the qdrant API declares different\n    # types for any search API.\n    NullResult: GroupsResult = GroupsResult(groups=[])\n    # load language model\n    from requests.exceptions import ConnectionError\n\n    try:\n        from lmm.models.langchain.runnables import (\n            create_embeddings,\n        )\n\n        encoder: Embeddings = create_embeddings(embedding_settings)\n    except ConnectionError:\n        logger.error(\n            \"Could not connect to language models.\\n\"\n            + \"Check the internet connection.\",\n        )\n        return NullResult\n    except ImportError:\n        logger.error(\n            \"Could not create langchain kernel. Check that langchain \"\n            \"is installed, and an internet connection is available.\"\n        )\n        return NullResult\n    except Exception as e:\n        logger.error(\n            \"Could not initialize language model engine:\\n\" + str(e)\n        )\n        return NullResult\n\n    response: GroupsResult = NullResult\n    try:\n        match qdrant_model:\n            case QdrantEmbeddingModel.UUID:\n                # ignore grouping\n                hits: list[ScoredPoint] = query(\n                    client,\n                    collection_name,\n                    qdrant_model,\n                    embedding_settings,\n                    querytext,\n                    limit=limit,\n                    payload=payload,\n                    logger=logger,\n                )\n                return GroupsResult(\n                    groups=[PointGroup(hits=hits, id=querytext)]\n                )\n\n            case QdrantEmbeddingModel.DENSE:\n                vect = encoder.embed_query(querytext)\n                query_dict: dict[str, Any] = {\n                    'collection_name': collection_name,\n                    'query': vect,\n                    'using': DENSE_VECTOR_NAME,\n                    'group_by': group_field,\n                    'limit': limit,\n                    'group_size': group_size,\n                    'with_lookup': models.WithLookup(\n                        collection=group_collection,\n                        with_payload=payload,\n                        with_vectors=False,\n                    ),\n                }\n                response = client.query_points_groups(**query_dict)\n\n            case QdrantEmbeddingModel.MULTIVECTOR:\n                vect = encoder.embed_query(querytext)\n                query_dict: dict[str, Any] = {\n                    'collection_name': collection_name,\n                    'query': [vect, vect],\n                    'using': DENSE_VECTOR_NAME,\n                    'group_by': group_field,\n                    'limit': limit,\n                    'group_size': group_size,\n                    'with_lookup': models.WithLookup(\n                        collection=group_collection,\n                        with_payload=payload,\n                        with_vectors=False,\n                    ),\n                }\n                response = client.query_points_groups(**query_dict)\n\n            case QdrantEmbeddingModel.SPARSE:\n                sparse_model: SparseTextEmbedding = _get_sparse_model(\n                    embedding_settings\n                )\n                sparse_embeddings: list[SparseEmbedding] = list(\n                    sparse_model.embed(querytext)\n                )\n                query_dict: dict[str, Any] = {\n                    'collection_name': collection_name,\n                    'query': models.SparseVector(\n                        indices=list(sparse_embeddings[0].indices),\n                        values=list(sparse_embeddings[0].values),\n                    ),\n                    'using': SPARSE_VECTOR_NAME,\n                    'group_by': group_field,\n                    'limit': limit,\n                    'group_size': group_size,\n                    'with_lookup': models.WithLookup(\n                        collection=group_collection,\n                        with_payload=payload,\n                        with_vectors=False,\n                    ),\n                }\n                response = client.query_points_groups(**query_dict)\n\n            case QdrantEmbeddingModel.HYBRID_DENSE:\n                try:\n                    vect: list[float] = encoder.embed_query(querytext)\n                    sparse_model: SparseTextEmbedding = (\n                        _get_sparse_model(embedding_settings)\n                    )\n                    sparse_embeddings: list[SparseEmbedding] = list(\n                        sparse_model.embed(querytext)\n                    )\n                except Exception:\n                    logger.error(\"Could not create encoding\")\n                    return NullResult\n                dense_dict: dict[str, Any] = {\n                    'query': vect,\n                    'using': DENSE_VECTOR_NAME,\n                    'limit': 25,\n                }\n                sparse_dict: dict[str, Any] = {\n                    'query': models.SparseVector(\n                        indices=list(sparse_embeddings[0].indices),\n                        values=list(sparse_embeddings[0].values),\n                    ),\n                    'using': SPARSE_VECTOR_NAME,\n                    'limit': limit,\n                }\n                query_dict: dict[str, Any] = {\n                    'collection_name': collection_name,\n                    'prefetch': [\n                        models.Prefetch(**dense_dict),\n                        models.Prefetch(**sparse_dict),\n                    ],\n                    'query': models.FusionQuery(\n                        fusion=models.Fusion.RRF\n                    ),\n                    'group_by': group_field,\n                    'group_size': group_size,\n                    'with_lookup': models.WithLookup(\n                        collection=group_collection,\n                        with_payload=payload,\n                        with_vectors=False,\n                    ),\n                }\n                response = client.query_points_groups(**query_dict)\n\n            case QdrantEmbeddingModel.HYBRID_MULTIVECTOR:\n                try:\n                    vect: list[float] = encoder.embed_query(querytext)\n                    sparse_model: SparseTextEmbedding = (\n                        _get_sparse_model(embedding_settings)\n                    )\n                    sparse_embeddings: list[SparseEmbedding] = list(\n                        sparse_model.embed(querytext)\n                    )\n                except Exception:\n                    logger.error(\"Could not create encoding\")\n                    return NullResult\n                dense_dict: dict[str, Any] = {\n                    'query': [vect, vect],\n                    'using': DENSE_VECTOR_NAME,\n                    'limit': 25,\n                }\n                sparse_dict: dict[str, Any] = {\n                    'query': models.SparseVector(\n                        indices=list(sparse_embeddings[0].indices),\n                        values=list(sparse_embeddings[0].values),\n                    ),\n                    'using': SPARSE_VECTOR_NAME,\n                    'limit': limit,\n                }\n                query_dict: dict[str, Any] = {\n                    'collection_name': collection_name,\n                    'prefetch': [\n                        models.Prefetch(**dense_dict),\n                        models.Prefetch(**sparse_dict),\n                    ],\n                    'query': models.FusionQuery(\n                        fusion=models.Fusion.RRF\n                    ),\n                    'group_by': group_field,\n                    'group_size': group_size,\n                    'with_lookup': models.WithLookup(\n                        collection=group_collection,\n                        with_payload=payload,\n                        with_vectors=False,\n                    ),\n                }\n                response = client.query_points_groups(**query_dict)\n\n            case _:\n                raise RuntimeError(\n                    \"Unreachable code reached due to invalid \"\n                    + \"embedding model: \"\n                    + str(qdrant_model)\n                )\n\n    except ConnectionError:\n        logger.error(\n            \"Could not connect to the qdrant server, which may be down.\"\n        )\n        return NullResult\n    except UnexpectedResponse as e:\n        logger.error(f\"Could not read from the vector database: {e}\")\n        return NullResult\n    except ApiException as e:\n        logger.error(\n            f\"Could not read from the database due to API error: {e}\"\n        )\n        return NullResult\n    except Exception as e:\n        logger.error(f\"Could not read from the database: {e}\")\n        return NullResult\n\n    return response\n</code></pre>"},{"location":"API/vector_store_qdrant/#lmm_education.stores.vector_store_qdrant.upload","title":"<code>upload(client, collection_name, qdrant_model, embedding_settings, chunks, *, logger=default_logger)</code>","text":"<p>Upload a list of chunks into the Qdrant database</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>QdrantClient</code> <p>the connection to the database</p> required <code>collection_name</code> <code>str</code> <p>the collection</p> required <code>qdrant_model</code> <code>QdrantEmbeddingModel</code> <p>the qdrant embedding model</p> required <code>embedding_settings</code> <code>EmbeddingSettings</code> <p>the embedding settings</p> required <code>chunks</code> <code>list[Chunk]</code> <p>the chunk list</p> required <p>Returns:</p> Type Description <code>list[PointStruct]</code> <p>a list of Point objects</p> Source code in <code>lmm_education/stores/vector_store_qdrant.py</code> <pre><code>def upload(\n    client: QdrantClient,\n    collection_name: str,\n    qdrant_model: QdrantEmbeddingModel,\n    embedding_settings: EmbeddingSettings,\n    chunks: list[Chunk],\n    *,\n    logger: LoggerBase = default_logger,\n) -&gt; list[Point]:\n    \"\"\"\n    Upload a list of chunks into the Qdrant database\n\n    Args:\n        client: the connection to the database\n        collection_name: the collection\n        qdrant_model: the qdrant embedding model\n        embedding_settings: the embedding settings\n        chunks: the chunk list\n\n    Returns:\n        a list of Point objects\n    \"\"\"\n\n    if not initialize_collection(\n        client,\n        collection_name,\n        qdrant_model,\n        embedding_settings,\n        logger=logger,\n    ):\n        logger.error(\"Could not initialize collection.\")\n        return []\n\n    points: list[Point] = chunks_to_points(\n        chunks, qdrant_model, embedding_settings, logger=logger\n    )\n\n    if not points:\n        return []\n\n    try:\n        client.upsert(collection_name=collection_name, points=points)\n    except ConnectionError:\n        logger.error(\n            \"Could not connect to the qdrant server, which may be down.\"\n        )\n        return []\n    except UnexpectedResponse as e:\n        logger.error(f\"Could not write to vector database: {e}\")\n        return []\n    except ApiException as e:\n        logger.error(\n            f\"Could not write to database due to API error: {e}\"\n        )\n        return []\n    except Exception as e:\n        logger.error(f\"Could not upload chunks to database:\\n{e}\")\n        return []\n\n    return points\n</code></pre>"},{"location":"API/vector_store_qdrant_context/","title":"Module vector store context","text":"<p>Provides a global context to share a qdrant connection globally. Provides singleton client objects and handles destruction automatically.</p> <p>Global QdrantClient objects may be obtained by calling the functions global_client_from_config and global_async_client_from_config. Both accept a DatabaseSource value to identify the database. If no argument is provided, the database storage in the configuration file config.toml is used.</p> <p>Database source possible example values are: \":memory:\", LocalStorage(folder=\"path_to_storage\"), RemoteStorage(url=127.0.0.1,12324) (see config.py):</p> <pre><code>from lmm_education.stores.vector_store_qdrant_context import (\n    global_async_client_from_config,\n)\n\n# get an async client using default config from config.toml\nclient = global_async_client_from_config()\n</code></pre> <p>Important! Because destruction closes the client connection, which is stored globally, DO NOT close the client obtained rhough this module manually, i.e. by calling client.close().</p> <p>Closing clients is handled automatically, but if you need to close them, use global_clients_close() or global_async_clients_close().</p> Behavior <p>Raises ValueError if creation of Client object fails.</p> <p>options:     show_root_heading: false</p>"},{"location":"API/vector_store_qdrant_context/#lmm_education.stores.vector_store_qdrant_context.global_async_client_from_config","title":"<code>global_async_client_from_config(dbsource=None, logger=_logger)</code>","text":"<p>Override of vector_store_qdrant homonymous function. This version caches a unique link to the database source.</p> <p>Parameters:</p> Name Type Description Default <code>dbsource</code> <code>DatabaseSource | None</code> <p>a DatabaseSource type or None. If None or missing, returns the database as specified in config.toml.</p> <code>None</code> <code>logger</code> <code>LoggerBase</code> <p>a logger object for error reporting.</p> <code>_logger</code> <p>Returns:</p> Type Description <code>AsyncQdrantClient</code> <p>an AsyncQdrantClient object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If settings cannot be loaded from config.toml when dbsource is None, or if the underlying async_client_from_config() function fails to create an asynchronous client.</p> Source code in <code>lmm_education/stores/vector_store_qdrant_context.py</code> <pre><code>def global_async_client_from_config(\n    dbsource: DatabaseSource | None = None,\n    logger: LoggerBase = _logger,\n) -&gt; AsyncQdrantClient:\n    \"\"\"Override of vector_store_qdrant homonymous function.\n    This version caches a unique link to the database source.\n\n    Args:\n        dbsource: a DatabaseSource type or None. If None or missing,\n            returns the database as specified in config.toml.\n        logger: a logger object for error reporting.\n\n    Returns:\n        an AsyncQdrantClient object.\n\n    Raises:\n        ValueError: If settings cannot be loaded from config.toml when\n            dbsource is None, or if the underlying async_client_from_config()\n            function fails to create an asynchronous client.\n    \"\"\"\n\n    if dbsource is None:\n        opts: ConfigSettings | None = load_settings(logger=logger)\n        if opts is None:\n            raise ValueError(\n                \"Could not read settings to create global client.\"\n            )\n        dbsource = opts.storage\n\n    return qdrant_async_clients[dbsource]\n</code></pre>"},{"location":"API/vector_store_qdrant_context/#lmm_education.stores.vector_store_qdrant_context.global_async_clients_close","title":"<code>global_async_clients_close()</code>","text":"<p>Close all asynchronous global clients.</p> <p>This function clears the global client cache, triggering the destructor for each cached client connection.</p> <p>Args: None</p> <p>Returns: None</p> Source code in <code>lmm_education/stores/vector_store_qdrant_context.py</code> <pre><code>def global_async_clients_close() -&gt; None:\n    \"\"\"Close all asynchronous global clients.\n\n    This function clears the global client cache, triggering\n    the destructor for each cached client connection.\n\n    Args: None\n\n    Returns: None\n    \"\"\"\n    qdrant_async_clients.clear()\n</code></pre>"},{"location":"API/vector_store_qdrant_context/#lmm_education.stores.vector_store_qdrant_context.global_client_from_config","title":"<code>global_client_from_config(dbsource=None, logger=_logger)</code>","text":"<p>Override of vector_store_qdrant homonymous function. This version caches a unique link to the database source.</p> <p>Parameters:</p> Name Type Description Default <code>dbsource</code> <code>DatabaseSource | None</code> <p>a DatabaseSource type or None. If None or missing, returns the database as specified in config.toml.</p> <code>None</code> <code>logger</code> <code>LoggerBase</code> <p>a logger object for error reporting.</p> <code>_logger</code> <p>Returns:</p> Type Description <code>QdrantClient</code> <p>a QdrantClient object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If settings cannot be loaded from config.toml when dbsource is None, or if the underlying client_from_config() function fails to create a synchronous client.</p> Source code in <code>lmm_education/stores/vector_store_qdrant_context.py</code> <pre><code>def global_client_from_config(\n    dbsource: DatabaseSource | None = None,\n    logger: LoggerBase = _logger,\n) -&gt; QdrantClient:\n    \"\"\"Override of vector_store_qdrant homonymous function.\n    This version caches a unique link to the database source.\n\n    Args:\n        dbsource: a DatabaseSource type or None. If None or missing,\n            returns the database as specified in config.toml.\n        logger: a logger object for error reporting.\n\n    Returns:\n        a QdrantClient object.\n\n    Raises:\n        ValueError: If settings cannot be loaded from config.toml when\n            dbsource is None, or if the underlying client_from_config()\n            function fails to create a synchronous client.\n    \"\"\"\n\n    if dbsource is None:\n        opts: ConfigSettings | None = load_settings(logger=logger)\n        if opts is None:\n            raise ValueError(\n                \"Could not read settings to create global client.\"\n            )\n        dbsource = opts.storage\n\n    return qdrant_clients[dbsource]\n</code></pre>"},{"location":"API/vector_store_qdrant_context/#lmm_education.stores.vector_store_qdrant_context.global_clients_close","title":"<code>global_clients_close()</code>","text":"<p>Close all synchronous global clients.</p> <p>This function clears the global client cache, triggering the destructor for each cached client connection.</p> <p>Args: None</p> <p>Returns: None</p> Source code in <code>lmm_education/stores/vector_store_qdrant_context.py</code> <pre><code>def global_clients_close() -&gt; None:\n    \"\"\"Close all synchronous global clients.\n\n    This function clears the global client cache, triggering\n    the destructor for each cached client connection.\n\n    Args: None\n\n    Returns: None\n    \"\"\"\n    qdrant_clients.clear()\n</code></pre>"},{"location":"API/vector_store_qdrant_langchain/","title":"Module vector_store_qdrant_langchain.py","text":"<p>A langchain interface to Qdrant vector store retrievers.</p> <p>This module provides multiple retriever implementations for querying Qdrant vector stores through the Langchain framework:</p> <ul> <li>QdrantVectorStoreRetriever: Synchronous retriever for basic queries</li> <li>AsyncQdrantVectorStoreRetriever: Asynchronous retriever for basic     queries</li> <li>QdrantVectorStoreRetrieverGrouped: Synchronous retriever for grouped     queries</li> <li>AsyncQdrantVectorStoreRetrieverGrouped: Asynchronous retriever for     grouped queries</li> </ul> <p>Note: only the query functions are supported (no document insertion).</p> <p>Examples:</p> <pre><code>Basic usage with configuration file:\n\n```python\nretriever = QdrantVectorStoreRetriever.from_config_settings()\nresults: list[Document] = retriever.invoke(\n    \"What are the main uses of logistic regression?\"\n)\n```\n\nManual initialization with all required parameters:\n\n```python\nfrom qdrant_client import QdrantClient\nfrom lmm_education.stores.vector_store_qdrant import QdrantEmbeddingModel\nfrom lmm_education.config.config import ConfigSettings\n\nclient = QdrantClient(\"./storage\")\nconfig = ConfigSettings()\n\nretriever = QdrantVectorStoreRetriever(\n    client,\n    collection_name=\"documents\",\n    qdrant_embedding=QdrantEmbeddingModel.DENSE,\n    embedding_settings=config.embeddings,\n)\nresults: list[Document] = retriever.invoke(\n    \"What are the main uses of logistic regression?\"\n)\n```\n\nUsing the asynchronous retriever:\n\n```python\nfrom qdrant_client import AsyncQdrantClient\n\nasync_retriever = AsyncQdrantVectorStoreRetriever.from_config_settings()\nresults: list[Document] = await async_retriever.ainvoke(\n    \"What are the main uses of logistic regression?\"\n)\n```\n\nUsing grouped queries for document organization:\n\n```python\ngrouped_retriever = QdrantVectorStoreRetrieverGrouped.from_config_settings()\nresults: list[Document] = grouped_retriever.invoke(\n    \"What are the main uses of logistic regression?\"\n)\n```\n</code></pre> <p>options:     show_root_heading: false</p>"},{"location":"API/vector_store_qdrant_langchain/#lmm_education.stores.langchain.vector_store_qdrant_langchain.AsyncQdrantVectorStoreRetriever","title":"<code>AsyncQdrantVectorStoreRetriever</code>","text":"<p>               Bases: <code>BaseRetriever</code></p> <p>Langchain asynchronous retriever interface to the Qdrant vector store.</p> Source code in <code>lmm_education/stores/langchain/vector_store_qdrant_langchain.py</code> <pre><code>class AsyncQdrantVectorStoreRetriever(BaseRetriever):\n    \"\"\"\n    Langchain asynchronous retriever interface to the Qdrant\n    vector store.\n    \"\"\"\n\n    # Implementation note: the default QdrantClient is needed\n    # by the Pydantic constructor, and is just left to garbage\n    # collection. It cannot be await-closed in init.\n    # The real client object is passed by the constructor.\n    # No need to close it in a destructor, as it is passed in.\n\n    client: AsyncQdrantClient = Field(\n        # Pydantic's BaseModel requires an object here\n        default=AsyncQdrantClient(\":memory:\"),\n        init=False,\n    )\n    collection_name: str = Field(default=\"\", init=False)\n    qdrant_embedding: QdrantEmbeddingModel = Field(\n        default=encoding_to_qdrantembedding_model(\n            ConfigSettings().RAG.encoding_model\n        ),\n        init=False,\n    )\n    embedding_settings: EmbeddingSettings = Field(\n        default=ConfigSettings().embeddings, init=False\n    )\n\n    def __init__(\n        self,\n        client: AsyncQdrantClient,\n        collection_name: str,\n        qdrant_embedding: QdrantEmbeddingModel,\n        embedding_settings: EmbeddingSettings,\n    ):\n        # TODO: verify the collection asynchronously\n        # flag: bool = vsq.initialize_collection(\n        #     client, collection_name, embedding_model\n        # )\n        # if not flag:\n        #   raise RuntimeError(\"Could not initialize the collection\")\n\n        super().__init__(\n            metadata={'embedding_model': qdrant_embedding.value}\n        )\n\n        # Now these can be set normally. self.client may already\n        # be initialized, but we can't await close() here.\n        self.client = client\n        self.collection_name = collection_name\n        self.qdrant_embedding = qdrant_embedding\n        self.embedding_settings = embedding_settings\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    async def close_client(self) -&gt; None:\n        \"\"\"Close client. It is a no-op since we delegate\n        closing to the global dict repo.\"\"\"\n        pass\n\n    @staticmethod\n    def from_config_settings(\n        opts: ConfigSettings | None = None,\n        *,\n        client: AsyncQdrantClient | None = None,\n    ) -&gt; BaseRetriever:\n        \"\"\"\n        Initializes a ansynchronous QdrantVectorStoreRetriever from a\n        ConfigSettings object, or from the config.toml file.\n\n        Args:\n            opts: a ConfigSettings object, or none to read settings\n                from the configutation file\n            client: an Async QdrantClient object (optional). If\n                provided, overrides the settings in ConfigSettings\n                to locate the database.\n\n\n        Returns:\n            An AsyncQdrantVectorStoreRetriever object\n        \"\"\"\n        logger = ExceptionConsoleLogger()\n        opts = opts or load_settings(logger=logger)\n        if opts is None:\n            raise ValueError(\n                \"Could not initialize retriever due to \"\n                + \"invalid config settings\"\n            )\n\n        if opts.RAG.retrieve_companion_docs:\n            return AsyncQdrantVectorStoreRetrieverGrouped.from_config_settings(\n                opts, client=client\n            )\n\n        if client is None:\n            # will raise error if not creatable\n            client = global_async_client_from_config(opts.storage)\n        return AsyncQdrantVectorStoreRetriever(\n            client,\n            opts.database.collection_name,\n            encoding_to_qdrantembedding_model(\n                opts.RAG.encoding_model\n            ),\n            embedding_settings=opts.embeddings,\n        )\n\n    def _points_to_documents(\n        self, points: list[vsq.ScoredPoint]\n    ) -&gt; list[Document]:\n        docs: list[Document] = []\n        for p in points:\n            payload = p.payload if p.payload is not None else {}\n            docs.append(\n                Document(\n                    page_content=payload.pop('page_content', \"\"),\n                    metadata=payload.pop('metadata', {}),\n                )\n            )\n\n        return docs\n\n    @override\n    def _get_relevant_documents(\n        self,\n        query: str,\n        *,\n        run_manager: CallbackManagerForRetrieverRun,\n        limit: int = 12,\n        payload: list[str] = ['page_content'],\n    ) -&gt; list[Document]:\n        # this is a required override, so we need to await the async\n        import nest_asyncio  # type: ignore[stubFileNotFound]\n        from lmm.utils.logging import ConsoleLogger\n\n        logger = ConsoleLogger(__name__)\n        logger.warning(\n            \"Sync function in vector store called in async\"\n            + \" context. Are you using .invoke instad of \"\n            + \".ainvoke?\"\n        )\n        nest_asyncio.apply()  # type: ignore\n\n        cpoints: Coroutine[Any, Any, list[vsq.ScoredPoint]] = (\n            vsq.aquery(\n                self.client,\n                self.collection_name,\n                self.qdrant_embedding,\n                self.embedding_settings,\n                query,\n                limit=limit,\n                payload=payload,\n            )\n        )\n\n        loop = asyncio.get_event_loop()\n        task = loop.create_task(cpoints)\n        points: list[vsq.ScoredPoint] = loop.run_until_complete(task)\n\n        return self._points_to_documents(points)\n\n    @override\n    async def _aget_relevant_documents(\n        self,\n        query: str,\n        *,\n        run_manager: AsyncCallbackManagerForRetrieverRun,\n        limit: int = 12,\n        payload: list[str] = ['page_content'],\n    ) -&gt; list[Document]:\n\n        points: list[vsq.ScoredPoint] = await vsq.aquery(\n            self.client,\n            self.collection_name,\n            self.qdrant_embedding,\n            self.embedding_settings,\n            query,\n            limit=limit,\n            payload=payload,\n        )\n\n        return self._points_to_documents(points)\n</code></pre>"},{"location":"API/vector_store_qdrant_langchain/#lmm_education.stores.langchain.vector_store_qdrant_langchain.AsyncQdrantVectorStoreRetriever.close_client","title":"<code>close_client()</code>  <code>async</code>","text":"<p>Close client. It is a no-op since we delegate closing to the global dict repo.</p> Source code in <code>lmm_education/stores/langchain/vector_store_qdrant_langchain.py</code> <pre><code>async def close_client(self) -&gt; None:\n    \"\"\"Close client. It is a no-op since we delegate\n    closing to the global dict repo.\"\"\"\n    pass\n</code></pre>"},{"location":"API/vector_store_qdrant_langchain/#lmm_education.stores.langchain.vector_store_qdrant_langchain.AsyncQdrantVectorStoreRetriever.from_config_settings","title":"<code>from_config_settings(opts=None, *, client=None)</code>  <code>staticmethod</code>","text":"<p>Initializes a ansynchronous QdrantVectorStoreRetriever from a ConfigSettings object, or from the config.toml file.</p> <p>Parameters:</p> Name Type Description Default <code>opts</code> <code>ConfigSettings | None</code> <p>a ConfigSettings object, or none to read settings from the configutation file</p> <code>None</code> <code>client</code> <code>AsyncQdrantClient | None</code> <p>an Async QdrantClient object (optional). If provided, overrides the settings in ConfigSettings to locate the database.</p> <code>None</code> <p>Returns:</p> Type Description <code>BaseRetriever</code> <p>An AsyncQdrantVectorStoreRetriever object</p> Source code in <code>lmm_education/stores/langchain/vector_store_qdrant_langchain.py</code> <pre><code>@staticmethod\ndef from_config_settings(\n    opts: ConfigSettings | None = None,\n    *,\n    client: AsyncQdrantClient | None = None,\n) -&gt; BaseRetriever:\n    \"\"\"\n    Initializes a ansynchronous QdrantVectorStoreRetriever from a\n    ConfigSettings object, or from the config.toml file.\n\n    Args:\n        opts: a ConfigSettings object, or none to read settings\n            from the configutation file\n        client: an Async QdrantClient object (optional). If\n            provided, overrides the settings in ConfigSettings\n            to locate the database.\n\n\n    Returns:\n        An AsyncQdrantVectorStoreRetriever object\n    \"\"\"\n    logger = ExceptionConsoleLogger()\n    opts = opts or load_settings(logger=logger)\n    if opts is None:\n        raise ValueError(\n            \"Could not initialize retriever due to \"\n            + \"invalid config settings\"\n        )\n\n    if opts.RAG.retrieve_companion_docs:\n        return AsyncQdrantVectorStoreRetrieverGrouped.from_config_settings(\n            opts, client=client\n        )\n\n    if client is None:\n        # will raise error if not creatable\n        client = global_async_client_from_config(opts.storage)\n    return AsyncQdrantVectorStoreRetriever(\n        client,\n        opts.database.collection_name,\n        encoding_to_qdrantembedding_model(\n            opts.RAG.encoding_model\n        ),\n        embedding_settings=opts.embeddings,\n    )\n</code></pre>"},{"location":"API/vector_store_qdrant_langchain/#lmm_education.stores.langchain.vector_store_qdrant_langchain.AsyncQdrantVectorStoreRetrieverGrouped","title":"<code>AsyncQdrantVectorStoreRetrieverGrouped</code>","text":"<p>               Bases: <code>BaseRetriever</code></p> <p>Langchain asynchronous retriever interface to the Qdrant vector store for grouped queries.</p> Source code in <code>lmm_education/stores/langchain/vector_store_qdrant_langchain.py</code> <pre><code>class AsyncQdrantVectorStoreRetrieverGrouped(BaseRetriever):\n    \"\"\"\n    Langchain asynchronous retriever interface to the Qdrant vector\n    store for grouped queries.\n    \"\"\"\n\n    # Implementation note: the default QdrantClient is needed\n    # by the Pydantic constructor, and is just left to garbage\n    # collection. It cannot be await-closed in init.\n    # The real client object is passed by the constructor.\n    # No need to close it in a destructor, as it is passed in.\n\n    client: AsyncQdrantClient = Field(\n        # Pydantic's BaseModel requires an object here\n        default=AsyncQdrantClient(\":memory:\"),\n        init=False,\n    )\n    collection_name: str = Field(default=\"\", init=False)\n    group_collection: str = Field(default=\"\", init=False)\n    group_field: str = Field(default=GROUP_UUID_KEY, init=False)\n    limitgroups: int = Field(default=4, init=False)\n    qdrant_embedding: QdrantEmbeddingModel = Field(\n        default=encoding_to_qdrantembedding_model(\n            ConfigSettings().RAG.encoding_model\n        ),\n        init=False,\n    )\n    embedding_settings: EmbeddingSettings = Field(\n        default=ConfigSettings().embeddings, init=False\n    )\n\n    def __init__(\n        self,\n        client: AsyncQdrantClient,\n        collection_name: str,\n        group_collection: str,\n        group_field: str,\n        limitgroups: int,\n        qdrant_embedding: vsq.QdrantEmbeddingModel,\n        embedding_settings: EmbeddingSettings,\n    ):\n        # TODO: verify the collection asynchronously\n        # flag: bool = vsq.initialize_collection(\n        #     client, collection_name, embedding_model\n        # )\n        # if not flag:\n        #   raise RuntimeError(\"Could not initialize the collection\")\n\n        super().__init__(\n            metadata={'embedding_model': qdrant_embedding.value}\n        )\n\n        # Now these can be set normally. self.client may already\n        # be initialized, but we can't await close() here.\n        self.client = client\n        self.collection_name = collection_name\n        self.group_collection = group_collection\n        self.group_field = group_field\n        self.limitgroups = limitgroups\n        self.qdrant_embedding = qdrant_embedding\n        self.embedding_settings = embedding_settings\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    async def close_client(self) -&gt; None:\n        \"\"\"Close client. It is a no-op since we delegate\n        closing to the global dict repo.\"\"\"\n        pass\n\n    @staticmethod\n    def from_config_settings(\n        opts: ConfigSettings | None = None,\n        *,\n        client: AsyncQdrantClient | None = None,\n    ) -&gt; BaseRetriever:\n        \"\"\"\n        Initializes a ansynchronous QdrantVectorStoreRetrieverGrouped\n        instance from a ConfigSettings object, or from the\n        config.toml file.\n\n        Args:\n            opts: a ConfigSettings object, or none to read settings\n                from the configutation file\n\n        Returns:\n            An AsyncQdrantVectorStoreRetrieverGrouped object\n        \"\"\"\n        from lmm.scan.scan_keys import GROUP_UUID_KEY\n\n        logger = ExceptionConsoleLogger()\n        opts = opts or load_settings(logger=logger)\n        if opts is None:\n            raise ValueError(\n                \"Could not initialize retriever due to \"\n                + \"invalid config settings\"\n            )\n        dbOpts: DatabaseSettings = opts.database\n        retrieve_docs: bool = opts.RAG.retrieve_companion_docs\n\n        if retrieve_docs and not bool(dbOpts.companion_collection):\n            logger.warning(\n                \"Retrieve docs directive ignored, no companion collection\"\n            )\n            retrieve_docs = False\n\n        if not retrieve_docs:\n            # Directly instantiate to avoid infinite recursion\n            # (opts still has retrieve_companion_docs=True)\n            if client is None:\n                client = global_async_client_from_config(opts.storage)\n            return AsyncQdrantVectorStoreRetriever(\n                client,\n                dbOpts.collection_name,\n                encoding_to_qdrantembedding_model(\n                    opts.RAG.encoding_model\n                ),\n                opts.embeddings,\n            )\n\n        if client is None:\n            # will raise error if not creatable\n            client = global_async_client_from_config(opts.storage)\n        return AsyncQdrantVectorStoreRetrieverGrouped(\n            client,\n            dbOpts.collection_name,\n            dbOpts.companion_collection,  # type: ignore (checked above)\n            GROUP_UUID_KEY,\n            opts.RAG.max_companion_docs,\n            encoding_to_qdrantembedding_model(\n                opts.RAG.encoding_model\n            ),\n            opts.embeddings,\n        )\n\n    def _results_to_documents(\n        self, results: vsq.GroupsResult\n    ) -&gt; list[Document]:\n        docs: list[Document] = []\n        result_points: list[vsq.ScoredPoint] = vsq.groups_to_points(\n            results\n        )\n        for p in result_points:\n            payload = p.payload or {}\n            metadata = payload.get('metadata', {})\n            context_summary: str = metadata.pop(CTXT_SUMMARY_KEY, \"\")\n            if context_summary:\n                context_summary += \"\\n\\n\"\n            docs.append(\n                Document(\n                    page_content=context_summary\n                    + payload.pop('page_content', \"\"),\n                    metadata=payload.pop('metadata', {}),\n                )\n            )\n\n        return docs\n\n    @override\n    def _get_relevant_documents(\n        self,\n        query: str,\n        *,\n        run_manager: CallbackManagerForRetrieverRun,\n        limit: int = 4,\n        payload: list[str] | bool = True,\n    ) -&gt; list[Document]:\n        # this is a required override, so we need to await the async\n        import nest_asyncio  # type: ignore[stubFileNotFound]\n        from lmm.utils.logging import ConsoleLogger\n\n        logger = ConsoleLogger(__name__)\n        logger.warning(\n            \"Sync function in vector store called in async\"\n            + \" context. Are you using .invoke instad of \"\n            + \".ainvoke?\"\n        )\n\n        if isinstance(payload, list):\n            if 'page_content' not in payload:\n                payload.append('page_content')\n\n        nest_asyncio.apply()  # type: ignore\n\n        gresults: Coroutine[Any, Any, vsq.GroupsResult] = (\n            vsq.aquery_grouped(\n                self.client,\n                self.collection_name,\n                self.group_collection,\n                self.qdrant_embedding,\n                self.embedding_settings,\n                query,\n                group_field=self.group_field,\n                limit=limit,\n                group_size=self.limitgroups,\n                payload=payload,\n            )\n        )\n\n        loop = asyncio.get_event_loop()\n        task = loop.create_task(gresults)\n        results: vsq.GroupsResult = loop.run_until_complete(task)\n\n        return self._results_to_documents(results)\n\n    @override\n    async def _aget_relevant_documents(\n        self,\n        query: str,\n        *,\n        run_manager: AsyncCallbackManagerForRetrieverRun,\n        limit: int = 4,\n        payload: list[str] | bool = True,\n    ) -&gt; list[Document]:\n\n        if isinstance(payload, list):\n            if 'page_content' not in payload:\n                payload.append('page_content')\n\n        results: vsq.GroupsResult = await vsq.aquery_grouped(\n            self.client,\n            self.collection_name,\n            self.group_collection,\n            self.qdrant_embedding,\n            self.embedding_settings,\n            query,\n            limit=limit,\n            group_field=self.group_field,\n            group_size=self.limitgroups,\n            payload=payload,\n        )\n\n        return self._results_to_documents(results)\n</code></pre>"},{"location":"API/vector_store_qdrant_langchain/#lmm_education.stores.langchain.vector_store_qdrant_langchain.AsyncQdrantVectorStoreRetrieverGrouped.close_client","title":"<code>close_client()</code>  <code>async</code>","text":"<p>Close client. It is a no-op since we delegate closing to the global dict repo.</p> Source code in <code>lmm_education/stores/langchain/vector_store_qdrant_langchain.py</code> <pre><code>async def close_client(self) -&gt; None:\n    \"\"\"Close client. It is a no-op since we delegate\n    closing to the global dict repo.\"\"\"\n    pass\n</code></pre>"},{"location":"API/vector_store_qdrant_langchain/#lmm_education.stores.langchain.vector_store_qdrant_langchain.AsyncQdrantVectorStoreRetrieverGrouped.from_config_settings","title":"<code>from_config_settings(opts=None, *, client=None)</code>  <code>staticmethod</code>","text":"<p>Initializes a ansynchronous QdrantVectorStoreRetrieverGrouped instance from a ConfigSettings object, or from the config.toml file.</p> <p>Parameters:</p> Name Type Description Default <code>opts</code> <code>ConfigSettings | None</code> <p>a ConfigSettings object, or none to read settings from the configutation file</p> <code>None</code> <p>Returns:</p> Type Description <code>BaseRetriever</code> <p>An AsyncQdrantVectorStoreRetrieverGrouped object</p> Source code in <code>lmm_education/stores/langchain/vector_store_qdrant_langchain.py</code> <pre><code>@staticmethod\ndef from_config_settings(\n    opts: ConfigSettings | None = None,\n    *,\n    client: AsyncQdrantClient | None = None,\n) -&gt; BaseRetriever:\n    \"\"\"\n    Initializes a ansynchronous QdrantVectorStoreRetrieverGrouped\n    instance from a ConfigSettings object, or from the\n    config.toml file.\n\n    Args:\n        opts: a ConfigSettings object, or none to read settings\n            from the configutation file\n\n    Returns:\n        An AsyncQdrantVectorStoreRetrieverGrouped object\n    \"\"\"\n    from lmm.scan.scan_keys import GROUP_UUID_KEY\n\n    logger = ExceptionConsoleLogger()\n    opts = opts or load_settings(logger=logger)\n    if opts is None:\n        raise ValueError(\n            \"Could not initialize retriever due to \"\n            + \"invalid config settings\"\n        )\n    dbOpts: DatabaseSettings = opts.database\n    retrieve_docs: bool = opts.RAG.retrieve_companion_docs\n\n    if retrieve_docs and not bool(dbOpts.companion_collection):\n        logger.warning(\n            \"Retrieve docs directive ignored, no companion collection\"\n        )\n        retrieve_docs = False\n\n    if not retrieve_docs:\n        # Directly instantiate to avoid infinite recursion\n        # (opts still has retrieve_companion_docs=True)\n        if client is None:\n            client = global_async_client_from_config(opts.storage)\n        return AsyncQdrantVectorStoreRetriever(\n            client,\n            dbOpts.collection_name,\n            encoding_to_qdrantembedding_model(\n                opts.RAG.encoding_model\n            ),\n            opts.embeddings,\n        )\n\n    if client is None:\n        # will raise error if not creatable\n        client = global_async_client_from_config(opts.storage)\n    return AsyncQdrantVectorStoreRetrieverGrouped(\n        client,\n        dbOpts.collection_name,\n        dbOpts.companion_collection,  # type: ignore (checked above)\n        GROUP_UUID_KEY,\n        opts.RAG.max_companion_docs,\n        encoding_to_qdrantembedding_model(\n            opts.RAG.encoding_model\n        ),\n        opts.embeddings,\n    )\n</code></pre>"},{"location":"API/vector_store_qdrant_langchain/#lmm_education.stores.langchain.vector_store_qdrant_langchain.QdrantVectorStoreRetriever","title":"<code>QdrantVectorStoreRetriever</code>","text":"<p>               Bases: <code>BaseRetriever</code></p> <p>Langchain retriever interface to the Qdrant vector store.</p> Source code in <code>lmm_education/stores/langchain/vector_store_qdrant_langchain.py</code> <pre><code>class QdrantVectorStoreRetriever(BaseRetriever):\n    \"\"\"\n    Langchain retriever interface to the Qdrant vector store.\n    \"\"\"\n\n    # Implementation note: the default QdrantClient is needed\n    # by the Pydantic constructor, and is closed immediately.\n    # The real client object is passed by the constructor.\n    # Do not close it in a destructor, as it is passed in.\n\n    client: QdrantClient = Field(\n        # Pydantic's BaseModel requires an object here\n        default=QdrantClient(\":memory:\"),\n        init=False,\n    )\n    collection_name: str = Field(default=\"\", init=False)\n    qdrant_embedding: QdrantEmbeddingModel = Field(\n        default=encoding_to_qdrantembedding_model(\n            ConfigSettings().RAG.encoding_model\n        ),\n        init=False,\n    )\n    embedding_settings: EmbeddingSettings = Field(\n        default=ConfigSettings().embeddings, init=False\n    )\n\n    def __init__(\n        self,\n        client: QdrantClient,\n        collection_name: str,\n        qdrant_embedding: QdrantEmbeddingModel,\n        embedding_settings: EmbeddingSettings,\n    ):\n        flag: bool = vsq.initialize_collection(\n            client,\n            collection_name,\n            qdrant_embedding,\n            embedding_settings,\n        )\n        if not flag:\n            raise RuntimeError(\"Could not initialize the collection\")\n\n        super().__init__(\n            metadata={'embedding_model': qdrant_embedding.value}\n        )\n\n        # Now these can be set normally, except that self.client\n        # may have already been initialized\n        try:\n            self.client.close()\n        except Exception:\n            pass\n        self.client = client\n        self.collection_name = collection_name\n        self.qdrant_embedding = qdrant_embedding\n        self.embedding_settings = embedding_settings\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def close_client(self) -&gt; None:\n        \"\"\"Close client. It is a no-op since we delegate\n        closing to the global dict repo.\"\"\"\n        pass\n\n    @staticmethod\n    def from_config_settings(\n        opts: ConfigSettings | None = None,\n        *,\n        client: QdrantClient | None = None,\n    ) -&gt; BaseRetriever:\n        \"\"\"\n        Initializes a QdrantVectorStoreRetriever from a ConfigSettings\n        object, or from the config.toml file.\n\n        Args:\n            opts: a ConfigSettings object, or none to read settings\n                from the configutation file\n            client: a QdrantClient object (optional). If provided,\n                overrides the settings in ConfigSettings to locate\n                the database.\n\n        Returns:\n            A QdrantVectorStoreRetriever object\n        \"\"\"\n        logger = ExceptionConsoleLogger()\n        opts = opts or load_settings(logger=logger)\n        if opts is None:\n            raise ValueError(\n                \"Could not initialize retriever due to \"\n                + \"invalid config settings\"\n            )\n\n        if opts.RAG.retrieve_companion_docs:\n            return QdrantVectorStoreRetrieverGrouped.from_config_settings(\n                opts, client=client\n            )\n\n        if client is None:\n            # will raise error if not creatable\n            client = global_client_from_config(opts.storage)\n        return QdrantVectorStoreRetriever(\n            client,\n            opts.database.collection_name,\n            encoding_to_qdrantembedding_model(\n                opts.RAG.encoding_model\n            ),\n            opts.embeddings,\n        )\n\n    def _points_to_documents(\n        self, points: list[vsq.ScoredPoint]\n    ) -&gt; list[Document]:\n        docs: list[Document] = []\n        for p in points:\n            payload = p.payload if p.payload is not None else {}\n            docs.append(\n                Document(\n                    page_content=payload.pop('page_content', \"\"),\n                    metadata=payload.pop('metadata', {}),\n                )\n            )\n\n        return docs\n\n    @override\n    def _get_relevant_documents(\n        self,\n        query: str,\n        *,\n        run_manager: CallbackManagerForRetrieverRun,\n        limit: int = 12,\n        payload: list[str] = ['page_content'],\n    ) -&gt; list[Document]:\n        logger = ExceptionConsoleLogger()\n        points: list[vsq.ScoredPoint] = vsq.query(\n            self.client,\n            self.collection_name,\n            self.qdrant_embedding,\n            self.embedding_settings,\n            query,\n            limit=limit,\n            payload=payload,\n            logger=logger,\n        )\n\n        return self._points_to_documents(points)\n</code></pre>"},{"location":"API/vector_store_qdrant_langchain/#lmm_education.stores.langchain.vector_store_qdrant_langchain.QdrantVectorStoreRetriever.close_client","title":"<code>close_client()</code>","text":"<p>Close client. It is a no-op since we delegate closing to the global dict repo.</p> Source code in <code>lmm_education/stores/langchain/vector_store_qdrant_langchain.py</code> <pre><code>def close_client(self) -&gt; None:\n    \"\"\"Close client. It is a no-op since we delegate\n    closing to the global dict repo.\"\"\"\n    pass\n</code></pre>"},{"location":"API/vector_store_qdrant_langchain/#lmm_education.stores.langchain.vector_store_qdrant_langchain.QdrantVectorStoreRetriever.from_config_settings","title":"<code>from_config_settings(opts=None, *, client=None)</code>  <code>staticmethod</code>","text":"<p>Initializes a QdrantVectorStoreRetriever from a ConfigSettings object, or from the config.toml file.</p> <p>Parameters:</p> Name Type Description Default <code>opts</code> <code>ConfigSettings | None</code> <p>a ConfigSettings object, or none to read settings from the configutation file</p> <code>None</code> <code>client</code> <code>QdrantClient | None</code> <p>a QdrantClient object (optional). If provided, overrides the settings in ConfigSettings to locate the database.</p> <code>None</code> <p>Returns:</p> Type Description <code>BaseRetriever</code> <p>A QdrantVectorStoreRetriever object</p> Source code in <code>lmm_education/stores/langchain/vector_store_qdrant_langchain.py</code> <pre><code>@staticmethod\ndef from_config_settings(\n    opts: ConfigSettings | None = None,\n    *,\n    client: QdrantClient | None = None,\n) -&gt; BaseRetriever:\n    \"\"\"\n    Initializes a QdrantVectorStoreRetriever from a ConfigSettings\n    object, or from the config.toml file.\n\n    Args:\n        opts: a ConfigSettings object, or none to read settings\n            from the configutation file\n        client: a QdrantClient object (optional). If provided,\n            overrides the settings in ConfigSettings to locate\n            the database.\n\n    Returns:\n        A QdrantVectorStoreRetriever object\n    \"\"\"\n    logger = ExceptionConsoleLogger()\n    opts = opts or load_settings(logger=logger)\n    if opts is None:\n        raise ValueError(\n            \"Could not initialize retriever due to \"\n            + \"invalid config settings\"\n        )\n\n    if opts.RAG.retrieve_companion_docs:\n        return QdrantVectorStoreRetrieverGrouped.from_config_settings(\n            opts, client=client\n        )\n\n    if client is None:\n        # will raise error if not creatable\n        client = global_client_from_config(opts.storage)\n    return QdrantVectorStoreRetriever(\n        client,\n        opts.database.collection_name,\n        encoding_to_qdrantembedding_model(\n            opts.RAG.encoding_model\n        ),\n        opts.embeddings,\n    )\n</code></pre>"},{"location":"API/vector_store_qdrant_langchain/#lmm_education.stores.langchain.vector_store_qdrant_langchain.QdrantVectorStoreRetrieverGrouped","title":"<code>QdrantVectorStoreRetrieverGrouped</code>","text":"<p>               Bases: <code>BaseRetriever</code></p> <p>Langchain retriever interface to the Qdrant vector store for grouped queries.</p> Source code in <code>lmm_education/stores/langchain/vector_store_qdrant_langchain.py</code> <pre><code>class QdrantVectorStoreRetrieverGrouped(BaseRetriever):\n    \"\"\"\n    Langchain retriever interface to the Qdrant vector store\n    for grouped queries.\n    \"\"\"\n\n    # Implementation note: the default QdrantClient is needed\n    # by the Pydantic constructor, and is closed immediately.\n    # The real client object is passed by the constructor.\n    # Do not close it in a destructor, as it is passed in.\n\n    client: QdrantClient = Field(\n        # Pydantic's BaseModel requires an object here\n        default=QdrantClient(\":memory:\"),\n        init=False,\n    )\n    collection_name: str = Field(default=\"\", init=False)\n    group_collection: str = Field(default=\"\", init=False)\n    group_field: str = Field(default=GROUP_UUID_KEY, init=False)\n    limitgroups: int = Field(default=4, init=False)\n    qdrant_embedding: QdrantEmbeddingModel = Field(\n        default=encoding_to_qdrantembedding_model(\n            ConfigSettings().RAG.encoding_model\n        ),\n        init=False,\n    )\n    embedding_settings: EmbeddingSettings = Field(\n        default=ConfigSettings().embeddings, init=False\n    )\n\n    def __init__(\n        self,\n        client: QdrantClient,\n        collection_name: str,\n        group_collection: str,\n        group_field: str,\n        limitgroups: int,\n        qdrant_embedding: QdrantEmbeddingModel,\n        embedding_settings: EmbeddingSettings,\n    ):\n        flag: bool = vsq.initialize_collection(\n            client,\n            collection_name,\n            qdrant_embedding,\n            embedding_settings,\n        )\n        if not flag:\n            raise RuntimeError(\"Could not initialize the collection\")\n\n        super().__init__(\n            metadata={'embedding_model': qdrant_embedding.value}\n        )\n\n        # Now these can be set normally, except that self.client\n        # may have already been initialized\n        try:\n            self.client.close()\n        except Exception:\n            pass\n        self.client = client\n        self.collection_name = collection_name\n        self.group_collection = group_collection\n        self.group_field = group_field\n        self.limitgroups = limitgroups\n        self.qdrant_embedding = qdrant_embedding\n        self.embedding_settings = embedding_settings\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def close_client(self) -&gt; None:\n        \"\"\"Close client. It is a no-op since we delegate\n        closing to the global dict repo.\"\"\"\n        pass\n\n    @staticmethod\n    def from_config_settings(\n        opts: ConfigSettings | None = None,\n        *,\n        client: QdrantClient | None = None,\n    ) -&gt; BaseRetriever:\n        \"\"\"\n        Initializes a QdrantVectorStoreRetrieverGrouped from a\n        ConfigSettings object, or from the config.toml file.\n\n        Args:\n            opts: a ConfigSettings object, or none to read settings\n                from the configutation file\n\n        Returns:\n            A QdrantVectorStoreRetrieverGrouped object\n        \"\"\"\n        from lmm.scan.scan_keys import GROUP_UUID_KEY\n\n        logger = ExceptionConsoleLogger()\n        opts = opts or load_settings(logger=logger)\n        if opts is None:\n            raise ValueError(\n                \"Could not initialize retriever due to \"\n                + \"invalid config settings\"\n            )\n        dbOpts: DatabaseSettings = opts.database\n        retrieve_docs: bool = opts.RAG.retrieve_companion_docs\n\n        if retrieve_docs and not bool(dbOpts.companion_collection):\n            logger.warning(\n                \"Retrieve docs directive ignored, no companion collection\"\n            )\n            retrieve_docs = False\n\n        if not retrieve_docs:\n            # Directly instantiate to avoid infinite recursion\n            # (opts still has retrieve_companion_docs=True)\n            if client is None:\n                client = global_client_from_config(opts.storage)\n            return QdrantVectorStoreRetriever(\n                client,\n                dbOpts.collection_name,\n                encoding_to_qdrantembedding_model(\n                    opts.RAG.encoding_model\n                ),\n                opts.embeddings,\n            )\n\n        if client is None:\n            # will raise error if not creatable\n            client = global_client_from_config(opts.storage)\n        return QdrantVectorStoreRetrieverGrouped(\n            client,\n            dbOpts.collection_name,\n            dbOpts.companion_collection,  # type: ignore (checked above)\n            GROUP_UUID_KEY,\n            opts.RAG.max_companion_docs,\n            encoding_to_qdrantembedding_model(\n                opts.RAG.encoding_model\n            ),\n            opts.embeddings,\n        )\n\n    def _results_to_documents(\n        self, results: vsq.GroupsResult\n    ) -&gt; list[Document]:\n        docs: list[Document] = []\n        result_points: list[vsq.ScoredPoint] = vsq.groups_to_points(\n            results\n        )\n        for p in result_points:\n            payload = p.payload or {}\n            metadata = payload.get('metadata', {})\n            context_summary: str = metadata.pop(CTXT_SUMMARY_KEY, \"\")\n            if context_summary:\n                context_summary += \"\\n\\n\"\n            docs.append(\n                Document(\n                    page_content=context_summary\n                    + payload.pop('page_content', \"\"),\n                    metadata=payload.pop('metadata', {}),\n                )\n            )\n\n        return docs\n\n    @override\n    def _get_relevant_documents(\n        self,\n        query: str,\n        *,\n        run_manager: CallbackManagerForRetrieverRun,\n        limit: int = 4,\n        payload: list[str] | bool = True,\n    ) -&gt; list[Document]:\n\n        results: vsq.GroupsResult = vsq.query_grouped(\n            self.client,\n            self.collection_name,\n            self.group_collection,\n            self.qdrant_embedding,\n            self.embedding_settings,\n            query,\n            group_field=self.group_field,\n            group_size=self.limitgroups,\n            limit=limit,\n            payload=payload,\n        )\n\n        return self._results_to_documents(results)\n</code></pre>"},{"location":"API/vector_store_qdrant_langchain/#lmm_education.stores.langchain.vector_store_qdrant_langchain.QdrantVectorStoreRetrieverGrouped.close_client","title":"<code>close_client()</code>","text":"<p>Close client. It is a no-op since we delegate closing to the global dict repo.</p> Source code in <code>lmm_education/stores/langchain/vector_store_qdrant_langchain.py</code> <pre><code>def close_client(self) -&gt; None:\n    \"\"\"Close client. It is a no-op since we delegate\n    closing to the global dict repo.\"\"\"\n    pass\n</code></pre>"},{"location":"API/vector_store_qdrant_langchain/#lmm_education.stores.langchain.vector_store_qdrant_langchain.QdrantVectorStoreRetrieverGrouped.from_config_settings","title":"<code>from_config_settings(opts=None, *, client=None)</code>  <code>staticmethod</code>","text":"<p>Initializes a QdrantVectorStoreRetrieverGrouped from a ConfigSettings object, or from the config.toml file.</p> <p>Parameters:</p> Name Type Description Default <code>opts</code> <code>ConfigSettings | None</code> <p>a ConfigSettings object, or none to read settings from the configutation file</p> <code>None</code> <p>Returns:</p> Type Description <code>BaseRetriever</code> <p>A QdrantVectorStoreRetrieverGrouped object</p> Source code in <code>lmm_education/stores/langchain/vector_store_qdrant_langchain.py</code> <pre><code>@staticmethod\ndef from_config_settings(\n    opts: ConfigSettings | None = None,\n    *,\n    client: QdrantClient | None = None,\n) -&gt; BaseRetriever:\n    \"\"\"\n    Initializes a QdrantVectorStoreRetrieverGrouped from a\n    ConfigSettings object, or from the config.toml file.\n\n    Args:\n        opts: a ConfigSettings object, or none to read settings\n            from the configutation file\n\n    Returns:\n        A QdrantVectorStoreRetrieverGrouped object\n    \"\"\"\n    from lmm.scan.scan_keys import GROUP_UUID_KEY\n\n    logger = ExceptionConsoleLogger()\n    opts = opts or load_settings(logger=logger)\n    if opts is None:\n        raise ValueError(\n            \"Could not initialize retriever due to \"\n            + \"invalid config settings\"\n        )\n    dbOpts: DatabaseSettings = opts.database\n    retrieve_docs: bool = opts.RAG.retrieve_companion_docs\n\n    if retrieve_docs and not bool(dbOpts.companion_collection):\n        logger.warning(\n            \"Retrieve docs directive ignored, no companion collection\"\n        )\n        retrieve_docs = False\n\n    if not retrieve_docs:\n        # Directly instantiate to avoid infinite recursion\n        # (opts still has retrieve_companion_docs=True)\n        if client is None:\n            client = global_client_from_config(opts.storage)\n        return QdrantVectorStoreRetriever(\n            client,\n            dbOpts.collection_name,\n            encoding_to_qdrantembedding_model(\n                opts.RAG.encoding_model\n            ),\n            opts.embeddings,\n        )\n\n    if client is None:\n        # will raise error if not creatable\n        client = global_client_from_config(opts.storage)\n    return QdrantVectorStoreRetrieverGrouped(\n        client,\n        dbOpts.collection_name,\n        dbOpts.companion_collection,  # type: ignore (checked above)\n        GROUP_UUID_KEY,\n        opts.RAG.max_companion_docs,\n        encoding_to_qdrantembedding_model(\n            opts.RAG.encoding_model\n        ),\n        opts.embeddings,\n    )\n</code></pre>"},{"location":"API/vector_store_qdrant_utils/","title":"Utilities","text":"<p>Support functions for the vector_store_qdrant module.</p> <p>Main functions:</p> <ul> <li>check_schema/acheck_schema: add or check the schema of a collection</li> <li>get_schema/aget_schema: a utility to inspect the schema of a collection.</li> </ul> <p>A schema consists of the qdrant embedding model enum selection and the embedding settings. Schemas are collection-specific.</p> Behaviour <p>Functions use a custom <code>Logger</code> class from the <code>lmm</code> package for logging errors and information.</p> <p>options:     show_root_heading: false</p>"},{"location":"API/vector_store_qdrant_utils/#lmm_education.stores.vector_store_qdrant_utils.acheck_schema","title":"<code>acheck_schema(client, collection_name, qdrant_model, embedding_settings, *, logger=default_logger)</code>  <code>async</code>","text":"<p>Performs a check that the collection has been initialized with the correct qdrant model and embedding settings.</p> <p>If no schema exists for the collection, it will add the schema. Hence, this function may be called after creating a collection to add the schema to the database.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncQdrantClient</code> <p>the qdrant client</p> required <code>collection_name</code> <code>str</code> <p>the collection to check</p> required <code>qdrant_model</code> <code>Enum</code> <p>the model of the collection</p> required <code>embedding_settings</code> <code>EmbeddingSettings</code> <p>the settings of the collection</p> required <code>logger</code> <code>LoggerBase</code> <p>a logger object</p> <code>default_logger</code> <p>Returns:</p> Type Description <code>bool</code> <p>a boolean flag indicating success (True) or failure (False)</p> <p>Examples:</p> <pre><code>from qdrant_client import AsyncQdrantClient\nfrom lmm.config.config import EmbeddingSettings\nfrom lmm_education.stores.vector_store_qdrant import QdrantEmbeddingModel\n\nclient = AsyncQdrantClient(\":memory:\")\nsettings = EmbeddingSettings(model=\"text-embedding-3-small\")\nsuccess = await acheck_schema(client, \"my_collection\",\n                             QdrantEmbeddingModel.OPENAI, settings)\n</code></pre> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If unable to connect to Qdrant server</p> <code>UnexpectedResponse</code> <p>If Qdrant returns an unexpected response</p> <code>ApiException</code> <p>If Qdrant API raises an error</p> Note <p>This is a low-level function for internal use.</p> Source code in <code>lmm_education/stores/vector_store_qdrant_utils.py</code> <pre><code>async def acheck_schema(\n    client: AsyncQdrantClient,\n    collection_name: str,\n    qdrant_model: Enum,\n    embedding_settings: EmbeddingSettings,\n    *,\n    logger: LoggerBase = default_logger,\n) -&gt; bool:\n    \"\"\"\n    Performs a check that the collection has been initialized\n    with the correct qdrant model and embedding settings.\n\n    If no schema exists for the collection, it will add the\n    schema. Hence, this function may be called after creating\n    a collection to add the schema to the database.\n\n    Args:\n        client: the qdrant client\n        collection_name: the collection to check\n        qdrant_model: the model of the collection\n        embedding_settings: the settings of the collection\n        logger: a logger object\n\n    Returns:\n        a boolean flag indicating success (True) or failure (False)\n\n    Examples:\n        ```python\n        from qdrant_client import AsyncQdrantClient\n        from lmm.config.config import EmbeddingSettings\n        from lmm_education.stores.vector_store_qdrant import QdrantEmbeddingModel\n\n        client = AsyncQdrantClient(\":memory:\")\n        settings = EmbeddingSettings(model=\"text-embedding-3-small\")\n        success = await acheck_schema(client, \"my_collection\",\n                                     QdrantEmbeddingModel.OPENAI, settings)\n        ```\n\n    Raises:\n        ConnectionError: If unable to connect to Qdrant server\n        UnexpectedResponse: If Qdrant returns an unexpected response\n        ApiException: If Qdrant API raises an error\n\n    Note:\n        This is a low-level function for internal use.\n    \"\"\"\n\n    try:\n        if not await client.collection_exists(SCHEMA_COLLECTION_NAME):\n            flag: bool = await client.create_collection(\n                collection_name=SCHEMA_COLLECTION_NAME,\n                vectors_config={},\n            )\n            if flag:\n                logger.info(\"Schema collection created.\")\n            else:\n                logger.error(\n                    \"Schema collection could not be created.\"\n                )\n                return False\n\n        uuid: str = generate_uuid(collection_name)\n        payload = {\n            'qdrant_embedding_model': qdrant_model.value,\n            'embeddings': (\n                {}\n                if qdrant_model.value == \"UUID\"\n                else embedding_settings.model_dump(mode=\"json\")\n            ),\n        }\n\n        records: list[Record] = await client.retrieve(\n            collection_name=SCHEMA_COLLECTION_NAME,\n            ids=[uuid],\n            with_payload=True,\n        )\n\n        if not records:\n            if not await client.collection_exists(collection_name):\n                logger.error(\n                    f\"The collection {collection_name} is \"\n                    \"not present in the database\"\n                )\n                return False\n\n            pt = Point(id=uuid, vector={}, payload=payload)\n            result = await client.upsert(SCHEMA_COLLECTION_NAME, [pt])\n            if result.status == \"completed\":\n                logger.info(f\"Schema added for {collection_name}\")\n            else:\n                logger.info(\n                    f\"Attempted schema registation {collection_name}\"\n                )\n        else:\n            if records[0].payload is None:\n                logger.error(\n                    \"System error. The internal schema\"\n                    f\" for collection {collection_name}\"\n                    \" is corrupted. Repeat call to recreate\"\n                    \" schema with current settings.\"\n                )\n                await client.delete(\n                    SCHEMA_COLLECTION_NAME,\n                    [uuid],\n                )\n                return False\n            check = find_dictionary_differences(\n                payload, records[0].payload\n            )\n            if check:\n                logger.error(\n                    format_difference_report(\n                        check,\n                        collection_name,\n                    )\n                )\n                return False\n\n        return True\n    except ConnectionError:\n        logger.error(\n            \"Could not connect to the qdrant server, which may be down.\"\n        )\n        raise\n    except UnexpectedResponse as e:\n        logger.error(f\"Could not access vector database: {e}\")\n        raise\n    except ApiException as e:\n        logger.error(\n            f\"Could not access vector database due to API error: {e}\"\n        )\n        raise\n</code></pre>"},{"location":"API/vector_store_qdrant_utils/#lmm_education.stores.vector_store_qdrant_utils.adatabase_info","title":"<code>adatabase_info(client=None, *, logger=default_logger)</code>  <code>async</code>","text":"<p>Utility to extract information on the database. The utility looks in config.toml to figure out what collections should be present, and displays information on their existence and their schema.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncQdrantClient | None</code> <p>a qdrant client, or None to instantiate one with the settings from config.toml</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, object]</code> <p>a dictionary with database storage location and collection information,</p> <code>dict[str, object]</code> <p>including schema details for each configured collection</p> Source code in <code>lmm_education/stores/vector_store_qdrant_utils.py</code> <pre><code>async def adatabase_info(\n    client: AsyncQdrantClient | None = None,\n    *,\n    logger: LoggerBase = default_logger,\n) -&gt; dict[str, object]:\n    \"\"\"\n    Utility to extract information on the database. The utility looks\n    in config.toml to figure out what collections should be present,\n    and displays information on their existence and their schema.\n\n    Args:\n        client: a qdrant client, or None to instantiate one with the\n            settings from config.toml\n\n    Returns:\n        a dictionary with database storage location and collection information,\n        including schema details for each configured collection\n    \"\"\"\n    from lmm_education.config.config import (\n        ConfigSettings,\n        LocalStorage,\n        RemoteSource,\n    )\n\n    create_flag: bool = False\n    try:\n        config = ConfigSettings()\n\n        if client is None:\n            match config.storage:\n                case \":memory:\":\n                    client = AsyncQdrantClient(\":memory:\")\n                case LocalStorage(folder=folder):\n                    path = Path(folder)\n                    if not path.exists():\n                        return {\n                            'storage': f\"the configured database {folder} does not exist yet.\"\n                        }\n                    client = AsyncQdrantClient(path=folder)\n                case RemoteSource(url=url, port=port):\n                    client = AsyncQdrantClient(\n                        url=str(url), port=port\n                    )\n            create_flag = True\n\n        main_collection: str = config.database.collection_name\n        comp_collection: str | None = (\n            config.database.companion_collection\n        )\n        inits = client.init_options\n\n        async def _coll_info(\n            collection: str,\n        ) -&gt; str | dict[str, object]:\n            if not collection:\n                return \"not in config\"\n            if await client.collection_exists(collection):\n                schema = await aget_schema(client, collection)\n                return {\n                    collection: (schema if schema else \"no schema\")\n                }\n\n            else:\n                return \"not present\"\n\n        info: dict[str, object]\n        if await client.collection_exists(SCHEMA_COLLECTION_NAME):\n            info = {\n                'storage': inits['location']\n                or inits['path']\n                or (inits['url'] + \":\" + inits['port']),\n                'schema_collection': \"present\",\n                'main_collection': await _coll_info(main_collection),\n            }\n        else:\n            info = {\n                'storage': inits['location']\n                or inits['path']\n                or (inits['url'] + \":\" + inits['port']),\n                'schema_collection': \"none\",\n                'main_collection': await _coll_info(main_collection),\n            }\n        if comp_collection:\n            info['companion_collection'] = await _coll_info(\n                comp_collection\n            )\n\n        return info\n    except Exception as e:\n        logger.error(f\"Could not read database: {e}\")\n        return {}\n    finally:\n        if create_flag and client:\n            await client.close()\n</code></pre>"},{"location":"API/vector_store_qdrant_utils/#lmm_education.stores.vector_store_qdrant_utils.aget_schema","title":"<code>aget_schema(client, collection_name, *, logger=default_logger)</code>  <code>async</code>","text":"<p>Retrieves the schema for a collection.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncQdrantClient</code> <p>the qdrant client</p> required <code>collection_name</code> <code>str</code> <p>the name of the collection</p> required <code>logger</code> <code>LoggerBase</code> <p>a logger object</p> <code>default_logger</code> <p>Returns:</p> Type Description <code>dict[str, Any] | None</code> <p>a dictionary with the qdrant embedding model and</p> <code>dict[str, Any] | None</code> <p>vector embedding settings, or None if the collection</p> <code>dict[str, Any] | None</code> <p>does not exist, or errors are raised.</p> <p>Examples:</p> <pre><code>from qdrant_client import AsyncQdrantClient\n\nclient = AsyncQdrantClient(\":memory:\")\nschema = await aget_schema(client, \"my_collection\")\nif schema:\n    print(f\"Model: {schema['qdrant_embedding_model']}\")\n    print(f\"Settings: {schema['embeddings']}\")\n</code></pre> Source code in <code>lmm_education/stores/vector_store_qdrant_utils.py</code> <pre><code>async def aget_schema(\n    client: AsyncQdrantClient,\n    collection_name: str,\n    *,\n    logger: LoggerBase = default_logger,\n) -&gt; dict[str, Any] | None:\n    \"\"\"\n    Retrieves the schema for a collection.\n\n    Args:\n        client: the qdrant client\n        collection_name: the name of the collection\n        logger: a logger object\n\n    Returns:\n        a dictionary with the qdrant embedding model and\n        vector embedding settings, or None if the collection\n        does not exist, or errors are raised.\n\n    Examples:\n        ```python\n        from qdrant_client import AsyncQdrantClient\n\n        client = AsyncQdrantClient(\":memory:\")\n        schema = await aget_schema(client, \"my_collection\")\n        if schema:\n            print(f\"Model: {schema['qdrant_embedding_model']}\")\n            print(f\"Settings: {schema['embeddings']}\")\n        ```\n    \"\"\"\n\n    try:\n        if not await client.collection_exists(collection_name):\n            logger.info(f\"{collection_name} is not in the database.\")\n            return None\n\n        if not await client.collection_exists(SCHEMA_COLLECTION_NAME):\n            logger.error(\n                \"System error. The internal schema\"\n                \" is corrupted. Call initialize_collection on\"\n                \" a collection to reinitialize.\"\n            )\n            return None\n\n        uuid: str = generate_uuid(collection_name)\n\n        records: list[Record] = await client.retrieve(\n            collection_name=SCHEMA_COLLECTION_NAME,\n            ids=[uuid],\n            with_payload=True,\n        )\n\n        if (not records) or (records[0].payload is None):\n            logger.error(\n                \"System error. The internal schema\"\n                f\" for collection {collection_name}\"\n                \" is corrupted. Repeat call to recreate\"\n                \" schema with current settings.\"\n            )\n            return None\n    except ConnectionError:\n        logger.error(\n            \"Could not connect to the qdrant server, which may be down.\"\n        )\n        return None\n    except UnexpectedResponse as e:\n        logger.error(f\"Could not initialize vector database: {e}\")\n        return None\n    except ApiException as e:\n        logger.error(\n            f\"Could not initialize vector database due to API error: {e}\"\n        )\n        return None\n\n    return records[0].payload\n</code></pre>"},{"location":"API/vector_store_qdrant_utils/#lmm_education.stores.vector_store_qdrant_utils.alist_property_values","title":"<code>alist_property_values(client, property, collection, *, logger=default_logger)</code>  <code>async</code>","text":"<p>List all unique values and their counts for a given property in a collection.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncQdrantClient</code> <p>the async qdrant client</p> required <code>property</code> <code>str</code> <p>the property name to query (under metadata)</p> required <code>collection</code> <code>str</code> <p>the collection name</p> required <code>logger</code> <code>LoggerBase</code> <p>a logger object</p> <code>default_logger</code> <p>Returns:</p> Type Description <code>list[tuple[str, int]]</code> <p>a list of tuples containing (value, count) pairs for the property</p> Source code in <code>lmm_education/stores/vector_store_qdrant_utils.py</code> <pre><code>async def alist_property_values(\n    client: AsyncQdrantClient,\n    property: str,\n    collection: str,\n    *,\n    logger: LoggerBase = default_logger,\n) -&gt; list[tuple[str, int]]:\n    \"\"\"\n    List all unique values and their counts for a given property in a collection.\n\n    Args:\n        client: the async qdrant client\n        property: the property name to query (under metadata)\n        collection: the collection name\n        logger: a logger object\n\n    Returns:\n        a list of tuples containing (value, count) pairs for the property\n    \"\"\"\n    try:\n        result: FacetResponse = await client.facet(\n            collection, \"metadata.\" + property\n        )\n        return [(str(h.value), h.count) for h in result.hits]\n    except Exception as e:\n        logger.error(str(e))\n        return []\n</code></pre>"},{"location":"API/vector_store_qdrant_utils/#lmm_education.stores.vector_store_qdrant_utils.check_schema","title":"<code>check_schema(client, collection_name, qdrant_model, embedding_settings, *, logger=default_logger)</code>","text":"<p>Performs a check that the collection has been initialized with the correct qdrant model and embedding settings.</p> <p>If no schema exists for the collection, it will add the schema. Hence, this function may be called after creating a collection to add the schema to the database.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>QdrantClient</code> <p>the qdrant client</p> required <code>collection_name</code> <code>str</code> <p>the collection to check</p> required <code>qdrant_model</code> <code>Enum</code> <p>the model of the collection</p> required <code>embedding_settings</code> <code>EmbeddingSettings</code> <p>the settings of the collection</p> required <code>logger</code> <code>LoggerBase</code> <p>a logger object</p> <code>default_logger</code> <p>Returns:</p> Type Description <code>bool</code> <p>a boolean flag indicating success (True) or failure (False)</p> <p>Examples:</p> <pre><code>from qdrant_client import QdrantClient\nfrom lmm.config.config import EmbeddingSettings\nfrom lmm_education.stores.vector_store_qdrant import QdrantEmbeddingModel\n\nclient = QdrantClient(\":memory:\")\nsettings = EmbeddingSettings(model=\"text-embedding-3-small\")\nsuccess = check_schema(client, \"my_collection\",\n                      QdrantEmbeddingModel.OPENAI, settings)\n</code></pre> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If unable to connect to Qdrant server</p> <code>UnexpectedResponse</code> <p>If Qdrant returns an unexpected response</p> <code>ApiException</code> <p>If Qdrant API raises an error</p> Note <p>This is a low-level function for internal use.</p> Source code in <code>lmm_education/stores/vector_store_qdrant_utils.py</code> <pre><code>def check_schema(\n    client: QdrantClient,\n    collection_name: str,\n    qdrant_model: Enum,\n    embedding_settings: EmbeddingSettings,\n    *,\n    logger: LoggerBase = default_logger,\n) -&gt; bool:\n    \"\"\"\n    Performs a check that the collection has been initialized\n    with the correct qdrant model and embedding settings.\n\n    If no schema exists for the collection, it will add the\n    schema. Hence, this function may be called after creating\n    a collection to add the schema to the database.\n\n    Args:\n        client: the qdrant client\n        collection_name: the collection to check\n        qdrant_model: the model of the collection\n        embedding_settings: the settings of the collection\n        logger: a logger object\n\n    Returns:\n        a boolean flag indicating success (True) or failure (False)\n\n    Examples:\n        ```python\n        from qdrant_client import QdrantClient\n        from lmm.config.config import EmbeddingSettings\n        from lmm_education.stores.vector_store_qdrant import QdrantEmbeddingModel\n\n        client = QdrantClient(\":memory:\")\n        settings = EmbeddingSettings(model=\"text-embedding-3-small\")\n        success = check_schema(client, \"my_collection\",\n                              QdrantEmbeddingModel.OPENAI, settings)\n        ```\n\n    Raises:\n        ConnectionError: If unable to connect to Qdrant server\n        UnexpectedResponse: If Qdrant returns an unexpected response\n        ApiException: If Qdrant API raises an error\n\n    Note:\n        This is a low-level function for internal use.\n    \"\"\"\n\n    try:\n        if not client.collection_exists(SCHEMA_COLLECTION_NAME):\n            flag: bool = client.create_collection(\n                collection_name=SCHEMA_COLLECTION_NAME,\n                vectors_config={},\n            )\n            if flag:\n                logger.info(\"Schema collection created.\")\n            else:\n                logger.error(\n                    \"Schema collection could not be created.\"\n                )\n                return False\n\n        uuid: str = generate_uuid(collection_name)\n        payload = {\n            'qdrant_embedding_model': qdrant_model.value,\n            'embeddings': (\n                {}\n                if qdrant_model.value == \"UUID\"\n                else embedding_settings.model_dump(mode=\"json\")\n            ),\n        }\n\n        records: list[Record] = client.retrieve(\n            collection_name=SCHEMA_COLLECTION_NAME,\n            ids=[uuid],\n            with_payload=True,\n        )\n\n        if not records:\n            if not client.collection_exists(collection_name):\n                logger.error(\n                    f\"The collection {collection_name} is \"\n                    \"not present in the database\"\n                )\n                return False\n\n            pt = Point(id=uuid, vector={}, payload=payload)\n            result: UpdateResult = client.upsert(\n                SCHEMA_COLLECTION_NAME, [pt]\n            )\n            if result.status == \"completed\":\n                logger.info(f\"Schema added for {collection_name}\")\n            else:\n                logger.info(\n                    f\"Attempted schema registation {collection_name}\"\n                )\n        else:\n            if not records[0].payload:\n                logger.error(\n                    \"System error. The internal schema\"\n                    f\" for collection {collection_name}\"\n                    \" is corrupted. Repeat call to recreate\"\n                    \" schema with current settings.\"\n                )\n                client.delete(\n                    SCHEMA_COLLECTION_NAME,\n                    [uuid],\n                )\n                return False\n            delta = find_dictionary_differences(\n                payload, records[0].payload\n            )\n            if delta:\n                logger.error(\n                    format_difference_report(\n                        delta,\n                        collection_name,\n                    )\n                )\n                return False\n\n        return True\n    except ConnectionError:\n        logger.error(\n            \"Could not connect to the qdrant server, which may be down.\"\n        )\n        raise\n    except UnexpectedResponse as e:\n        logger.error(f\"Could not access vector database: {e}\")\n        raise\n    except ApiException as e:\n        logger.error(\n            f\"Could not access vector database due to API error: {e}\"\n        )\n        raise\n</code></pre>"},{"location":"API/vector_store_qdrant_utils/#lmm_education.stores.vector_store_qdrant_utils.database_info","title":"<code>database_info(client=None, *, logger=default_logger)</code>","text":"<p>Utility to extract information on the database.</p> <p>Inspects config.toml to determine expected collections and displays information on their existence and schema.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>QdrantClient | None</code> <p>a qdrant client, or None to instantiate one with the settings from config.toml</p> <code>None</code> <code>logger</code> <code>LoggerBase</code> <p>a logger object</p> <code>default_logger</code> <p>Returns:</p> Type Description <code>dict[str, object]</code> <p>a dictionary with database storage location and collection information</p> Source code in <code>lmm_education/stores/vector_store_qdrant_utils.py</code> <pre><code>def database_info(\n    client: QdrantClient | None = None,\n    *,\n    logger: LoggerBase = default_logger,\n) -&gt; dict[str, object]:\n    \"\"\"\n    Utility to extract information on the database.\n\n    Inspects config.toml to determine expected collections and displays\n    information on their existence and schema.\n\n    Args:\n        client: a qdrant client, or None to instantiate one with the\n            settings from config.toml\n        logger: a logger object\n\n    Returns:\n        a dictionary with database storage location and collection information\n    \"\"\"\n    from lmm_education.config.config import (\n        ConfigSettings,\n        LocalStorage,\n        RemoteSource,\n    )\n\n    create_flag: bool = False\n    try:\n        config = ConfigSettings()\n\n        if client is None:\n            match config.storage:\n                case \":memory:\":\n                    client = QdrantClient(\":memory:\")\n                case LocalStorage(folder=folder):\n                    path = Path(folder)\n                    if not path.exists():\n                        return {\n                            'storage': f\"the configured database {folder} does not exist yet.\"\n                        }\n                    client = QdrantClient(path=folder)\n                case RemoteSource(url=url, port=port):\n                    client = QdrantClient(url=str(url), port=port)\n            create_flag = True\n\n        main_collection: str = config.database.collection_name\n        comp_collection: str | None = (\n            config.database.companion_collection\n        )\n        inits = client.init_options\n\n        def _coll_info(collection: str) -&gt; str | dict[str, object]:\n            if not collection:\n                return \"not in config\"\n            if client.collection_exists(collection):\n                schema = get_schema(client, collection)\n                return {\n                    collection: (schema if schema else \"no schema\")\n                }\n\n            else:\n                return \"not present\"\n\n        info: dict[str, object]\n        if client.collection_exists(SCHEMA_COLLECTION_NAME):\n            info = {\n                'storage': inits['location']\n                or inits['path']\n                or (inits['url'] + \":\" + inits['port']),\n                'schema_collection': \"present\",\n                'main_collection': _coll_info(main_collection),\n            }\n        else:\n            info = {\n                'storage': inits['location']\n                or inits['path']\n                or (inits['url'] + \":\" + inits['port']),\n                'schema_collection': \"none\",\n                'main_collection': _coll_info(main_collection),\n            }\n        if comp_collection:\n            info['companion_collection'] = _coll_info(comp_collection)\n\n        return info\n    except Exception as e:\n        logger.error(f\"Could not read database: {e}\")\n        return {}\n    finally:\n        if create_flag and client:\n            client.close()\n</code></pre>"},{"location":"API/vector_store_qdrant_utils/#lmm_education.stores.vector_store_qdrant_utils.database_name","title":"<code>database_name(client)</code>","text":"<p>Extract the database name/location from a Qdrant client.</p> Source code in <code>lmm_education/stores/vector_store_qdrant_utils.py</code> <pre><code>def database_name(\n    client: QdrantClient | AsyncQdrantClient,\n) -&gt; str:\n    \"\"\"Extract the database name/location from a Qdrant client.\"\"\"\n    return (\n        client.init_options['path']\n        or client.init_options['location']\n        or client.init_options['url']\n        or \"&lt;unknown&gt;\"\n    )\n</code></pre>"},{"location":"API/vector_store_qdrant_utils/#lmm_education.stores.vector_store_qdrant_utils.get_schema","title":"<code>get_schema(client, collection_name, *, logger=default_logger)</code>","text":"<p>Retrieves the schema for a collection.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>QdrantClient</code> <p>the qdrant client</p> required <code>collection_name</code> <code>str</code> <p>the name of the collection</p> required <code>logger</code> <code>LoggerBase</code> <p>a logger object</p> <code>default_logger</code> <p>Returns:</p> Type Description <code>dict[str, object] | None</code> <p>a dictionary with the qdrant embedding model and</p> <code>dict[str, object] | None</code> <p>vector embedding settings, or None if the collection</p> <code>dict[str, object] | None</code> <p>does not exist, or errors are raised.</p> <p>Examples:</p> <pre><code>from qdrant_client import QdrantClient\n\nclient = QdrantClient(\":memory:\")\nschema = get_schema(client, \"my_collection\")\nif schema:\n    print(f\"Model: {schema['qdrant_embedding_model']}\")\n    print(f\"Settings: {schema['embeddings']}\")\n</code></pre> Source code in <code>lmm_education/stores/vector_store_qdrant_utils.py</code> <pre><code>def get_schema(\n    client: QdrantClient,\n    collection_name: str,\n    *,\n    logger: LoggerBase = default_logger,\n) -&gt; dict[str, object] | None:\n    \"\"\"\n    Retrieves the schema for a collection.\n\n    Args:\n        client: the qdrant client\n        collection_name: the name of the collection\n        logger: a logger object\n\n    Returns:\n        a dictionary with the qdrant embedding model and\n        vector embedding settings, or None if the collection\n        does not exist, or errors are raised.\n\n    Examples:\n        ```python\n        from qdrant_client import QdrantClient\n\n        client = QdrantClient(\":memory:\")\n        schema = get_schema(client, \"my_collection\")\n        if schema:\n            print(f\"Model: {schema['qdrant_embedding_model']}\")\n            print(f\"Settings: {schema['embeddings']}\")\n        ```\n    \"\"\"\n\n    try:\n        if not client.collection_exists(collection_name):\n            logger.info(f\"{collection_name} is not in the database.\")\n            return None\n\n        if not client.collection_exists(SCHEMA_COLLECTION_NAME):\n            logger.error(\n                \"System error. The internal schema\"\n                \" is corrupted. Call initialize_collection on\"\n                \" a collection to reinitialize.\"\n            )\n            return None\n\n        uuid: str = generate_uuid(collection_name)\n\n        records: list[Record] = client.retrieve(\n            collection_name=SCHEMA_COLLECTION_NAME,\n            ids=[uuid],\n            with_payload=True,\n        )\n\n        if (not records) or (records[0].payload is None):\n            logger.error(\n                \"System error. The internal schema\"\n                f\" for collection {collection_name}\"\n                \" is corrupted or missing. Call initialize_collection\"\n                f\" on {collection_name} to restore schema with current\"\n                \" settings.\"\n            )\n            return None\n    except ConnectionError:\n        logger.error(\n            \"Could not connect to the qdrant server, which may be down.\"\n        )\n        return None\n    except UnexpectedResponse as e:\n        logger.error(f\"Could not initialize vector database: {e}\")\n        return None\n    except ApiException as e:\n        logger.error(\n            f\"Could not initialize vector database due to API error: {e}\"\n        )\n        return None\n\n    return records[0].payload\n</code></pre>"},{"location":"API/vector_store_qdrant_utils/#lmm_education.stores.vector_store_qdrant_utils.list_property_values","title":"<code>list_property_values(client, property, collection, *, logger=default_logger)</code>","text":"<p>List all unique values and their counts for a given property in a collection.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>QdrantClient</code> <p>the qdrant client</p> required <code>property</code> <code>str</code> <p>the property name to query (under metadata)</p> required <code>collection</code> <code>str</code> <p>the collection name</p> required <code>logger</code> <code>LoggerBase</code> <p>a logger object</p> <code>default_logger</code> <p>Returns:</p> Type Description <code>list[tuple[str, int]]</code> <p>a list of tuples containing (value, count) pairs for the property</p> Source code in <code>lmm_education/stores/vector_store_qdrant_utils.py</code> <pre><code>def list_property_values(\n    client: QdrantClient,\n    property: str,\n    collection: str,\n    *,\n    logger: LoggerBase = default_logger,\n) -&gt; list[tuple[str, int]]:\n    \"\"\"\n    List all unique values and their counts for a given property in a collection.\n\n    Args:\n        client: the qdrant client\n        property: the property name to query (under metadata)\n        collection: the collection name\n        logger: a logger object\n\n    Returns:\n        a list of tuples containing (value, count) pairs for the property\n    \"\"\"\n    try:\n        result: FacetResponse = client.facet(\n            collection, \"metadata.\" + property\n        )\n        return [(str(h.value), h.count) for h in result.hits]\n    except Exception as e:\n        logger.error(str(e))\n        return []\n</code></pre>"},{"location":"API/workflow_base/","title":"Base","text":"<p>workflows.langchain.base module. Definitions and utility functions for LangChain graph models.</p> <p>Important definitions:</p> <ul> <li>ChatState and ChatWorkflContext: state and context for graphs</li> <li>create_initial_state: default initialized state</li> <li>graph_logger: a function to log ChatState to a .csv database</li> </ul> <p>options:     show_root_heading: false</p>"},{"location":"API/workflow_base/#lmm_education.workflows.langchain.base.create_initial_state","title":"<code>create_initial_state(querytext, *, messages=None, history=None, timestamp=None)</code>","text":"<p>Create initial ChatState from query and optional history.</p> <p>Parameters:</p> Name Type Description Default <code>querytext</code> <code>str</code> <p>The user's query text</p> required <code>history</code> <code>list[dict[str, str]] | None</code> <p>optional Gradio-format conversation history</p> <code>None</code> <code>timestamp</code> <code>datetime | None</code> <p>optional timestamp (will be provided if omitted)</p> <code>None</code> <p>Returns:</p> Type Description <code>ChatState</code> <p>ChatState initialized for the workflow</p> Source code in <code>lmm_education/workflows/langchain/base.py</code> <pre><code>def create_initial_state(\n    querytext: str,\n    *,\n    messages: (\n        list[HumanMessage | AIMessage | BaseMessage] | None\n    ) = None,\n    history: list[dict[str, str]] | None = None,\n    timestamp: datetime | None = None,\n) -&gt; ChatState:\n    \"\"\"\n    Create initial ChatState from query and optional history.\n\n    Args:\n        querytext: The user's query text\n        history: optional Gradio-format conversation history\n        timestamp: optional timestamp (will be provided if\n            omitted)\n\n    Returns:\n        ChatState initialized for the workflow\n    \"\"\"\n\n    return ChatState(\n        messages=messages or [],\n        status=\"valid\",  # Will be validated by workflow\n        model_identification=\"&lt;unknown&gt;\",\n        tool_call_count=0,\n        query=querytext,\n        refined_query=\"\",\n        query_classification=\"\",\n        context=\"\",\n        response=\"\",\n        timestamp=timestamp or datetime.now(),\n        time_to_context=None,\n        time_to_FB=None,\n        time_to_response=None,\n    )\n</code></pre>"},{"location":"API/workflow_base/#lmm_education.workflows.langchain.base.graph_logger","title":"<code>graph_logger(state, database, context, client_host='&lt;unknown&gt;', session_hash='&lt;unknown&gt;', timestamp=None, record_id=None)</code>  <code>async</code>","text":"<p>Log to database function.</p> <p>Extracts information from the state and context and delegates to the database interface. Adds context validation information to the log.</p> Source code in <code>lmm_education/workflows/langchain/base.py</code> <pre><code>async def graph_logger(\n    state: ChatState,\n    database: ChatDatabaseInterface,\n    context: ChatWorkflowContext,\n    client_host: str = \"&lt;unknown&gt;\",\n    session_hash: str = \"&lt;unknown&gt;\",\n    timestamp: datetime | None = None,\n    record_id: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Log to database function.\n\n    Extracts information from the state and context and delegates to\n    the database interface. Adds context validation information to the\n    log.\n    \"\"\"\n    # Deferred imports to avoid circular import with base.py\n    # from .base import ChatState, ChatWorkflowContext\n\n    if timestamp is None:\n        timestamp = datetime.now()\n\n    if record_id is None:\n        record_id = generate_random_string()\n\n    logger: LoggerBase = context.logger\n\n    # info from state and context\n    model_name: str = state.get(\"model_identification\") or \"&lt;unknown&gt;\"\n\n    messages: list[BaseMessage] = state.get(\"messages\", [])\n    status = state.get(\"status\", \"error\")\n\n    # Extract info from state\n    response: str = state.get(\"response\", \"\")\n    query: str = state.get(\"query\", \"\")\n    classification: str = (\n        state.get(\"query_classification\", \"\") or \"NA\"\n    )\n    message_count = len(messages)\n\n    # audit system performance\n    time_to_context: float | None = state.get(\"time_to_context\", None)\n    time_to_FB: float | None = state.get(\"time_to_FB\", None)\n    time_to_response: float | None = state.get(\n        \"time_to_response\", None\n    )\n\n    try:\n        match status:\n            case \"valid\":\n                # Check for context\n                context_text: str = state.get(\"context\", \"\")\n\n                if context_text:\n                    # Evaluate consistency of context prior to saving\n                    validation: str = \"&lt;unknown&gt;\"\n                    try:\n                        lmm_validator: RunnableType = create_runnable(\n                            \"context_validator\"  # will be a lookup\n                        )\n                        validation_res: str = (\n                            await lmm_validator.ainvoke(\n                                {\n                                    \"query\": f\"{query}. {response}\",\n                                    \"context\": context_text,\n                                }\n                            )\n                        )\n                        validation = str(\n                            validation_res.strip().upper()\n                        )\n                    except Exception as e:\n                        logger.error(\n                            f\"Could not connect to aux model to validate context: {e}\"\n                        )\n                        validation = \"&lt;failed&gt;\"\n\n                    await database.log_message_with_context(\n                        record_id=record_id,\n                        client_host=client_host,\n                        session_hash=session_hash,\n                        timestamp=timestamp,\n                        message_count=message_count,\n                        model_name=model_name,\n                        interaction_type=\"MESSAGES\",\n                        query=query,\n                        response=response,\n                        validation=validation,\n                        context=context_text,\n                        classification=classification,\n                        time_to_context=time_to_context,\n                        time_to_FB=time_to_FB,\n                        time_to_response=time_to_response,\n                    )\n                else:\n                    await database.log_message(\n                        record_id=record_id,\n                        client_host=client_host,\n                        session_hash=session_hash,\n                        timestamp=timestamp,\n                        message_count=message_count,\n                        model_name=model_name,\n                        interaction_type=\"MESSAGES\",\n                        query=query,\n                        response=response,\n                        time_to_FB=time_to_FB,\n                        time_to_response=time_to_response,\n                    )\n\n            case \"empty_query\":\n                await database.log_message(\n                    record_id=record_id,\n                    client_host=client_host,\n                    session_hash=session_hash,\n                    timestamp=timestamp,\n                    message_count=message_count,\n                    model_name=\"\",\n                    interaction_type=\"EMPTYQUERY\",\n                    query=\"\",\n                    response=response,\n                )\n\n            case \"long_query\":\n                await database.log_message(\n                    record_id=record_id,\n                    client_host=client_host,\n                    session_hash=session_hash,\n                    timestamp=timestamp,\n                    message_count=message_count,\n                    model_name=\"\",\n                    interaction_type=\"LONGQUERY\",\n                    query=(\n                        query[:1500] + \"...\"\n                        if len(query) &gt; 1500\n                        else query\n                    ),\n                    response=response,\n                )\n\n            case \"rejected\":\n                # Check for context\n                context_text: str = state.get(\"context\", \"\")\n\n                await database.log_message_with_context(\n                    record_id=record_id,\n                    client_host=client_host,\n                    session_hash=session_hash,\n                    timestamp=timestamp,\n                    message_count=message_count,\n                    model_name=model_name,\n                    interaction_type=\"REJECTED\",\n                    query=query,\n                    response=response,\n                    validation=\"NA\",\n                    context=context_text,\n                    classification=classification,\n                )\n\n            case _:  # ignore all others\n                pass\n\n    except Exception as e:\n        logger.error(f\"Async logging failed: {e}\")\n</code></pre>"},{"location":"API/workflow_base/#lmm_education.workflows.langchain.base.prepare_messages_for_llm","title":"<code>prepare_messages_for_llm(state, context, system_message='', history_window=None)</code>","text":"<p>Prepare messages for LLM invocation from state.</p> <p>This replaces the _prepare_messages function, using state instead of the history list.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>ChatState</code> <p>Current chat state</p> required <code>system_message</code> <code>str</code> <p>Optional system message to prepend</p> <code>''</code> <code>history_window</code> <code>int | None</code> <p>Number of recent messages to include (defaults to 4)</p> <code>None</code> <p>Returns:</p> Type Description <code>list[tuple[str, str]]</code> <p>List of (role, content) tuples for LLM invocation</p> Source code in <code>lmm_education/workflows/langchain/base.py</code> <pre><code>def prepare_messages_for_llm(\n    state: ChatState,\n    context: ChatWorkflowContext,\n    system_message: str = \"\",\n    history_window: int | None = None,\n) -&gt; list[tuple[str, str]]:\n    \"\"\"\n    Prepare messages for LLM invocation from state.\n\n    This replaces the _prepare_messages function, using state instead\n    of the history list.\n\n    Args:\n        state: Current chat state\n        system_message: Optional system message to prepend\n        history_window: Number of recent messages to include\n            (defaults to 4)\n\n    Returns:\n        List of (role, content) tuples for LLM invocation\n    \"\"\"\n    if history_window is None:\n        history_window = context.chat_settings.history_length\n\n    messages: list[tuple[str, str]] = []\n\n    if system_message:\n        messages.append((\"system\", system_message))\n\n    # Add recent conversation history\n    state_messages: list[BaseMessage] = state.get(\"messages\", [])\n    for msg in state_messages[-history_window:]:\n        role: str = (\n            \"user\" if isinstance(msg, HumanMessage) else \"assistant\"\n        )\n        content: str = \"\"\n        try:\n            content = str(msg.content)  # type: ignore (dict type)\n        except Exception:\n            pass\n        messages.append((role, content))\n\n    # Add the current formatted query\n    messages.append((\"user\", state[\"refined_query\"]))\n\n    return messages\n</code></pre>"},{"location":"API/workflow_factory/","title":"Workflow factory","text":"<p>Creates LangGraph workflows.</p> <p>options:     show_root_heading: false</p>"},{"location":"API/workflow_factory/#lmm_education.workflows.workflow_factory.workflow_factory","title":"<code>workflow_factory(workflow_name, workflow_context, settings=None, logger=ConsoleLogger())</code>","text":"<p>Factory function to retrieve compiled workflow graphs.</p> <p>Parameters:</p> Name Type Description Default <code>workflow_name</code> <code>str</code> <p>the name of the workflow to load.</p> required <code>settings</code> <code>ConfigSettings | None</code> <p>a ConfigSettings object for the major, minor, and aux models used in the workflow</p> <code>None</code> <code>logger</code> <code>LoggerBase</code> <p>a logger object for errors.</p> <code>ConsoleLogger()</code> Behaviour <p>raises errors if fails to load settings or create workflow.</p> Source code in <code>lmm_education/workflows/workflow_factory.py</code> <pre><code>def workflow_factory(\n    workflow_name: str,\n    workflow_context: ChatWorkflowContext,\n    settings: ConfigSettings | None = None,\n    logger: LoggerBase = ConsoleLogger(),\n) -&gt; ChatStateGraphType:\n    \"\"\"\n    Factory function to retrieve compiled workflow graphs.\n\n    Args:\n        workflow_name: the name of the workflow to load.\n        settings: a ConfigSettings object for the major,\n            minor, and aux models used in the workflow\n        logger: a logger object for errors.\n\n    Behaviour:\n        raises errors if fails to load settings or create workflow.\n    \"\"\"\n\n    # will raise errors if failed.\n    if settings is None:\n        settings = load_settings(logger=logger)\n    if settings is None:\n        raise ValueError(\"Could not create workflow.\")\n\n    # At present, we only have one graph, so we put this function\n    # here, but it will be moved to a factory module when we have\n    # more workflows.\n    match workflow_name:\n        case \"workflow\":  # only query at first chat\n            return create_chat_workflow(settings)\n        case \"agent\":\n            return create_chat_agent(settings, workflow_context)\n        case _:\n            raise ValueError(f\"Invalid workflow: {workflow_name}\")\n</code></pre>"},{"location":"API/workflows/","title":"Architecture","text":"<p>LangGraph-based chat workflows for RAG-enabled language model interactions.</p> <p>This package implements the core workflow engine for the application's chatbot. It is defined by three things: graph definition, graph  streaming, and dependency injection.</p> <p>options:     show_root_heading: false</p>"},{"location":"API/workflows/#lmm_education.workflows.langchain--1-graphs-and-state","title":"1. Graphs and State","text":"<p>A graph in LangGraph is a directed graph of nodes (Python functions that transform a shared state dictionary) connected by edges (which determine execution order). At each step, the node receives the current state and returns a partial update to it; LangGraph merges the update back into the state.</p> <p>The shared state is defined as a <code>TypedDict</code> called <code>ChatState</code> (see <code>base.py</code>). It carries the user query, conversation history (as LangChain messages), the retrieved context, the generated response, timing information, and a <code>status</code> field that drives routing decisions. A graph is initialised with <code>create_initial_state()</code>, which populates the required fields.</p> <p>Two concrete graphs are provided:</p> <ul> <li>chat_graph.py \u2014 a linear workflow: validate \u2192 integrate history   \u2192 retrieve context \u2192 format query \u2192 generate response.</li> <li>chat_agent.py \u2014 an agentic workflow where the LLM decides when   to call a <code>retrieve_context</code> tool, allowing multi-hop retrieval   and iterative reasoning.</li> </ul> <p>Both are compiled into a <code>CompiledStateGraph</code> (aliased as <code>ChatStateGraphType</code>) by their respective <code>create_chat_workflow()</code> and <code>create_chat_agent()</code> factory functions. The <code>workflow_factory</code> module (one level up, <code>workflows/workflow_factory.py</code>) selects between them at runtime based on the <code>appchat.toml</code> configuration.</p>"},{"location":"API/workflows/#lmm_education.workflows.langchain--2-shared-nodes-and-routing","title":"2. Shared Nodes and Routing","text":"<p>Nodes shared by both graphs \u2014 <code>validate_query</code> and <code>integrate_history</code> \u2014 live in <code>nodes.py</code> so that they have a single definition. Because <code>integrate_history</code> needs to capture <code>ConfigSettings</code> for the summariser model, it is produced by a factory function <code>create_integrate_history_node(settings)</code>.</p> <p>Conditional routing functions (<code>continue_if_valid</code>, <code>continue_if_no_error</code>, <code>continue_after_tool_call</code>) live in <code>graph_routing.py</code>. They return callbacks that inspect the <code>status</code> field of <code>ChatState</code> to decide whether the graph should continue to the next node or terminate.</p>"},{"location":"API/workflows/#lmm_education.workflows.langchain--3-dependency-injection","title":"3. Dependency Injection","text":"<p>Graph nodes receive external resources through LangGraph's runtime context mechanism. A <code>ChatWorkflowContext</code> (defined in <code>base.py</code>) is a Pydantic model that carries:</p> <ul> <li><code>retriever</code> \u2014 the vector-store retriever (<code>BaseRetriever</code>)</li> <li><code>system_message</code> \u2014 the system prompt</li> <li><code>chat_settings</code> \u2014 the <code>ChatSettings</code> from <code>appchat.toml</code></li> <li><code>logger</code> \u2014 a <code>LoggerBase</code> instance</li> </ul> <p>The context is constructed once \u2014 typically from the configuration files via <code>ChatWorkflowContext.from_default_config()</code> \u2014 and passed to the graph at stream time. Nodes access it through <code>runtime.context</code>, keeping them free of global state and easy to test in isolation.</p>"},{"location":"API/workflows/#lmm_education.workflows.langchain--4-streaming-architecture","title":"4. Streaming Architecture","text":"<p>Graphs are designed to be consumed as asynchronous streams, not via <code>.ainvoke()</code>. The streaming infrastructure is organised in <code>stream_adapters.py</code> as a three-tier pipeline of composable async iterators:</p> <p>Tier 1 \u2014 Multi-mode raw stream     The richest representation, produced by calling     <code>stream_graph_state()</code> or <code>stream_graph_updates()</code>. Each     item is a <code>(mode, event)</code> tuple, where mode is \"messages\",     \"values\", or \"updates\". This tier carries everything: message     chunks for display, full state snapshots for logging, and     differential updates for reacting to specific field changes.</p> <p>Tier 2 \u2014 Messages only     Produced by <code>tier_1_to_2_adapter()</code>. Each item is a     <code>(chunk, metadata)</code> tuple from the \"messages\" stream mode.     State and update events are discarded.</p> <p>Tier 3 \u2014 Plain text     Produced by <code>tier_2_to_3_adapter()</code> or <code>tier_1_to_3_adapter()</code>.     Each item is a <code>str</code> \u2014 the text fragment to display.</p> <p>Information flows downward only: tier 1 \u2192 tier 2 \u2192 tier 3. Adapters at each tier can filter, transform, or inject content. Key adapters include:</p> <ul> <li><code>terminal_tier1_adapter</code> \u2014 calls a callback with the terminal   state (used to log the exchange to a database after the stream   completes).</li> <li><code>tier_1_filter_messages_adapter</code> \u2014 suppresses messages from   specific nodes (e.g., tool calls in the agent workflow).</li> <li><code>field_change_tier_1_adapter</code> \u2014 reacts to state field changes   (e.g., printing retrieved context).</li> <li><code>terminal_field_change_adapter</code> \u2014 a tier 1 \u2192 tier 3 shortcut   that emits text and additionally inserts output when specific   state fields change.</li> </ul> <p>The domain-specific <code>stateful_validation_adapter</code> in <code>chat_stream_adapters.py</code> is a tier 1 adapter that buffers streamed chunks, sends them to a secondary LLM for content classification, and suppresses the stream if the content falls outside the allowed categories.</p> <p>A typical composition in <code>query.py</code> looks like::</p> <pre><code>raw = stream_graph_state(workflow, initial_state, context)\nvalidated = stateful_validation_adapter(raw, ...)\nlogged = terminal_tier1_adapter(validated, on_terminal_state=...)\ntext = tier_1_to_3_adapter(logged)\n\nasync for chunk in text:\n    print(chunk, end=\"\")\n</code></pre>"},{"location":"API/workflows/#lmm_education.workflows.langchain--5-database-logging","title":"5. Database Logging","text":"<p><code>graph_logging.py</code> defines <code>ChatDatabaseInterface</code> (an ABC) and two concrete CSV-based implementations (<code>CsvChatDatabase</code> for in-memory streams, <code>CsvFileChatDatabase</code> for files). The <code>graph_logger()</code> function in <code>base.py</code> extracts fields from <code>ChatState</code> and <code>ChatWorkflowContext</code> and delegates to the database interface. It is invoked as a terminal-state callback through <code>terminal_tier1_adapter</code> in the stream pipeline.</p>"},{"location":"API/workflows/#lmm_education.workflows.langchain--6-module-map","title":"6. Module Map","text":"<p>Foundation     <code>base.py</code> <code>ChatState</code>, <code>ChatWorkflowContext</code>,         <code>create_initial_state()</code>, <code>prepare_messages_for_llm()</code>,         <code>graph_logger()</code>.</p> <p>Graph Definition     <code>nodes.py</code>         Shared node functions: <code>validate_query</code>,         <code>create_integrate_history_node</code>.     <code>graph_routing.py</code>         Conditional-edge callbacks: <code>continue_if_valid</code>,         <code>continue_if_no_error</code>, <code>continue_after_tool_call</code>.     <code>chat_graph.py</code>         Linear RAG workflow \u2014 <code>create_chat_workflow()</code>.     <code>chat_agent.py</code>         Agentic RAG workflow \u2014 <code>create_chat_agent()</code>.</p> <p>Streaming     <code>stream_adapters.py</code>         Generic three-tier stream adapters, entry-point functions         (<code>stream_graph_state</code>, <code>stream_graph_updates</code>).     <code>chat_stream_adapters.py</code>         Domain-specific adapter: <code>stateful_validation_adapter</code>.</p> <p>Logging     <code>graph_logging.py</code> <code>ChatDatabaseInterface</code>, <code>CsvChatDatabase</code>,         <code>CsvFileChatDatabase</code>.</p>"},{"location":"API/Apps/","title":"Apps","text":"<p>There are two apps in LM Markdown for Education:</p> <ul> <li>appChat: a simple chatbot</li> <li>appWebcast: a combination of a video viewer and chatbot.</li> </ul>"},{"location":"API/Apps/appChat/","title":"appChat","text":"<p>The appChat module is the entry point for the RAG chat in LM markdown for education. By coding at the console,</p> <pre><code>appChat\n</code></pre> <p>one starts the chatbot server at the port specified in the appchat.toml configuation file.</p> <p>The appChat.py module itself consists of three parts. After loading and initializing the required libraries in the first part, the module codes the callback functions that determine the behaviour of the web page when the user interacts with it (see functions below). In the last part, the Gradio interface is coded.</p> <p>options:     show_root_heading: false</p>"},{"location":"API/Apps/appChat/#lmm_education.appChat--entry-point-for-the-rag-model-chat-application","title":"Entry point for the RAG model chat application.","text":"<p>Main functions:</p> <ul> <li>gradio_callback_fn: the main Gradio callback driving the chat</li> <li>postcomment: triggered by comments posted by the user</li> <li>vote: triggered by a vote of the user</li> </ul>"},{"location":"API/Apps/appChat/#lmm_education.appChat.gradio_callback_fn","title":"<code>gradio_callback_fn(querytext, history, request, async_log=None)</code>  <code>async</code>","text":"<p>This function is called by the gradio framework each time the user posts a new message in the chatbot. The user message is the <code>querytext</code> argument, and <code>history</code> the list of previous exchanges. The <code>request</code> argument is a wrapper around FastAPI information, such as the IP address of the user.</p> <p>This function uses the query.py module to implement the chat.</p> Source code in <code>lmm_education/appChat.py</code> <pre><code>async def gradio_callback_fn(\n    querytext: str,\n    history: list[dict[str, str]],\n    request: gr.Request,\n    async_log: AsyncLogfunType | None = None,\n) -&gt; AsyncGenerator[str, None]:\n    \"\"\"\n    This function is called by the gradio framework each time the\n    user posts a new message in the chatbot. The user message is the\n    `querytext` argument, and `history` the list of previous\n    exchanges. The `request` argument is a wrapper around FastAPI\n    information, such as the IP address of the user.\n\n    This function uses the query.py module to implement the chat.\n    \"\"\"\n\n    # this only to override closure in testing\n    if async_log is None:\n        async_log = async_log_partial\n\n    # Safely extract client host and session hash\n    client_host: str = getattr(\n        getattr(request, \"client\", None), \"host\", \"[in-process]\"\n    )\n    session_hash: str = getattr(request, \"session_hash\", \"[none]\")\n\n    # Create stream\n    buffer: str = \"\"\n    try:\n        stream_raw: tier_1_iterator = create_chat_stream(\n            querytext=querytext,\n            history=history or None,\n            context=context,\n            validate=context.chat_settings.check_response,\n            database_log=False,  # do downstream in on_terminal_state\n            logger=logger,\n        )\n\n        stream: tier_3_iterator = terminal_field_change_adapter(\n            stream_raw,\n            on_terminal_state=partial(\n                async_log,\n                client_host=client_host,\n                session_hash=session_hash,\n                timestamp=None,  # will be set at time of msg\n                record_id=None,  # handled by logger\n            ),\n        )\n    except Exception as e:\n        logger.error(f\"Could not create stream: {e}\")\n        yield context.chat_settings.MSG_ERROR_QUERY\n        return\n\n    try:\n        # Stream and yield for Gradio\n        async for item in stream:\n            buffer += _preproc_for_markdown(item)\n            yield buffer\n\n    except Exception as e:\n        logger.error(\n            f\"{client_host}: {e}\\nOFFENDING QUERY:\\n{querytext}\\n\\n\"\n        )\n        buffer = str(e)\n        yield context.chat_settings.MSG_ERROR_QUERY\n        return\n\n    return\n</code></pre>"},{"location":"API/Apps/appChat/#lmm_education.appChat.postcomment","title":"<code>postcomment(comment, request, logging_db=None)</code>  <code>async</code>","text":"<p>Async function to log user comments.</p> Source code in <code>lmm_education/appChat.py</code> <pre><code>async def postcomment(\n    comment: object,\n    request: gr.Request,\n    logging_db: ChatDatabaseInterface | None = None,\n):\n    \"\"\"\n    Async function to log user comments.\n    \"\"\"\n    # this to override closure for testing purposes\n    if logging_db is None:\n        logging_db = logging_database\n\n    record_id = generate_random_string()\n\n    # Safely extract client host and session hash\n    client_host: str = getattr(\n        getattr(request, \"client\", None), \"host\", \"unknown\"\n    )\n    session_hash: str = getattr(request, \"session_hash\", \"unknown\")\n\n    logging_db.schedule_message(\n        record_id=record_id,\n        client_host=client_host,\n        session_hash=session_hash,\n        timestamp=datetime.now(),\n        message_count=0,\n        model_name=\"\",\n        interaction_type=\"USER COMMENT\",\n        query=str(comment),\n        response=\"\",\n    )\n</code></pre>"},{"location":"API/Apps/appChat/#lmm_education.appChat.vote","title":"<code>vote(data, request, logging_db=None)</code>  <code>async</code>","text":"<p>Async function to log user reactions (like/dislike) to messages.</p> Source code in <code>lmm_education/appChat.py</code> <pre><code>async def vote(\n    data: gr.LikeData,\n    request: gr.Request,\n    logging_db: ChatDatabaseInterface | None = None,\n):\n    \"\"\"\n    Async function to log user reactions (like/dislike) to messages.\n    \"\"\"\n    # this to override closure for testing purposes\n    if logging_db is None:\n        logging_db = logging_database\n\n    record_id = generate_random_string()\n    reaction = \"approved\" if data.liked else \"disapproved\"\n\n    # Safely extract client host and session hash\n    client_host: str = getattr(\n        getattr(request, \"client\", None), \"host\", \"unknown\"\n    )\n    session_hash: str = getattr(request, \"session_hash\", \"unknown\")\n\n    logging_db.schedule_message(\n        record_id=record_id,\n        client_host=client_host,\n        session_hash=session_hash,\n        timestamp=datetime.now(),\n        message_count=0,\n        model_name=\"\",\n        interaction_type=\"USER REACTION\",\n        query=reaction,\n        response=\"\",\n    )\n</code></pre>"},{"location":"API/Apps/appWebcast/","title":"appWebcast","text":"<p>The appWebcast module is the entry point for the video-displaying RAG chatbot in LM Markdown for Education. By coding at the console,</p> <pre><code>appWebcast\n</code></pre> <p>one starts the chatbot server at the port specified in the appchat.toml configuation file.</p> <p>The appWebcast.py module itself consists of three parts. After loading and initializing the required libraries in the first part, the module codes the callback functions that determine the behaviour of the web page when the user interacts with it (see functions below). In the last part, the Gradio interface is coded.</p> <p>options:     show_root_heading: false</p>"},{"location":"API/Apps/appWebcast/#lmm_education.appWebcast--entry-point-for-the-video-chatbot-application","title":"Entry point for the video chatbot application.","text":"<p>The web page isplays a video and allows to chat with a language model about the content of the video.</p> <p>Main functions:</p> <ul> <li>gradio_callback_fn: the main Gradio callback driving the chat</li> <li>postcomment: triggered by comments posted by the user</li> <li>vote: triggered by a vote of the user</li> </ul>"},{"location":"API/Apps/appWebcast/#lmm_education.appWebcast.NavigationResult","title":"<code>NavigationResult</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Result of video navigation operations.</p> Source code in <code>lmm_education/appWebcast.py</code> <pre><code>class NavigationResult(NamedTuple):\n    \"\"\"Result of video navigation operations.\"\"\"\n\n    video_source: str | dict[str, str]  # Video file path or gr.skip()\n    progress_text: str  # Progress indicator text\n    video_index: int  # Current video position (0-based)\n    chat_history: list[gr.ChatMessage]  # Updated chat history\n</code></pre>"},{"location":"API/Apps/appWebcast/#lmm_education.appWebcast.gradio_callback_fn","title":"<code>gradio_callback_fn(querytext, history, request, async_log=None)</code>  <code>async</code>","text":"<p>This function is called by the gradio framework each time the user posts a new message in the chatbot. The user message is the <code>querytext</code> argument, and <code>history</code> the list of previous exchanges. The <code>request</code> argument is a wrapper around FastAPI information, such as the IP address of the user.</p> <p>This function uses the query.py module to implement the chat.</p> Source code in <code>lmm_education/appWebcast.py</code> <pre><code>async def gradio_callback_fn(\n    querytext: str,\n    history: list[dict[str, str]],\n    request: gr.Request,\n    async_log: AsyncLogfunType | None = None,\n) -&gt; AsyncGenerator[str, None]:\n    \"\"\"\n    This function is called by the gradio framework each time the\n    user posts a new message in the chatbot. The user message is the\n    `querytext` argument, and `history` the list of previous\n    exchanges. The `request` argument is a wrapper around FastAPI\n    information, such as the IP address of the user.\n\n    This function uses the query.py module to implement the chat.\n    \"\"\"\n\n    # this only to override closure in testing\n    if async_log is None:\n        async_log = async_log_partial\n\n    # Safely extract client host and session hash\n    client_host: str = getattr(\n        getattr(request, \"client\", None), \"host\", \"[in-process]\"\n    )\n    session_hash: str = getattr(request, \"session_hash\", \"[none]\")\n\n    # Create stream\n    buffer: str = \"\"\n    try:\n        stream_raw: tier_1_iterator = create_chat_stream(\n            querytext=querytext,\n            history=history or None,\n            context=context,\n            validate=context.chat_settings.check_response,\n            database_log=False,  # do downstream in on_terminal_state\n            logger=logger,\n        )\n\n        stream: tier_3_iterator = terminal_field_change_adapter(\n            stream_raw,\n            on_terminal_state=partial(\n                async_log,\n                client_host=client_host,\n                session_hash=session_hash,\n                timestamp=None,  # will be set at time of msg\n                record_id=None,  # handled by logger\n            ),\n        )\n    except Exception as e:\n        logger.error(f\"Could not create stream: {e}\")\n        yield context.chat_settings.MSG_ERROR_QUERY\n        return\n\n    try:\n        # Stream and yield for Gradio\n        async for item in stream:\n            buffer += _preproc_for_markdown(item)\n            yield buffer\n\n    except Exception as e:\n        logger.error(\n            f\"{client_host}: {e}\\nOFFENDING QUERY:\\n{querytext}\\n\\n\"\n        )\n        buffer = str(e)\n        yield context.chat_settings.MSG_ERROR_QUERY\n        return\n\n    return\n</code></pre>"},{"location":"API/Apps/appWebcast/#lmm_education.appWebcast.postcomment","title":"<code>postcomment(comment, request, logging_db=None)</code>  <code>async</code>","text":"<p>Async function to log user comments.</p> Source code in <code>lmm_education/appWebcast.py</code> <pre><code>async def postcomment(\n    comment: object,\n    request: gr.Request,\n    logging_db: ChatDatabaseInterface | None = None,\n):\n    \"\"\"\n    Async function to log user comments.\n    \"\"\"\n    # this to override closure for testing purposes\n    if logging_db is None:\n        logging_db = logging_database\n\n    record_id = generate_random_string()\n\n    # Safely extract client host and session hash\n    client_host: str = getattr(\n        getattr(request, \"client\", None), \"host\", \"unknown\"\n    )\n    session_hash: str = getattr(request, \"session_hash\", \"unknown\")\n\n    logging_db.schedule_message(\n        record_id=record_id,\n        client_host=client_host,\n        session_hash=session_hash,\n        timestamp=datetime.now(),\n        message_count=0,\n        model_name=\"\",\n        interaction_type=\"USER COMMENT\",\n        query=str(comment),\n        response=\"\",\n    )\n</code></pre>"},{"location":"API/Apps/appWebcast/#lmm_education.appWebcast.vote","title":"<code>vote(data, request, logging_db=None)</code>  <code>async</code>","text":"<p>Async function to log user reactions (like/dislike) to messages.</p> Source code in <code>lmm_education/appWebcast.py</code> <pre><code>async def vote(\n    data: gr.LikeData,\n    request: gr.Request,\n    logging_db: ChatDatabaseInterface | None = None,\n):\n    \"\"\"\n    Async function to log user reactions (like/dislike) to messages.\n    \"\"\"\n    # this to override closure for testing purposes\n    if logging_db is None:\n        logging_db = logging_database\n\n    record_id = generate_random_string()\n    reaction = \"approved\" if data.liked else \"disapproved\"\n\n    # Safely extract client host and session hash\n    client_host: str = getattr(\n        getattr(request, \"client\", None), \"host\", \"unknown\"\n    )\n    session_hash: str = getattr(request, \"session_hash\", \"unknown\")\n\n    logging_db.schedule_message(\n        record_id=record_id,\n        client_host=client_host,\n        session_hash=session_hash,\n        timestamp=datetime.now(),\n        message_count=0,\n        model_name=\"\",\n        interaction_type=\"USER REACTION\",\n        query=reaction,\n        response=\"\",\n    )\n</code></pre>"},{"location":"API/Configuration/","title":"Configuration modules","text":"<p>The config modules in LM Markdown for Education configure the application. There are two configuration files: config.toml and appchat.toml. The first contains settings that are common to the app and the rest of LM Markdown for Education (for example, when interacting with it through the CLI). The second contains settings that are specific to apps (the web applications). Both files are written in TOML format.</p>"},{"location":"API/Configuration/appchat/","title":"Module appchat.py","text":"<p>Configuration file for the appChat application.</p> <p>Settings specified in appchat.toml are:</p> <pre><code>- title, description, comment: text displayed on the web\n    application\n- MSG_EMPTY_QUERY, MSG_WRONG_CONTENT, MSG_LONG_QUERY,\n    MSG_ERROR_QUERY: responses displayed in several user\n    error conditions. If the English language is acceptable,\n    can be left as they are.\n- SYSTEM_MESSAGE: this is a system message prepended to all\n    chat turns. Customize this message to specify the personality\n    of the chatbot and -- importantly -- to specify constraints\n    on the content the chatbot may deliver.\n- PROMPT_TEMPLATE: used in a simple chat workflow to prompt the\n    model with the query and the context from the vector database.\n- max_query_word_count: the maximal number of words contained in a\n    query. Larger query texts will be rejected with MSG_LONG_QUERY\n- history_integration: the approach used to integrate past chat\n    history when retrieving context. Possible values: 'none',\n    'summary', 'content_extraction', and 'rewrite'. 'none'\n    attaches a fixed number of past messages in the exchange. It\n    is fast and the most economical choice, but tends to reproduce\n    the original context in every new chat turn. The most\n    performant options are 'content_extraction' and 'rewrite'.\n- history_length: the number of past messages sent to the language\n    model when chatting.\n- workflow: the agent used for chatting. Possible options are\n    'workflow' (straightforward linear progression of message\n    processing) and 'agent' (more sophisticated in handling\n    complex or invalid requests, but slower and more expensive).\n- check_response: validates the response of the model prior to\n    release streaming. This validation is independent of any\n    instruction given to the model in the system and user prompts.\n    Includes the settings\n    - check_response: boolean true or false\n    - allowed_content: list of topics allowed by the chatbot.\n        This list will depend on the topic treated in the material\n    - initial_buffer_size: the buffer captured from the model\n        stream sent to the language model for categorization.\n        Larger buffers improve the performance of the categori-\n        zation, but introduce longer delays before the model is\n        seen streaming in the chat.\n    - continue_on_fail: whether the stream should be released if\n        the model fails to respond. If false, the chat will\n        respond with MSG_ERROR_QUERY (because the streaming\n        model and the model used for categorization differ, it is\n        conceivable that categorization may fail even if the main\n        model is streaming). Defaults to true.\n    - server: the address/port used by the server, and\n        authentication credentials\n    - chat_database: the database used to record the messages\n        exchanged with users. At present, only supports .csv\n        files. Consists of the following fields:\n        - messages_database_file: the file saving queries and\n            responses.\n        - context_database_file: saves the retrieved context.\n</code></pre>"},{"location":"API/Configuration/appchat/#lmm_education.config.appchat.ChatDatabase","title":"<code>ChatDatabase</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Names of databases to log queries and responses.</p> Source code in <code>lmm_education/config/appchat.py</code> <pre><code>class ChatDatabase(BaseModel):\n    \"\"\"Names of databases to log queries and responses.\"\"\"\n\n    messages_database_file: str = Field(\n        default=\"messages.csv\",\n        min_length=1,\n        description=\"Database of message exchanges\",\n    )\n    context_database_file: str = Field(\n        default=\"queries.csv\",\n        min_length=1,\n        description=\"Database for retrieved context\",\n    )\n</code></pre>"},{"location":"API/Configuration/appchat/#lmm_education.config.appchat.ChatSettings","title":"<code>ChatSettings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> Source code in <code>lmm_education/config/appchat.py</code> <pre><code>class ChatSettings(BaseSettings):\n    model_config = SettingsConfigDict(\n        toml_file=CHAT_CONFIG_FILE,\n        env_prefix=\"LMMEDU_\",  # Uppercase for environment variables\n        frozen=True,\n        validate_assignment=True,\n        extra=\"forbid\",  # Prevent unexpected fields\n    )\n\n    # This is displayed on the chatbot. Change it as appropriate\n    title: str = Field(default=\"VU Study Assistant\")\n    description: str = Field(\n        default=\"\"\"\nStudy assistant chatbot for VU Scientific Methods in Psychology:\nData analysis with linear models in R. \nAsk a question about the course, and the assistant will provide a \nresponse based on it. \nExample: \"How can I fit a model with kid_score as outcome and mom_iq as predictor?\" \n\"\"\"\n    )\n    comment: str = Field(\n        default=\"Please leave a comment on the response of the chatbot here\"\n    )\n\n    # messages\n    MSG_EMPTY_QUERY: str = Field(\n        default=(\n            \"It seems you didn't type anything in the input box... \"\n            \"If you have questions related to \"\n            \"linear models, their interpretation, or how to \"\n            \"implement them in R, I am happy to help.\"\n        )\n    )\n    MSG_WRONG_CONTENT: str = Field(\n        default=(\n            \"I do not have information to answer this query\"\n            \" as the course focuses on linear models and \"\n            \"their use in R. If you have questions related to \"\n            \"linear models, their interpretation, or how to \"\n            \"implement them in R, I would be happy to help!\"\n        )\n    )\n    MSG_LONG_QUERY: str = Field(\n        default=\"Hmm, your question is too long... Can you think\"\n        \" of a way to make it shorter?\"\n    )\n    MSG_ERROR_QUERY: str = Field(\n        default=(\n            \"I am sorry, due to an error I cannot answer \"\n            \"this question. The failure is being recorded by the system.\"\n        )\n    )\n\n    SYSTEM_MESSAGE: str = Field(\n        default=(\n            \"You are a university tutor teaching \"\n            \"undergraduates in a statistics course that uses R\"\n            \" to fit models, explaining background and guiding \"\n            \"understanding. Limit your responses in the chat to \"\n            \"the field of statistics and the syntax and use of \"\n            \"the R programming language.\"\n        )\n    )\n\n    PROMPT_TEMPLATE: str = Field(\n        default=\"\"\"\nPlease assist students by responding to their QUERY by using the provided CONTEXT.\nIf the CONTEXT does not provide information for your answer, integrate the CONTEXT\nonly for the use and syntax of R. Otherwise, reply that you do not have information \nto answer the query, as the course focuses on linear models and their use in R.\n\n####\nCONTEXT: \"{context}\"\n\n####\nQUERY: \"{query}\"\n\n\n\"\"\"\n    )\n\n    max_query_word_count: int = Field(\n        default=120, ge=0, description=\"Max word count in query\"\n    )\n\n    # Technique to include history\n    history_integration: HistoryIntegration = Field(\n        default='context_extraction',\n        description=\"Technique for the integration of history\",\n    )\n    history_length: int = Field(\n        default=2,\n        ge=0,\n        description=\"Number of exchanges to include in history\",\n    )\n\n    # workflow\n    workflow: Literal[\"workflow\", \"agent\"] = Field(\n        default=\"workflow\",\n        description=\"Workflow to use in interacting with language model\",\n    )\n    max_tool_retries: int = Field(\n        default=3,\n        ge=1,\n        description=\"Maximum number of retries for tool calls\",\n    )\n\n    # thematic control of interaction\n    check_response: CheckResponse = Field(\n        default_factory=CheckResponse,\n        description=\"Check thematic appropriateness of chat\",\n    )\n\n    # server config\n    server: ServerSettings = Field(\n        default_factory=ServerSettings,\n        description=\"Server configuration\",\n    )\n\n    # database of exchanges\n    chat_database: ChatDatabase = Field(\n        default_factory=ChatDatabase,\n        description=\"Database of chat exchanges\",\n    )\n\n    @classmethod\n    def settings_customise_sources(\n        cls,\n        settings_cls: type[BaseSettings],\n        init_settings: PydanticBaseSettingsSource,\n        env_settings: PydanticBaseSettingsSource,\n        dotenv_settings: PydanticBaseSettingsSource,\n        file_secret_settings: PydanticBaseSettingsSource,\n    ) -&gt; tuple[PydanticBaseSettingsSource, ...]:\n        \"\"\"Customize the order of settings sources to include TOML file.\"\"\"\n        return (\n            init_settings,\n            TomlConfigSettingsSource(settings_cls),\n            env_settings,\n        )\n\n    def from_instance(\n        self,\n        *,\n        title: str | None = None,\n        description: str | None = None,\n        comment: str | None = None,\n        MSG_EMPTY_QUERY: str | None = None,\n        MSG_WRONG_CONTENT: str | None = None,\n        MSG_LONG_QUERY: str | None = None,\n        MSG_ERROR_QUERY: str | None = None,\n        SYSTEM_MESSAGE: str | None = None,\n        PROMPT_TEMPLATE: str | None = None,\n        max_query_word_count: int | None = None,\n        history_integration: HistoryIntegration | None = None,\n        history_length: int | None = None,\n        workflow: str | None = None,\n        max_tool_retries: int | None = None,\n        check_response: CheckResponse | None = None,\n        server: ServerSettings | None = None,\n        chat_database: ChatDatabase | None = None,\n    ) -&gt; 'ChatSettings':\n        return ChatSettings(\n            title=title or self.title,\n            description=description or self.description,\n            comment=comment or self.comment,\n            MSG_EMPTY_QUERY=MSG_EMPTY_QUERY or self.MSG_EMPTY_QUERY,\n            MSG_WRONG_CONTENT=MSG_WRONG_CONTENT\n            or self.MSG_WRONG_CONTENT,\n            MSG_LONG_QUERY=MSG_LONG_QUERY or self.MSG_LONG_QUERY,\n            MSG_ERROR_QUERY=MSG_ERROR_QUERY or self.MSG_ERROR_QUERY,\n            SYSTEM_MESSAGE=SYSTEM_MESSAGE or self.SYSTEM_MESSAGE,\n            PROMPT_TEMPLATE=PROMPT_TEMPLATE or self.PROMPT_TEMPLATE,\n            max_query_word_count=(\n                max_query_word_count or self.max_query_word_count\n            ),\n            history_integration=history_integration\n            or self.history_integration,\n            history_length=history_length or self.history_length,\n            workflow=workflow or self.workflow,\n            max_tool_retries=max_tool_retries\n            or self.max_tool_retries,\n            check_response=check_response or self.check_response,\n            server=server or self.server,\n            chat_database=chat_database or self.chat_database,\n        )\n\n    def __init__(self, **data: Any) -&gt; None:\n        \"\"\"\n        Initialize ChatSettings with file existence verification.\n        Prints a warning message on the console if the file does not exist.\n        \"\"\"\n        config_path = Path(CHAT_CONFIG_FILE)\n        if not config_path.exists():\n            print(\n                f\"Configuration file not found: {config_path.absolute()}\\n\"\n                \"Creating a default configuration object.\"\n            )\n        super().__init__(**data)\n</code></pre>"},{"location":"API/Configuration/appchat/#lmm_education.config.appchat.ChatSettings.__init__","title":"<code>__init__(**data)</code>","text":"<p>Initialize ChatSettings with file existence verification. Prints a warning message on the console if the file does not exist.</p> Source code in <code>lmm_education/config/appchat.py</code> <pre><code>def __init__(self, **data: Any) -&gt; None:\n    \"\"\"\n    Initialize ChatSettings with file existence verification.\n    Prints a warning message on the console if the file does not exist.\n    \"\"\"\n    config_path = Path(CHAT_CONFIG_FILE)\n    if not config_path.exists():\n        print(\n            f\"Configuration file not found: {config_path.absolute()}\\n\"\n            \"Creating a default configuration object.\"\n        )\n    super().__init__(**data)\n</code></pre>"},{"location":"API/Configuration/appchat/#lmm_education.config.appchat.ChatSettings.settings_customise_sources","title":"<code>settings_customise_sources(settings_cls, init_settings, env_settings, dotenv_settings, file_secret_settings)</code>  <code>classmethod</code>","text":"<p>Customize the order of settings sources to include TOML file.</p> Source code in <code>lmm_education/config/appchat.py</code> <pre><code>@classmethod\ndef settings_customise_sources(\n    cls,\n    settings_cls: type[BaseSettings],\n    init_settings: PydanticBaseSettingsSource,\n    env_settings: PydanticBaseSettingsSource,\n    dotenv_settings: PydanticBaseSettingsSource,\n    file_secret_settings: PydanticBaseSettingsSource,\n) -&gt; tuple[PydanticBaseSettingsSource, ...]:\n    \"\"\"Customize the order of settings sources to include TOML file.\"\"\"\n    return (\n        init_settings,\n        TomlConfigSettingsSource(settings_cls),\n        env_settings,\n    )\n</code></pre>"},{"location":"API/Configuration/appchat/#lmm_education.config.appchat.CheckResponse","title":"<code>CheckResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Settings to check appropriateness of chat.</p> Source code in <code>lmm_education/config/appchat.py</code> <pre><code>class CheckResponse(BaseModel):\n    \"\"\"\n    Settings to check appropriateness of chat.\n    \"\"\"\n\n    check_response: bool = Field(default=False)\n    allowed_content: list[str] = Field(default=[])\n    initial_buffer_size: int = Field(\n        default=320,\n        ge=120,\n        le=12000,\n        description=\"buffer size to send to model to check content\",\n    )\n    continue_on_fail: bool = Field(\n        default=True,\n        description=\"validate response if server model not available\",\n    )\n\n    @model_validator(mode=\"after\")\n    def validate_allowed_content(self) -&gt; Self:\n        \"\"\"Validate that allowed_content is not empty when check_response is True.\"\"\"\n        if self.check_response and not self.allowed_content:\n            raise ValueError(\n                \"allowed_content must not be empty when check_response is True\"\n            )\n        if \"general knowledge\" in self.allowed_content:\n            raise ValueError(\n                \"'general knowledge' cannot be included in allowed content\"\n            )\n        return self\n</code></pre>"},{"location":"API/Configuration/appchat/#lmm_education.config.appchat.CheckResponse.validate_allowed_content","title":"<code>validate_allowed_content()</code>","text":"<p>Validate that allowed_content is not empty when check_response is True.</p> Source code in <code>lmm_education/config/appchat.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_allowed_content(self) -&gt; Self:\n    \"\"\"Validate that allowed_content is not empty when check_response is True.\"\"\"\n    if self.check_response and not self.allowed_content:\n        raise ValueError(\n            \"allowed_content must not be empty when check_response is True\"\n        )\n    if \"general knowledge\" in self.allowed_content:\n        raise ValueError(\n            \"'general knowledge' cannot be included in allowed content\"\n        )\n    return self\n</code></pre>"},{"location":"API/Configuration/appchat/#lmm_education.config.appchat.ServerSettings","title":"<code>ServerSettings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Server configuration settings.</p> <p>Attributes:</p> Name Type Description <code>mode</code> <code>Literal['local', 'remote']</code> <p>one of 'local' or 'remote'</p> <code>port</code> <code>int</code> <p>port number (only if mode is 'remote')</p> <code>host</code> <code>str</code> <p>server host address (defaults to 'localhost')</p> Source code in <code>lmm_education/config/appchat.py</code> <pre><code>class ServerSettings(BaseSettings):\n    \"\"\"\n    Server configuration settings.\n\n    Attributes:\n        mode: one of 'local' or 'remote'\n        port: port number (only if mode is 'remote')\n        host: server host address (defaults to 'localhost')\n    \"\"\"\n\n    mode: Literal[\"local\", \"remote\"] = Field(\n        default=\"local\", description=\"Server deployment mode\"\n    )\n    port: int = Field(\n        default=61543,\n        ge=0,\n        le=65535,\n        description=\"Server port (0 for auto-assignment)\",\n    )\n    host: str = Field(\n        default=\"localhost\", description=\"Server host address\"\n    )\n    auth_user: str = Field(\n        default=\"\", description=\"Username for authentication\"\n    )\n    auth_pass: str = Field(\n        default=\"\", description=\"Password for authentication\"\n    )\n\n    model_config = SettingsConfigDict(frozen=True, extra='forbid')\n\n    @field_validator('port')\n    @classmethod\n    def validate_port(cls, v: int) -&gt; int:\n        \"\"\"Validate port number is in acceptable range.\"\"\"\n        if v != 0 and not (\n            DEFAULT_PORT_RANGE[0] &lt;= v &lt;= DEFAULT_PORT_RANGE[1]\n        ):\n            raise ValueError(\n                f\"Port must be 0 (auto-assign) or between \"\n                f\"{DEFAULT_PORT_RANGE[0]} and {DEFAULT_PORT_RANGE[1]}\"\n            )\n        return v\n</code></pre>"},{"location":"API/Configuration/appchat/#lmm_education.config.appchat.ServerSettings.validate_port","title":"<code>validate_port(v)</code>  <code>classmethod</code>","text":"<p>Validate port number is in acceptable range.</p> Source code in <code>lmm_education/config/appchat.py</code> <pre><code>@field_validator('port')\n@classmethod\ndef validate_port(cls, v: int) -&gt; int:\n    \"\"\"Validate port number is in acceptable range.\"\"\"\n    if v != 0 and not (\n        DEFAULT_PORT_RANGE[0] &lt;= v &lt;= DEFAULT_PORT_RANGE[1]\n    ):\n        raise ValueError(\n            f\"Port must be 0 (auto-assign) or between \"\n            f\"{DEFAULT_PORT_RANGE[0]} and {DEFAULT_PORT_RANGE[1]}\"\n        )\n    return v\n</code></pre>"},{"location":"API/Configuration/appchat/#lmm_education.config.appchat.create_default_config_file","title":"<code>create_default_config_file(file_path=None)</code>","text":"<p>Create a default settings file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path | None</code> <p>config file (defaults to config.toml)</p> <code>None</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If tomlkit is not available</p> <code>OSError</code> <p>If file cannot be written</p> <code>ValueError</code> <p>If settings cannot be serialized</p> <code>ValidationError</code> <p>if config.toml is invalid</p> <code>TOMLDecodeError</code> <p>if config.toml is invalid</p> Example <pre><code># Creates appachat.toml in base folder with default values\nfrom lmm_education.config.appchat import create_default_config_file\ncreate_default_config_file()\n\n# Creates custom config file\ncreate_default_config_file(file_path=\"custom_config.toml\")\n</code></pre> Source code in <code>lmm_education/config/appchat.py</code> <pre><code>def create_default_config_file(\n    file_path: str | Path | None = None,\n) -&gt; None:\n    \"\"\"Create a default settings file.\n\n    Args:\n        file_path: config file (defaults to config.toml)\n\n    Raises:\n        ImportError: If tomlkit is not available\n        OSError: If file cannot be written\n        ValueError: If settings cannot be serialized\n        ValidationError: if config.toml is invalid\n        tomllib.TOMLDecodeError: if config.toml is invalid\n\n    Example:\n        ```python\n        # Creates appachat.toml in base folder with default values\n        from lmm_education.config.appchat import create_default_config_file\n        create_default_config_file()\n\n        # Creates custom config file\n        create_default_config_file(file_path=\"custom_config.toml\")\n        ```\n    \"\"\"\n    from lmm.config.config import create_default_config_file as _cdcf\n\n    if file_path is None:\n        file_path = CHAT_CONFIG_FILE\n\n    file_path = Path(file_path)\n\n    _cdcf(file_path, ChatSettings)\n</code></pre>"},{"location":"API/Configuration/appchat/#lmm_education.config.appchat.load_settings","title":"<code>load_settings(*, file_name=None, logger=ExceptionConsoleLogger())</code>","text":"<p>Load and return a ChatSettings object from the specified file.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str | Path | None</code> <p>Path to settings file (defaults to config.toml)</p> <code>None</code> <code>logger</code> <code>LoggerBase</code> <p>logger to use. Defaults to a exception-raising logger. This centralizes exception handling, instead of writing the except clauses for each instantiation of ConfigSettings().</p> <code>ExceptionConsoleLogger()</code> <p>Returns:</p> Name Type Description <code>ChatSettings</code> <code>ChatSettings | None</code> <p>The loaded configuration settings object, or None if exception raised.</p> Expected behaviour <p>Exceptions handled through logger, but raises exceptions in     the default logger.</p> Example <pre><code># Load settings from a custom config file\nsettings = load_settings(\"my_config.toml\")\n\n# Load settings using Path object\nfrom pathlib import Path\nsettings = load_settings(Path(\"configs/custom.toml\"))\n</code></pre> Note <p>Use of an ExceptionConsoleLogger still requires to check that return value is not None to satisfy a type checker.</p> <pre><code>logger = ExceptionConsoleLogger()\nsettings = load_settings(logger=logger)\nif settings is None:\n    raise ValueError(\"Unreacheable code reached\")\n</code></pre> <p>Here, the type checker is told that settings is not None, but the condition is always satisfied because load_settings will raise an exception whenever it would be returning None.</p> <p>Contrast with the following:</p> <pre><code>logger = ConsoleLogger()\nsettings = load_settings(logger=logger)\nif settings is None:\n    raise ValueError(\"Could not read config.toml\")\n</code></pre> Source code in <code>lmm_education/config/appchat.py</code> <pre><code>def load_settings(\n    *,\n    file_name: str | Path | None = None,\n    logger: LoggerBase = ExceptionConsoleLogger(),\n) -&gt; ChatSettings | None:\n    \"\"\"Load and return a ChatSettings object from the specified file.\n\n    Args:\n        file_name: Path to settings file (defaults to config.toml)\n        logger: logger to use. Defaults to a exception-raising logger.\n            This centralizes exception handling, instead of writing\n            the except clauses for each instantiation of ConfigSettings().\n\n    Returns:\n        ChatSettings: The loaded configuration settings object, or\n            None if exception raised.\n\n    Expected behaviour:\n        Exceptions handled through logger, but raises exceptions in\n            the default logger.\n\n    Example:\n        ```python\n        # Load settings from a custom config file\n        settings = load_settings(\"my_config.toml\")\n\n        # Load settings using Path object\n        from pathlib import Path\n        settings = load_settings(Path(\"configs/custom.toml\"))\n        ```\n\n    Note:\n        Use of an ExceptionConsoleLogger still requires to check that\n        return value is not None to satisfy a type checker.\n\n        ```python\n        logger = ExceptionConsoleLogger()\n        settings = load_settings(logger=logger)\n        if settings is None:\n            raise ValueError(\"Unreacheable code reached\")\n        ```\n\n        Here, the type checker is told that settings is not None, but\n        the condition is always satisfied because load_settings will\n        raise an exception whenever it would be returning None.\n\n        Contrast with the following:\n\n        ```python\n        logger = ConsoleLogger()\n        settings = load_settings(logger=logger)\n        if settings is None:\n            raise ValueError(\"Could not read config.toml\")\n        ```\n    \"\"\"\n    from lmm.config.config import load_settings as _load_settings\n\n    if file_name is None:\n        file_name = CHAT_CONFIG_FILE\n\n    file_path = Path(file_name)\n\n    return _load_settings(\n        file_name=file_path,\n        logger=logger,\n        settings_class=ChatSettings,\n    )\n</code></pre>"},{"location":"API/Configuration/config/","title":"Module config.py","text":"<p>This module provides the configuration of the project, i.e. the configurable ways in which we will extract properties from our markdown documents, so that one can experiment with different options.</p> <p>The configuration options are the following:</p> <pre><code>storage: one of\n    - ':memory:'\n    - LocalStorage(folder = \"./storage\")  (or another folder name)\n    - RemoteSource(url = \"1.1.1.127\", port = 21465)  (or others)\ndatabase: settings for the database in storage. Includes\n    - collection_name: the name of the main collection/table\n    - companion_collection: collection for documents\nRAG: settings for retrieval assisted generation. Includes\n    - titles: a boolean for generating titles or not\n    - questions: boolean for generating questions\n    - summaries: boolean for generating summaries\n    - encoding_model: the encoding model, one of NONE, CONTENT,\n        MERGED, MULTIVECTOR, SPARSE, SPARSE_CONTENT, SPARSE_MERGED\n        and SPARSE_MULTIVECTOR.\n    - annotation_model: the annotation model, including\n        inherited_properties and own_properties\n    - retrieve_companion_docs: retrieve documents. A\n        `companion_collection` must have been specified in the\n        `database` section. If not, the setting is ignored.\n    - max_companion_docs: number of full documents to retrieve.\ntext_splitter: the splitter class that will be used to split the\n    text into chunks\n</code></pre> <p>Encoding models (.chunks.EncodingModel):</p> <pre><code>NONE: no encoding (used to retrieve data via UUID)\nCONTENT: Encode only textual content in dense vector\nMERGED: Encode textual content merged with metadata annotations in\n    dense vectors\nMULTIVECTOR: Encode content and annotions using multivectors\nSPARSE: Sparse encoding of annotations only\nSPARSE_CONTENT: Sparse annotations, dense encoding of content\nSPARSE_MERGED: Sparse annotations, dense encoding of merged\n    content and annotations\nSPARSE_MULTIVECTOR: Sparse annotations, multivector encoding of\n    merged content and annotations\n</code></pre> <p>Here, 'annotations' are the titles and questions added to the markdown, or other metadata properties, as established in an <code>AnnotationModel</code> object.</p>"},{"location":"API/Configuration/config/#lmm_education.config.config.ConfigSettings","title":"<code>ConfigSettings</code>","text":"<p>               Bases: <code>Settings</code></p> <p>This object reads and writes to file the configuration options.</p> <p>Attributes:</p> Name Type Description <code>server</code> <p>the server specification for the web chat</p> <code>storage</code> <code>DatabaseSource</code> <p>where the database is located</p> <code>database</code> <code>DatabaseSettings</code> <p>database settings</p> <code>RAG</code> <code>RAGSettings</code> <p>generation of properties such as questions, summaries</p> <code>textSplitter</code> <code>TextSplitters</code> <p>the text plitter to use to form chunks</p> Note <p>The annotation model will usually want to include in the annotation information such as questions, that were generated as specified by the encoding model in the settings. To obtain an annotation model that is consistent with the encoding model, get the model through the member function <code>get_annotation_model</code>. Additional properties given to this function will include the properties as inherited properties.</p> Source code in <code>lmm_education/config/config.py</code> <pre><code>class ConfigSettings(LMMSettings):\n    \"\"\"\n    This object reads and writes to file the configuration options.\n\n    Attributes:\n        server: the server specification for the web chat\n        storage: where the database is located\n        database: database settings\n        RAG: generation of properties such as questions, summaries\n        textSplitter: the text plitter to use to form chunks\n\n    Note:\n        The annotation model will usually want to include in the\n        annotation information such as questions, that were generated\n        as specified by the encoding model in the settings. To obtain\n        an annotation model that is consistent with the encoding\n        model, get the model through the member function\n        `get_annotation_model`. Additional properties given to this\n        function will include the properties as inherited properties.\n    \"\"\"\n\n    storage: DatabaseSource = Field(\n        default=LocalStorage(folder=\"./storage\"),\n        description=\"Vector database local or remote source\",\n    )\n\n    database: DatabaseSettings = Field(\n        default_factory=DatabaseSettings,\n        description=\"Vector database settings\",\n    )\n\n    RAG: RAGSettings = Field(\n        default_factory=RAGSettings,\n        description=\"RAG settings\",\n    )\n\n    textSplitter: TextSplitters = Field(\n        default=TextSplitters(),\n        description=\"Provide the text splitter name to split \"\n        + \"text into chunks. The default uses the text blocks of\"\n        + \"the markdown as the chunks (no additional splitting).\",\n    )\n\n    def get_annotation_model(\n        self, keys: list[str] = []\n    ) -&gt; AnnotationModel:\n        \"\"\"\n        Returns an annotation model that is consistent with other\n        ConfigSettings options. The annotation model will add to\n        the model saved in the config file the appropriate keys.\n\n        Returns:\n            an annotation model object\n\n        Note:\n            use this function to add manual annotations to the\n                annotation model\n        \"\"\"\n        annotation_model: AnnotationModel = (\n            self.RAG.get_annotation_model()\n        )\n        annotation_model.add_inherited_properties(keys)\n        return annotation_model\n\n    @model_validator(mode='after')\n    def validate_comp_coll_name(self) -&gt; Self:\n        if self.textSplitter.splitter not in Splitter.__args__:\n            raise ValueError(\n                f\"Invalid splitter: {self.textSplitter.splitter}\\n\"\n                + \" must be one of {Splitter.__args__}\"\n            )\n        return self\n\n    model_config = SettingsConfigDict(\n        toml_file=DEFAULT_CONFIG_FILE,\n        env_prefix=\"LMMEDU_\",  # Uppercase for environment variables\n        frozen=True,\n        validate_assignment=True,\n        extra='forbid',  # Prevent unexpected fields\n    )\n\n    @classmethod\n    def settings_customise_sources(\n        cls,\n        settings_cls: type[BaseSettings],\n        init_settings: PydanticBaseSettingsSource,\n        env_settings: PydanticBaseSettingsSource,\n        dotenv_settings: PydanticBaseSettingsSource,\n        file_secret_settings: PydanticBaseSettingsSource,\n    ) -&gt; tuple[PydanticBaseSettingsSource, ...]:\n        \"\"\"Customize the order of settings sources.\"\"\"\n        return (\n            init_settings,\n            TomlConfigSettingsSource(settings_cls),\n            env_settings,\n        )\n\n    def __str__(self) -&gt; str:\n        return serialize_settings(self)\n\n    def __init__(self, **data: Any) -&gt; None:\n        \"\"\"\n        Initialize Settings with file existence verification.\n        Prints a warning message on the console if the file does not exist.\n        \"\"\"\n        config_path = Path(DEFAULT_CONFIG_FILE)\n        if not config_path.exists():\n            print(\n                f\"Configuration file not found: {config_path.absolute()}\\n\"\n                \"Creating a default configuration object.\"\n            )\n        super().__init__(**data)\n</code></pre>"},{"location":"API/Configuration/config/#lmm_education.config.config.ConfigSettings.__init__","title":"<code>__init__(**data)</code>","text":"<p>Initialize Settings with file existence verification. Prints a warning message on the console if the file does not exist.</p> Source code in <code>lmm_education/config/config.py</code> <pre><code>def __init__(self, **data: Any) -&gt; None:\n    \"\"\"\n    Initialize Settings with file existence verification.\n    Prints a warning message on the console if the file does not exist.\n    \"\"\"\n    config_path = Path(DEFAULT_CONFIG_FILE)\n    if not config_path.exists():\n        print(\n            f\"Configuration file not found: {config_path.absolute()}\\n\"\n            \"Creating a default configuration object.\"\n        )\n    super().__init__(**data)\n</code></pre>"},{"location":"API/Configuration/config/#lmm_education.config.config.ConfigSettings.get_annotation_model","title":"<code>get_annotation_model(keys=[])</code>","text":"<p>Returns an annotation model that is consistent with other ConfigSettings options. The annotation model will add to the model saved in the config file the appropriate keys.</p> <p>Returns:</p> Type Description <code>AnnotationModel</code> <p>an annotation model object</p> Note <p>use this function to add manual annotations to the     annotation model</p> Source code in <code>lmm_education/config/config.py</code> <pre><code>def get_annotation_model(\n    self, keys: list[str] = []\n) -&gt; AnnotationModel:\n    \"\"\"\n    Returns an annotation model that is consistent with other\n    ConfigSettings options. The annotation model will add to\n    the model saved in the config file the appropriate keys.\n\n    Returns:\n        an annotation model object\n\n    Note:\n        use this function to add manual annotations to the\n            annotation model\n    \"\"\"\n    annotation_model: AnnotationModel = (\n        self.RAG.get_annotation_model()\n    )\n    annotation_model.add_inherited_properties(keys)\n    return annotation_model\n</code></pre>"},{"location":"API/Configuration/config/#lmm_education.config.config.ConfigSettings.settings_customise_sources","title":"<code>settings_customise_sources(settings_cls, init_settings, env_settings, dotenv_settings, file_secret_settings)</code>  <code>classmethod</code>","text":"<p>Customize the order of settings sources.</p> Source code in <code>lmm_education/config/config.py</code> <pre><code>@classmethod\ndef settings_customise_sources(\n    cls,\n    settings_cls: type[BaseSettings],\n    init_settings: PydanticBaseSettingsSource,\n    env_settings: PydanticBaseSettingsSource,\n    dotenv_settings: PydanticBaseSettingsSource,\n    file_secret_settings: PydanticBaseSettingsSource,\n) -&gt; tuple[PydanticBaseSettingsSource, ...]:\n    \"\"\"Customize the order of settings sources.\"\"\"\n    return (\n        init_settings,\n        TomlConfigSettingsSource(settings_cls),\n        env_settings,\n    )\n</code></pre>"},{"location":"API/Configuration/config/#lmm_education.config.config.RAGSettings","title":"<code>RAGSettings</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>lmm_education/config/config.py</code> <pre><code>class RAGSettings(BaseModel):\n\n    titles: bool = Field(\n        default=True,\n        description=\"Annotate text blocks with titles to aid retrieval\",\n    )\n    questions: bool = Field(\n        default=False,\n        description=\"Annotate text with questions to aid retrieval\",\n    )\n    summaries: bool = Field(\n        default=False,\n        description=\"Add summaries as chunks to aid retrieval\",\n    )\n    encoding_model: EncodingModel = Field(\n        default=EncodingModel.CONTENT,\n        description=\"How the chunk propoerties are being encoded. \"\n        + \"Encoding options that are availble include hybrid dense+\"\n        + \"sparse embeddings and multivector embeddings.\",\n    )\n    annotation_model: AnnotationModel = Field(\n        default=AnnotationModel(),\n        description=\"Model to select metadata for annotations and \"\n        + \"filtering\",\n    )\n    retrieve_companion_docs: bool = Field(\n        default=True,\n        description=(\n            \"Retrieve full part of documents. A companion collection\"\n            \" must have been specified. If not, this setting will\"\n            \" be ignored.\"\n        ),\n    )\n    max_companion_docs: int = Field(\n        default=2,\n        gt=0,\n        lt=100,\n        description=(\n            \"Max number of retrieved full documents. Ignored if \"\n            \"`retrieve_companion_docs` set to False, or no \"\n            \"`companion_collection` was specified.\"\n        ),\n    )\n\n    def get_annotation_model(self) -&gt; AnnotationModel:\n        \"\"\"Returns an annotation model that is consistent with the\n        RAG settings\"\"\"\n        model: AnnotationModel = self.annotation_model.model_copy()\n\n        if self.titles:\n            if not model.has_property(TITLES_KEY):\n                model.add_own_properties(TITLES_KEY)\n        if self.questions:\n            if not model.has_property(QUESTIONS_KEY):\n                model.add_inherited_properties(QUESTIONS_KEY)\n        return model\n\n    def from_instance(\n        self,\n        titles: bool | None = None,\n        questions: bool | None = None,\n        summaries: bool | None = None,\n        encoding_model: EncodingModel | None = None,\n        annotation_model: AnnotationModel | None = None,\n        retrieve_companion_docs: bool | None = None,\n        max_companion_docs: int | None = None,\n    ) -&gt; 'RAGSettings':\n        \"\"\"Create a new RAGSettings object with modified properties\"\"\"\n        return RAGSettings(\n            titles=titles or self.titles,\n            questions=questions or self.questions,\n            summaries=summaries or self.summaries,\n            encoding_model=encoding_model or self.encoding_model,\n            annotation_model=annotation_model\n            or self.annotation_model,\n            retrieve_companion_docs=retrieve_companion_docs\n            or self.retrieve_companion_docs,\n            max_companion_docs=max_companion_docs\n            or self.max_companion_docs,\n        )\n</code></pre>"},{"location":"API/Configuration/config/#lmm_education.config.config.RAGSettings.from_instance","title":"<code>from_instance(titles=None, questions=None, summaries=None, encoding_model=None, annotation_model=None, retrieve_companion_docs=None, max_companion_docs=None)</code>","text":"<p>Create a new RAGSettings object with modified properties</p> Source code in <code>lmm_education/config/config.py</code> <pre><code>def from_instance(\n    self,\n    titles: bool | None = None,\n    questions: bool | None = None,\n    summaries: bool | None = None,\n    encoding_model: EncodingModel | None = None,\n    annotation_model: AnnotationModel | None = None,\n    retrieve_companion_docs: bool | None = None,\n    max_companion_docs: int | None = None,\n) -&gt; 'RAGSettings':\n    \"\"\"Create a new RAGSettings object with modified properties\"\"\"\n    return RAGSettings(\n        titles=titles or self.titles,\n        questions=questions or self.questions,\n        summaries=summaries or self.summaries,\n        encoding_model=encoding_model or self.encoding_model,\n        annotation_model=annotation_model\n        or self.annotation_model,\n        retrieve_companion_docs=retrieve_companion_docs\n        or self.retrieve_companion_docs,\n        max_companion_docs=max_companion_docs\n        or self.max_companion_docs,\n    )\n</code></pre>"},{"location":"API/Configuration/config/#lmm_education.config.config.RAGSettings.get_annotation_model","title":"<code>get_annotation_model()</code>","text":"<p>Returns an annotation model that is consistent with the RAG settings</p> Source code in <code>lmm_education/config/config.py</code> <pre><code>def get_annotation_model(self) -&gt; AnnotationModel:\n    \"\"\"Returns an annotation model that is consistent with the\n    RAG settings\"\"\"\n    model: AnnotationModel = self.annotation_model.model_copy()\n\n    if self.titles:\n        if not model.has_property(TITLES_KEY):\n            model.add_own_properties(TITLES_KEY)\n    if self.questions:\n        if not model.has_property(QUESTIONS_KEY):\n            model.add_inherited_properties(QUESTIONS_KEY)\n    return model\n</code></pre>"},{"location":"API/Configuration/config/#lmm_education.config.config.create_default_config_file","title":"<code>create_default_config_file(file_path=None)</code>","text":"<p>Create a default settings file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path | None</code> <p>config file (defaults to config.toml)</p> <code>None</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If tomlkit is not available</p> <code>OSError</code> <p>If file cannot be written</p> <code>ValueError</code> <p>If settings cannot be serialized</p> <code>ValidationError</code> <p>if config.toml is invalid</p> <code>TOMLDecodeError</code> <p>if config.toml is invalid</p> Example <pre><code># Creates config.toml in base folder with default values\ncreate_default_config_file()\n\n# Creates custom config file\ncreate_default_config_file(file_path=\"custom_config.toml\")\n</code></pre> Source code in <code>lmm_education/config/config.py</code> <pre><code>def create_default_config_file(\n    file_path: str | Path | None = None,\n) -&gt; None:\n    \"\"\"Create a default settings file.\n\n    Args:\n        file_path: config file (defaults to config.toml)\n\n    Raises:\n        ImportError: If tomlkit is not available\n        OSError: If file cannot be written\n        ValueError: If settings cannot be serialized\n        ValidationError: if config.toml is invalid\n        tomllib.TOMLDecodeError: if config.toml is invalid\n\n    Example:\n        ```python\n        # Creates config.toml in base folder with default values\n        create_default_config_file()\n\n        # Creates custom config file\n        create_default_config_file(file_path=\"custom_config.toml\")\n        ```\n    \"\"\"\n    from lmm.config.config import create_default_config_file as _cdcf\n\n    if file_path is None:\n        file_path = DEFAULT_CONFIG_FILE\n\n    file_path = Path(file_path)\n\n    _cdcf(file_path, ConfigSettings)\n</code></pre>"},{"location":"API/Configuration/config/#lmm_education.config.config.load_settings","title":"<code>load_settings(*, file_name=None, logger=ExceptionConsoleLogger())</code>","text":"<p>Load and return a ConfigSettings object from the specified file.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str | Path | None</code> <p>Path to settings file (defaults to config.toml)</p> <code>None</code> <code>logger</code> <code>LoggerBase</code> <p>logger to use. Defaults to a exception-raising logger. This centralizes exception handling, instead of writing the except clauses for each instantiation of ConfigSettings().</p> <code>ExceptionConsoleLogger()</code> <p>Returns:</p> Name Type Description <code>ConfigSettings</code> <code>ConfigSettings | None</code> <p>The loaded configuration settings object, or</p> <code>ConfigSettings | None</code> <p>None if exception raised.</p> Expected behaviour <p>Exceptions handled through logger, but raises exceptions in the default logger.</p> Example <pre><code># Load settings from a custom config file\nsettings = load_settings(\"my_config.toml\")\n\n# Load settings using Path object\nfrom pathlib import Path\nsettings = load_settings(Path(\"configs/custom.toml\"))\n</code></pre> Note <p>Use of an ExceptionConsoleLogger still requires to check that return value is not None to satisfy a type checker.</p> <pre><code>logger = ExceptionConsoleLogger()\nsettings = load_settings(logger=logger)\nif settings is None:\n    raise ValueError(\"Unreacheable code reached\")\n</code></pre> <p>Here, the type checker is told that settings is not None, but the condition is always satisfied because load_settings will raise an exception whenever it would be returning None.</p> <p>Contrast with the following:</p> <pre><code>logger = ConsoleLogger()\nsettings = load_settings(logger=logger)\nif settings is None:\n    raise ValueError(\"Could not read config.toml\")\n</code></pre> Source code in <code>lmm_education/config/config.py</code> <pre><code>def load_settings(\n    *,\n    file_name: str | Path | None = None,\n    logger: LoggerBase = ExceptionConsoleLogger(),\n) -&gt; ConfigSettings | None:\n    \"\"\"Load and return a ConfigSettings object from the specified file.\n\n    Args:\n        file_name: Path to settings file (defaults to config.toml)\n        logger: logger to use. Defaults to a exception-raising logger.\n            This centralizes exception handling, instead of writing\n            the except clauses for each instantiation of ConfigSettings().\n\n    Returns:\n        ConfigSettings: The loaded configuration settings object, or\n        None if exception raised.\n\n    Expected behaviour:\n        Exceptions handled through logger, but raises exceptions in\n        the default logger.\n\n    Example:\n        ```python\n        # Load settings from a custom config file\n        settings = load_settings(\"my_config.toml\")\n\n        # Load settings using Path object\n        from pathlib import Path\n        settings = load_settings(Path(\"configs/custom.toml\"))\n        ```\n\n    Note:\n        Use of an ExceptionConsoleLogger still requires to check that\n        return value is not None to satisfy a type checker.\n\n        ```python\n        logger = ExceptionConsoleLogger()\n        settings = load_settings(logger=logger)\n        if settings is None:\n            raise ValueError(\"Unreacheable code reached\")\n        ```\n\n        Here, the type checker is told that settings is not None, but\n        the condition is always satisfied because load_settings will\n        raise an exception whenever it would be returning None.\n\n        Contrast with the following:\n\n        ```python\n        logger = ConsoleLogger()\n        settings = load_settings(logger=logger)\n        if settings is None:\n            raise ValueError(\"Could not read config.toml\")\n        ```\n    \"\"\"\n    from lmm.config.config import load_settings as _load_settings\n\n    if file_name is None:\n        file_name = DEFAULT_CONFIG_FILE\n\n    file_path = Path(file_name)\n\n    return _load_settings(\n        file_name=file_path,\n        logger=logger,\n        settings_class=ConfigSettings,\n    )\n</code></pre>"},{"location":"Architecture/Architecture/","title":"Users and interfaces","text":"<p>There are broadly three groups of users for LM markdown for eduction, and correspondingly three interfaces.</p> <ol> <li> <p>Student users, or more generally RAG users, are those that log on to a web server and utilize the services (chats, webcasts etc.) through a web interface. There is a skeleton implementation of this web interface using the Gradio package. The web interface is self-explanatory for its users.</p> </li> <li> <p>RAG authors create, revise, and ingest documents. They primarily interact with the system through a command language interface (CLI), although they could also write Python scripts to use the API directly. See the chapter on RAG authoring in the Handbook.</p> </li> <li> <p>Administrators and developers set up an LM Markdown for Education server. The package is managed by poetry, which may be used for installation. They also use the files config.toml and appchat.toml to configure the server and the RAG application. See the configuration manual.</p> </li> </ol> <p>The figure below shows how the softare is organized to serve these interfaces.</p> <p></p>"},{"location":"Architecture/Architecture/#gradio-web-interface","title":"Gradio web interface","text":"<p>Gradio allows quickly prototyping web interfaces for AI applications. One specifies the elements of the interface stacked along rows or columns of the page, and provides callbacks to respond to the user requests. Gradio has a markedly server-centered architecture. All action takes place on the server when the callbacks are invoked by the framework, which passes in the argument of the callback the request of the user or information on events on the web client.</p> <p>The Gradio modules of the implementation are roughly composed of three sections. In the first, moudle-level objects are set up to reference other functionality (the database, the language model). This functionality is automatically configured as specified in config.toml and appchat.toml. In the second section, the callbacks are defined using these module-level objects. In the third and final section, the Gradio interface is defined with the callback functions, and a launch function is called on the Gradio interface object. This function blocks while a server is set up with the application. Hence, to start a server one runs the module through the Python interpreter. For example, <code>python -m appChat</code> starts the appChat server.</p> <p>The coupling with Gradio is weak. One could easily implement an alternative web server offering the same callback functionality through endpoints.</p> <p>In a chat application, the important callback is the one that provides the message from the user, optionally with the list of the messages of the previous interaction (this function is <code>gradio_callback_fn</code> in appChat.py). This callback returns the response to be displayed in the chat.</p>"},{"location":"Architecture/Architecture/#cli","title":"CLI","text":"<p>LM Markdown for Education offers two alternative CLIs. The first CLI can be invoked from a bash or Powershell terminal pointing to the folder where LM Markdown for Education was installed. Commands have the form lmme command [subcommand|--parameters ...]. Type <code>lmme --help</code> for a list and short explanation of all available commands. They may take some time since at each invocation the Python interpreter and all libraries must be reloaded.</p> <p>To load the library, keep it in memory and start a terminal that accepts the same commands, type <code>lmme terminal</code>. Type <code>help</code>for a list and short explanation of the same available commands. This second CLI preloads Python and all libraries, so that at each subsequent invocation the response is very fast. The first CLI is most useful when one need to call LM Markdown for Education with only one or two commands. The second CLI is probably the most practical choice when the interaction with the language model is more intensive.</p>"},{"location":"Architecture/Architecture/#configuration-and-management","title":"Configuration and management","text":"<p>This is a poetry-managed project: after cloning the repository, installation of dependencies is given by <code>poetry install</code>.</p> <p>Two configuration files (config.toml and appchat.toml) change the settings of the project started as a web server or through the CLI. When using the framework from the CLI, many of these options can be overridden. All can be overridden when using the framework programmatically.</p> <p>LM Markdown for Education uses the Pydantic settings package to read from and write to configuration files. This package validates the input from the configuration files or from the parameters given at the CLI.</p>"},{"location":"Architecture/Architecture/#main-software-domains","title":"Main software domains","text":"<p>The software provides the following main functionality:</p> <ol> <li> <p>Interaction with a vector database according to a specified encoding model. The encoding model is the specification of how semantic information should be embedded in the database. For more details, see encoding. At present, one vector database implementation is supported (Qdrant).</p> </li> <li> <p>Preparation and processing of markdown documents for RAG use. This functionality is based on services of the LM markdown package, and includes parsing and representing content of markdown documents.</p> </li> <li> <p>Interaction with language models and language model and embedding providers. This functionality is based on services of the LM markdown package. It currently uses LangChain models of the LLM providers, allowing use of different models in the same application. Some functionality is packaged into LangChain \"runnables\", which can be simply called through the member function 'invoke' to obtain the response from the language model for a specific task.</p> <p>Finally, all the above functionality is plugged onto a</p> </li> <li> <p>configuration module, which allows customization (choice of language providers, revision of prompts, encoding model to the database, etc.) The file config.toml contains settings on the database and the language models. The file appchat.toml configures the web chat application.</p> </li> </ol>"},{"location":"Architecture/Architecture/#vector-database","title":"Vector database","text":"<p>Common language model infrastructures allow swapping various vector databases to implement RAG. LM Markdown for Education uses Qdrant, a vector database implemented with sqlite, and offers fine-graded options to specify the encoding of information used to embed the semantic properties of documents and document parts (see encoding).</p> <p>A specific option in LM Markdown for Education is the possibility of retrieving whole document parts instead of the fragments of text used to embed documents. Text emebddings struggle to accurately encode the semantics of large texts. For this reason, the text is usually chunked up into smaller pieces, and the embeddings of these pieces are used to retrieve the content from the vector database. In LM Markdown for Education, text is chunked, but at retrieval the whole section of text where the chunk was located can optionally be retreived instead of the chunk itself (this functionality is available natively in Qdrant). This decouples the encoding for text for the purposes of locating relevant content and the selection of text that is retrieved. When a whole section of text is retrieved, the context that the language model has access to to formulate a response is coherent and systematic.</p> <p>Large language models can at present handle very large context windows, and have no difficulty extracting information from a larger portion of text. They will use the text in the context effectively even when this text is large. When the context contains smaller disjoint chunks, the language models can make a very good job of producing a coherent response (they present a strong tendency to produce polished, fluent and somewhat conventional text). However, they will introduce more of their own content in filling the gaps between the retrieved chunks. When the text in the context is organic and coherent, there is a higher chance that its content is reproduced accurately.</p>"},{"location":"Architecture/Architecture/#markdown-document-processing","title":"Markdown document processing","text":"<p>LM markdown processes markdown documents by internally building a hierarchical representation of their content (i.e., a tree) based on the headings and the heading levels (i.e., the titles of chapters and the level of the chapters). This hierarchical representation allows LM markdown to exchange messages with language models about the markdown file at different levels of granularity, providing the text at different levels of the tree.</p> <p>Another specific functionality of LM markdown is the use of metadata in markdown documents. Metadata (consisting yaml objects embedded in the document) are usually only present as the header of the whole document. The markdown specification, however, allows metadata to be interspersed between blocks across the whole document without restrictions. LM markdown uses these intra-document metadata blocks to specify metadata at each level of the hierarchical representation of the document. Metadata before a heading annotate the whole text under the heading and subheadings, and metadata before a text block annotate only the following text block (for details, see Working with markdown).</p> <p>These metadata are used in two ways. One is to allow the language model to reply to queries of the user. The other is to insert information extracted from text that is used to encode the semantic properties of the text for RAG. These uses are illustrated in RAG authoring.</p>"},{"location":"Architecture/Architecture/#interaction-with-language-models","title":"Interaction with language models","text":"<p>The project uses LangGraph to provide a unified interface for using different large language model providers and specify workflows of chained prompted models/agents. The default graph is \"workflow\", which is a simple workflow with low language model use. This workflow performs basic checks on the query, integrates it with the history of the messages to retrieve context, and sends context and query to the language model for response systetization. A second graph, \"agent\", is being developed to explore agentic retrieval of context.</p> <p>The functionality of the interaction with language models is split between the graphs themselves and the streams emitted by these graphs. Several properties of the applications, such as logging the exchanges to a database, streaming the retrieved context, validation of content etc. take place at the level of the stream, not at the graph. The output stream of the same graph may be combined with several stream adapters that provide the desired functionality. The adapters library is generic and should be applicable to different graphs.</p> <p>This architecure is extensible, as any LangGraph graph may be swapped in to replace the preformed graphs, as long as it uses state and dependency injection objects specified in the /workflows/langchain/base.py module.</p>"},{"location":"Architecture/Implementation/","title":"Implementation notes","text":"<p>LM markdown for education leverages the code from LM markdown to implement facilities to provide lectures online, using language models for the interaction with students. Here, we describe briefly the architecture of the parts that are specific to LM markdown for education.</p>"},{"location":"Architecture/Implementation/#configuration-config-module","title":"Configuration (config module)","text":"<p>This module defines a <code>ConfigSettings</code> object to read from <code>config.toml</code> the settings of the project. <code>ConfigSettings</code> inherit from the <code>Settings</code> class in LM markdown, so that the <code>config.toml</code> file includes both the settings that configure LM markdown and LM markdown for education. The <code>Settings</code> class inherits from pydantic's <code>BaseSettings</code>, which handles reading the settings from <code>config.toml</code> and validating them.</p> <p>A convenient aspect of these objects is that they may be initialized in code for the parts that need override the settings in config.toml, with the others being initialized from the values there. When looking in a file like <code>config.toml</code>, one can see the keys that are available for initialization. Keys that are under headings can by initialized via a dictionary.</p> <p>Example:</p> <pre><code># content of config.toml\n[RAG]\nquestions = false\nsummaries = false\n\n[embeddings]\ndense_model = \"OpenAI/text-embedding-3-small\"\n</code></pre> <p>These settings can be overriden in code as follows:</p> <pre><code>config = ConfigSettings(\n    RAG = {'questions': True},\n    embeddings = {'dense_model': \"OpenAI/text-embedding-3-large\"}\n)\n</code></pre> <p>The resulting <code>config</code> object will be initialized with <code>questions = true</code>, and all other values overridden as in code. (This coding will be flagged a type-invalid by the type checker, because Pydantic validates the content, also enforcing coertion, behind the hood).</p>"},{"location":"Architecture/Implementation/#error-handling","title":"Error handling","text":"<p>The code distinguishes between three types of exceptions: genuine coding errors (for example, a <code>match</code> statement with a missing case), validation errors, and errors that arise from normal adversity (like invalid yaml headers from the users, missing internet connections, etc.).</p> <p>The first type of errors are designed to crash the application while delivering an error trace that allows fixing them. Of note, the code is written using the most strict setting for type checking available, which means that most of this type of errors should be flagged by the type checker already. This kind of errors should be truly exceptional.</p> <p>The second type of errors arise when data for operating the framework is validated. For example, if an invalid setting is entered in <code>config.toml</code>, the code will crash as soon as the setting is read, leaving an error trace in the console that is informative of the problem. Also functions that are meant to be entered in Python's REPL are validated by pydantic, crashing immediately on invalid inputs instead of creating some problem down the line. This behaviour is justified by the fact that these invalid inputs are not compatible with the program's functioning. Note that function calls that are not validated by Pydantic are supposed to be used in code, not in a REPL, and in code strict type checking is followed in the whole framework.</p> <p>The third and final kind of \"errors\" are not cosidered exceptions, but are modelled in code by a <code>Logger</code> class (provided by the LM markdown framework). The term 'modelled' refers to the fact that this class is designed so that these situations may be handled like any other situation that code is supposed to handle, contrary to what the terms 'exceptions' or 'errors' imply. An object of the <code>Logger</code> class is passed to functions that follow this convention. Because different implementations of this class exist, the error handling may be customized according to the context in which the code is called, or the way in which these situation are modelled:</p> <ul> <li>providing a console trace. This is useful when calling code manually through the REPL, and is the default behaviour of the logger.</li> <li>collecting the error into a list of error messages. The list needs be checked manually. This is used in the code mostly in the tests.</li> <li>throwing a Python exception. This is useful when interfacing with code that behaves traditionally by rasing exceptions, as in the Langchain and in Qdrant APIs, and do not model the exception as it is done elsewehere in the LM markdow for education framework.</li> <li>logging a console message and throwing an exception. Similar use as above.</li> </ul> <p>The functions that adopt this convention are easily recognizable by the fact that they take a <code>Logger</code> object as a last argument. Many functions do not do this; they are function that may be viewed as \"pure\": they are not expected to raise exceptions, unless a genuine coding error intervenes.</p> <p>Functions that adopt the logger convention usually return a value that can be <code>None</code>. Because the type checker may be used to enforce checking for <code>None</code> return values, the programmer can consider this type of error handling explicit, as long as the type checker discipline is followed.</p> <p>Some functions from the LM markdown module do not return <code>None</code>, but things such as an empty list where a list is expected. This behaviour is intentional: several errors result in messages displayed in the markdown for the user to fix; from the perspective of the code, the program should keep working. This behaviour is documented in LM markown.</p>"},{"location":"Architecture/Implementation/#qdrant-vector-store","title":"Qdrant vector store","text":"<p>LM markdown for education uses Qdrant to store the RAG data. The package is organized as follows:</p> subpackage module purpose . vector_store_qdrant interface to qdrant code lmm chunks main code to tranform text into chunks langchain vector_store_qdrant_langchain langchain interface to the vector store (for retrieval)"},{"location":"Architecture/Implementation/#vector_store_qdrant-module","title":"vector_store_qdrant module","text":"<p>The Qdrant API uses A <code>QdrantClient</code> object as an argument to all functions that write and read to/from the database. The functions in this module also take this object as an argument; the object may be initialized directly through a call to the Qdrant API, or through the function <code>client_from_config</code>. This function takes a <code>ConfigSettings</code> object, reading from it the <code>storage</code> value that is relevant to initialize the database:</p> <ul> <li><code>\":memory:\"</code>: initializes the database in memory</li> <li><code>LocalStorage</code>: a class representing a local folder</li> <li><code>RemoteSource</code>: a class representing a remote Qdrant server.</li> </ul> <p>Note that if you obtain a client from the <code>client_from_config</code> function, you'll have to handle closing the connection to the database yourself. A better option is to use the function <code>global_client_from_config</code> from the vector_store_qdrant_context module. This function centralizes creation and destruction (including closing connections) of Qdrant objects. Only one connection is created and is closed automatically.</p> <p>Within the database represented by the <code>QdrantClient</code> object, there are collections (the equivalent of tables in a traditional database) that are initialized through a call to <code>initialize_collection</code>. </p> <p>The modalities in which data are stored in the database are captured by two elements: the <code>QdrantEmbeddingModel</code> and <code>Chunk</code> objects. The <code>QdrantEmbeddingModel</code>, which is declared in the call to <code>initialize_collection</code>, specifies how data are embedded in the database. These are the embedding models that are supported by Qdrant, as supported by the framework:</p> code config value description DENSE \"dense\" text embedded with dense vector MULTIVECTOR \"multivector\" text and annotations embedded with dense multivector SPARSE \"sparse\" annotations embedding only, sparse HYBRID_DENSE \"hybrid_dense\" sparse annotations, dense text HYBRID_MULTIVECTOR \"hybrid_multivector\" sparse annotations, dense text and annotations UUID \"UUID\" no embedding (chunks retrieved by UUID) <p>Dense embedding uses a dense vector to capture text; dense encoding is also used by Multivector, but in this case more than input is given to the encdoing model, resulting in two distinct embedding vectors (one from text, one from annotations). Sparse only embeds annotations, and the other combine dense and sparse encoding. The UUID encoding only uses an ID.</p> <p>These settings are used in conjunction with an annotation model to define the encoding strategy in the database. The annotation model defines what metadata properties are used in the embedding. The <code>encoding_to_qdrantembedding_model</code> function is used internally to translate an <code>EncodingModel</code> object into the embedding model understood by Qdrant.</p> <p>Instead of providing a <code>QdrantEmbeddingModel</code>, one can  provide a <code>ConfigSettings</code> object from which the <code>QdrantEmbeddingModel</code> may be deduced. A <code>ConfigSettings</code> object also overrides the embedding model specified in <code>config.toml</code>, which are used when a <code>QdrantEmbeddingModel</code> is specified.</p> <p>The <code>Chunk</code> objects are the elementary units that are stored in the database, i.e. the equivalent of records (Qdrant calls them \"points\"). In frameworks such as Langchain or Llamaindex, these objects are called \"Document\". Chunk objects emerge from chunking up large portions of text, however (the coduments proper), and also contain the metadata used for embedding them. The <code>chunk</code> module, detailed below, handles the conversion from markdown document to chunks prior to ingesting. The <code>vector_store_qdrant</code> module only handles ingesting and retrieving chunks:</p> function purpose upload upload chunks into database query retrieves chunks based on query text query_grouped retrieves text stored in a companion collection <p>All these functions are also present in an asynchronous version, and take a logger object to model errors.</p> <p>The <code>upload</code> function takes a list of chunks and ingests them into the database. Importantly, the embedding takes place here: chunk contain text and annotations that define the emebdding, and the <code>upload</code> function takes care of calling the language model and the sparse embedding library to create the embeddings (this work is handled internally by the <code>chunk_to_points</code> function). The specifications are loaded from <code>config.toml</code>.</p> <p>Query functions return list of <code>ScoredPoint</code> object, which is what Qdrant returns. These data may be transformed into markdown blocks by the <code>points_to_blocks</code> function.</p> <p>Here is a schematic example of the code that ingests and retrieves chunks/records into/from the database.</p> <pre><code># get a client as specified in config.toml\nclient = client_from_config()\nif client is None\n    ... #(error handling)\n\npoints: list[Point] = upload(\n    client,      # the Qdrant client object \n    collection_name = \"documents\", # the collection to ingest into\n    model = QdrantEmbeddingModel.DENSE, \n    chunks = chunks,      # the chunks we want to ingest\n)\n\n# Retrieval code with query text\npoints: list[ScoredPoints] = query(\n    client, \n    collection_name = \"documents\",\n    model = QdrantEmbeddingModel.DENSE,\n    querytext = \"What are the main uses of logistic regression?\",\n    limit = 6,        # max 6 ScoredPoints\n    payload = True,   # all payload fields\n)\n\n# Retrieve text\nfor pt in points:\n    print(f\"{pf.score} - {pt.payload['page_content']}\\n\")\n\n</code></pre> <p>The <code>ScoredPoint</code> object is the Qdrant object in which the data from the database are returned. The 'page_content' field contains the text that was chunked, while other fields contain other parts of the metadata that were ingested in the database (based on the annotation model).</p>"},{"location":"Architecture/Implementation/#vector_store_qdrant_langchain-module","title":"vector_store_qdrant_langchain module","text":"<p>Intead of using the functions from the vector_store_qdrant module to retrieve data from the vector database, it is possible to initialize a Langchain retriever. Instead of creating a QdrantClient object from a config settings, the Langchain retriever is initialized by a call to `from_config_settings':</p> <pre><code>retriever = QdrantVectorStoreRetriever.from_config_settings()\nresults: list[Document] = retriever.invoke(\n    \"What are the main uses of logistic regression?\"\n)\n</code></pre> <p>Here, <code>from_config_settings</code> retrieves the settings from <code>config.toml</code>, but a partially initialized <code>ConfigSettings</code> object may be passed as an argument to override these settings. <code>AsynQdrantVectorStoreRetriever</code> provides asynchronous calls. In alternative, the retriever object may be instantiated explicitly:</p> <pre><code>client = QdrantClient(\"./storage\")\nretriever = QdrantVectorStoreRetriever(\n    client, \n    collection_name = \"documents',\n    embedding_model = QdrantEmbeddingModel.DENSE,\n)\n</code></pre> <p>Note that the embedding_model must match that used to embed the chunks originally (this is an argument in favour of creating a <code>config_settings</code> object and using only this object to interface to all Qdrant functions).</p> <p>The query returns here a list of <code>Document</code> object, the record representation in Langchain. These objects contain two fields: <code>page_content</code>, the text, and <code>metadata</code>, a dictionary with key/value pairs.</p>"},{"location":"Architecture/Implementation/#chunks-module","title":"chunks module","text":"<p>This is a key module in the vector storage implementation, as it handles the conversion from a parsed markdown file (in the form of a list of blocks) into a list of chunks.</p> <p>The chunks module defines an encoding model to map properties of the markdown to the embeddings used to make parts of the markdown retrievable. The encoding model includes, on the one hand, the text that is stored in the database, and on the other hand metadata propoerties stored in the markdown that may be used in the embedding. Such metadata properties are referred to as annotations, to distinguish them from other metadata that are not suitable for embedding (for example, properties used for housekeeping purposes).</p> <p>This is the specification of the encoding model.</p> model description NONE no encoding (chunks identified by their UUID) CONTENT encode only textual content in dense vector MERGED encode textual content merged with metadata MULTIVECTOR encode content and annotations using multivectors SPARSE sparse encoding of annotations only SPARSE_CONTENT sparse encoding of content only SPARSE_MERGED sparse encoding of merged content and annotations SPARSE_MULTIVECTOR sparse encoding of merged content and annotations using multivectors <p>This is the specification of the annotation model, i.e. what counts as an annotation.</p> model description inherited properties properties own and inherited by parent nodes own properties properties owned in the node metadata filters (reserved for future use) <p>The central function in the module is <code>blocks_to_chunks</code>, which takes a list of parsed metadata blocks and returns a list of chunks (which are given to <code>upload</code> in the <code>vector_store_qdrant</code> module).</p> <pre><code>def blocks_to_chunks(\n    blocklist: list[Block],\n    encoding_model: EncodingModel,\n    annotation_model: AnnotationModel | list[str] = AnnotationModel(),\n    logger: LoggerBase = logger,\n) -&gt; list[Chunk]:\n</code></pre> <p>Internally, this function does the following. It uses <code>scan_rag</code> to generate text id's and UUIDs for the text blocks, if they are missing (<code>scan_rag</code> is idempotent). It then produces the tree representation of the blocks, and collected the metadata properties (inheriting them if appropriate) as specified by the annotation model. It also collects other metadata for possible storage in the payload of the database, even if these metadata properties are not used in the embedding. </p> <p>Note that <code>scan_rag</code> is not used here to produce annotations with a language model. This step should take place previously and while using an explicit call.</p> <p>The explicit call may be done so:</p> <pre><code>from lmm.scan.scan_rag import ScanOpts, markdown_rag\n\nmarkdown_rag(\"MyMarkdown.md\", ScanOpts(questions=True, titles=True))\n</code></pre> <p>This saves MyMarkdown.md with the annotations.</p>"},{"location":"Architecture/Implementation/#implementation-encoding","title":"Implementation encoding","text":"<p>The following tyble summarizes the flow of directives and their effects from encodings to embeddings specifications.</p> encoding effect qdrant emb. effect NONE (no effect) UUID uuid -&gt; id (also in all others) CONTENT content -&gt; dense_encoding DENSE dense_encoding -&gt; vector MULTIVECTOR content -&gt; dense_encoding MULTIVECTOR annotations, dense_encoding -&gt; [vectors] MERGED annotations+content -&gt; dense_encoding DENSE dense_encoding -&gt; vector SPARSE annotations -&gt; sparse_encoding SPARSE sparse_encoding -&gt; sparse vector SPARSE_CONTENT annotations -&gt; sparse_encoding HYBRID_DENSE sparse_encoding -&gt; sparse vector content -&gt; dense_encoding dense_encoding -&gt; vector SPARSE_MULTIVECTOR annotations -&gt; sparse_encoding HYBRID_MULTIVECTOR sparse_encoding -&gt; sparse vector content -&gt; dense_encoding annotations, dense_encoding -&gt; [vectors] SPARSE_MERGED annotations -&gt; sparse_encoding HYBRID_DENSE sparse_encoding -&gt; sparse vector annotations+content -&gt; dense_encoding dense_encoding -&gt; vector <p>Effect of encoding: in blocks_to_chunks, chunks module; qdrant emb: in encoding_to_qdrant_embedding, vector_store_qdrant module; effect of qdrant embedding: chunks_to_points, vector_store_qdrant module.</p>"},{"location":"Architecture/Implementation/#identification-of-portions-of-markdown-files","title":"Identification of portions of markdown files","text":"<p>Markdown files can be repeatedly ingested into a vector database without giving rise to duplicates. This relies on the value of a <code>docid</code> property in the header, which is set automatically by <code>scan_rag</code> at each call, unless this property is already present. It can be set manually to an intelligible string, as long as it is unique across markdown documents.</p> <p>The steps to identify parts of the text are the following.</p> <ol> <li>A markdown document is created and edited. Optionally, a <code>docid</code> property is added manually.</li> <li>Metadata can be created if desired and edited.</li> <li>Missing metadata are created automatically with the help of a language model. Since metadata are not recomputed if the text did not change, there can be several cycles of steps 2 and 3.</li> <li>The markdown document is split by <code>scan_split</code>.</li> <li>After splitting, the document is sent to <code>blocks_to_chunks</code> in the chunks module. Internally, this function calls <code>scan_rag</code>, which will create a <code>docid</code> if is is missing, and a sequential <code>textid</code> property for each chunk (i.e., shorter text block). The value of this property is {docid}.{sequential number}. A UUID field will also be created based on the textid value, which will identify the chunk. As long as the document grows, it can be repeatedly ingested without creating duplications.</li> <li>The chunks are sent to <code>upload</code> in the vector_store_qdrant module. Internally, this function calls the <code>chunks_to_points</code> function which creates embedding for each chunk, based on its text and annotations, as directed by the encoding model. </li> </ol> <p>All these steps are handled by <code>markdown_upload</code> (see next section).</p>"},{"location":"Architecture/Implementation/#ingestion-ingest-module","title":"Ingestion (ingest module)","text":"<p>This module contains the function <code>markdown_upload</code>, which takes a list of file names or the markdown documents, and a optional <code>ConfigSettings</code> object. This object specifies how the files are processed and ingested in the vector database, based on the settings specified in <code>config.toml</code>, or by settings that override these when an object is specified explicitly.</p> <p>Each markdown file is processed through a series of steps.</p> <p>First, it is read in and parsed by <code>markdown_scan</code>, a function from LM markdown scan module. This function checks that the markdown is well-formed and there are no problems. </p> <p>Second, it is processed by <code>blocklist_encode</code>, another function in the module. This function implements the specific RAG strategy used in the package and specified in the configuration options in <code>config.toml</code>. Internally, it calls <code>scan_rag</code> to use the language model to create annotations and summaries, if required, and <code>scan_split</code> to split the text into chunks. The chunks are created with the <code>blocks_to_chunks</code> function from the chunks module.</p> <p>Finally, the chunks are passed to <code>blocklist_upload</code>, another function in the module that is tasked with implementing the configuration options when calling the vector store qdrant functions and save the data in the vector database.</p> <p>(There is no equivalent functions for retrieval. One can just instantiate the Langchain retriever pointing at the database and call the <code>invoke</code> function with the query text.)</p>"},{"location":"Architecture/Overview/","title":"Overview","text":"<p>This file provides an overview for working with code in this repository.</p>"},{"location":"Architecture/Overview/#lm-markdown-for-education","title":"LM Markdown for Education","text":"<p>LM Markdown for Education is a RAG (Retrieval-Augmented Generation) system that enables educators to create AI-powered interactive lectures and chatbots from markdown documents. The system ingests markdown files into a vector database (Qdrant), processes them with language models, and serves them through chat interfaces or interactive webcasts.</p>"},{"location":"Architecture/Overview/#installation","title":"Installation","text":"<p>See Installation.</p>"},{"location":"Architecture/Overview/#development-setup","title":"Development Setup","text":"<pre><code># Install dependencies\npoetry install\n\n# Create default configuration files\npoetry run lmme create-default-config-file\n\n# Activate virtual environment\npoetry shell\n\n# Run tests\npoetry run pytest\n\n# Run a specific test\npoetry run pytest tests/test_integration.py -v\n\n# Run tests matching a pattern\npoetry run pytest -k \"test_config\"\n\n# Serve documentation locally\nmkdocs serve\n</code></pre>"},{"location":"Architecture/Overview/#core-commands","title":"Core Commands","text":""},{"location":"Architecture/Overview/#cli-interface","title":"CLI Interface","text":"<p>The <code>lmme</code> command provides the main CLI. For interactive use with faster response times:</p> <pre><code># Start interactive terminal (preloads Python and libraries)\nlmme terminal\n\n# Then use commands without 'lmme' prefix:\n&gt; scan_rag MyLecture.md\n&gt; ingest MyLecture.md\n&gt; query \"What are linear models?\"\n</code></pre>"},{"location":"Architecture/Overview/#rag-workflow","title":"RAG Workflow","text":"<pre><code># 1. Process markdown with language model annotations (summaries, questions)\nlmme scan-rag MyLecture.md\n\n# 2. Ingest processed markdown into vector database\nlmme ingest MyLecture.md\n\n# 3. Query the RAG system\nlmme query \"your question here\"\n\n# Query the database directly (without LLM synthesis)\nlmme querydb \"your question here\"\n\n# Ingest entire folder\nlmme ingest-folder ./lectures --extensions \".md;.Rmd\"\n</code></pre>"},{"location":"Architecture/Overview/#content-authoring","title":"Content Authoring","text":"<pre><code># Interact with LLM for content improvement (add 'query:' metadata in markdown)\nlmme scan-messages MyLecture.md\n\n# Clear LLM interaction messages from markdown\nlmme scan-clear-messages MyLecture.md\n\n# List titles that have changed since last scan\nlmme scan-changed-titles MyLecture.md\n</code></pre>"},{"location":"Architecture/Overview/#running-applications","title":"Running Applications","text":"<pre><code># Start chat web application (Gradio)\npython -m appChat\n\n# Start webcast application (video + chat)\npython -m appWebcast\n\n# Start videocast application\npython -m appVideocast\n</code></pre>"},{"location":"Architecture/Overview/#architecture","title":"Architecture","text":""},{"location":"Architecture/Overview/#directory-structure","title":"Directory Structure","text":"<ul> <li>lmm_education/ - Main package</li> <li>config/ - Configuration management (<code>config.toml</code>, <code>appchat.toml</code>)</li> <li>stores/ - Vector database interfaces<ul> <li><code>vector_store_qdrant.py</code> - Core Qdrant operations</li> <li><code>langchain/vector_store_qdrant_langchain.py</code> - LangChain integration</li> </ul> </li> <li>workflows/ - LangGraph workflows for chat and query processing<ul> <li><code>langchain/chat_graph.py</code> - Simple workflow for chat</li> <li><code>langchain/chat_agent.py</code> - Advanced agentic workflow</li> <li><code>langchain/stream_adapters.py</code> - Stream processing adapters</li> </ul> </li> <li><code>ingest.py</code> - Markdown ingestion pipeline</li> <li><code>query.py</code> - RAG query functions</li> <li><code>lmme.py</code> - CLI interface (Typer)</li> <li>appbase/ - Shared setup for Gradio applications</li> <li>tests/ - Pytest test suite</li> <li>docs/ - MkDocs documentation</li> <li>appChat.py, appWebcast.py, appVideocast.py - Gradio applications</li> </ul>"},{"location":"Architecture/Overview/#key-architectural-concepts","title":"Key Architectural Concepts","text":"<p>Markdown Processing: - Markdown documents are parsed into hierarchical trees based on heading levels - Metadata blocks (YAML) can annotate sections (before headings) or individual blocks - Text is chunked for embedding, but whole sections can optionally be retrieved - The <code>lmm</code> package (dependency) provides markdown parsing and tree utilities</p> <p>Vector Database (Qdrant): - Multiple encoding strategies: <code>dense</code>, <code>sparse</code>, <code>multivector</code>, <code>hybrid_dense</code>, <code>hybrid_multivector</code>, <code>UUID</code> - \"Annotations\" are metadata properties used for encoding (e.g., <code>questions</code>, <code>summaries</code>, <code>titles</code>) - Companion collections enable retrieving whole document sections instead of chunks - Collections are configured in <code>config.toml</code> under <code>[database]</code></p> <p>Workflows: - <code>workflow</code>: Simple sequential workflow (fast, economical, recommended default) - <code>agent</code>: Sophisticated agentic workflow with autonomous query rewriting (slower, more capable) - Workflows use LangGraph with dependency injection and state management - Stream adapters provide functionality like logging, content validation, and context streaming</p> <p>Configuration: - <code>config.toml</code>: System-wide settings (models, embeddings, storage, RAG parameters) - <code>appchat.toml</code>: Chat application settings (prompts, validation, logging) - Settings use Pydantic validation and can be overridden programmatically - Three model tiers: <code>major</code> (user-facing), <code>minor</code> (summaries), <code>aux</code> (classification)</p>"},{"location":"Architecture/Overview/#encoding-models","title":"Encoding Models","text":"<p>The encoding model determines how text and annotations are embedded in the vector database. Configured in <code>config.toml</code>:</p> <pre><code>[RAG]\nsummaries = true          # Generate summaries with LLM\nquestions = true          # Generate questions with LLM\ntitles = true             # Use titles in encoding\n\n[RAG.encoding]\nencoding = \"hybrid_dense\"  # Options: dense, sparse, multivector, hybrid_dense, hybrid_multivector\n\n[RAG.annotation_model]\ninherited_properties = [\"title\"]  # Search up ancestor tree\nown_properties = [\"questions\"]    # Only at current node\n</code></pre>"},{"location":"Architecture/Overview/#companion-collections","title":"Companion Collections","text":"<p>When <code>companion_collection</code> is set in config, whole document sections are stored separately from chunks: - Chunks provide embeddings for retrieval - Retrieved sections are coherent, complete text under a heading - Enable in config: <code>retrieve_companion_docs = true</code> - Control retrieval count: <code>max_companion_docs = 5</code></p>"},{"location":"Architecture/Overview/#error-handling-philosophy","title":"Error Handling Philosophy","text":"<p>The codebase uses a custom <code>Logger</code> class (from <code>lmm</code> package) instead of traditional exception handling:</p> <ol> <li>Coding errors: Crash with stack trace (type-checked to prevent)</li> <li>Validation errors: Pydantic validation crashes immediately with informative messages</li> <li>Expected errors: Handled via Logger passed as last function parameter</li> <li>Returns <code>None</code> or empty results instead of raising exceptions</li> <li>Type checker enforces checking return values</li> <li>Logger implementations: console trace, error collection, exception throwing</li> </ol> <p>Functions using this pattern are recognizable by their <code>logger</code> parameter.</p>"},{"location":"Architecture/Overview/#testing","title":"Testing","text":"<ul> <li>Tests use pytest with strict type checking</li> <li>Integration tests in <code>test_integration.py</code> and <code>test_integration_ingestion_query.py</code></li> <li>Mock objects in <code>test_mocks.py</code> for testing without real LLMs/databases</li> <li>Many tests verify configuration, vector store operations, and workflow streams</li> <li>Tests prefixed with <code>xtest_</code> are disabled (e.g., <code>xtest_vector_store_qdrant_nointernet.py</code>)</li> </ul>"},{"location":"Architecture/Overview/#important-notes","title":"Important Notes","text":""},{"location":"Architecture/Overview/#language-models","title":"Language Models","text":"<ul> <li>Supports multiple providers via LangChain: OpenAI, Anthropic, Mistral, etc.</li> <li>Format: <code>Provider/model-name</code> (e.g., <code>OpenAI/gpt-4o-mini</code>)</li> <li>Special provider: <code>Debug/debug</code> for testing without real LLM calls</li> <li>Temperature and other parameters configurable in config.toml</li> </ul>"},{"location":"Architecture/Overview/#embeddings","title":"Embeddings","text":"<ul> <li>Dense embeddings: <code>OpenAI/text-embedding-3-small</code>, <code>SentenceTransformers/all-MiniLM-L6-v2</code></li> <li>Sparse embeddings: <code>Qdrant/bm25</code> (multilingual), <code>prithivida/Splade_PP_en_v1</code></li> <li>Cannot change embeddings after database creation without re-ingesting</li> </ul>"},{"location":"Architecture/Overview/#storage","title":"Storage","text":"<ul> <li>Local: Specify folder path in <code>config.toml</code> <code>[storage]</code></li> <li>Remote: Qdrant server address and port</li> <li>Memory: <code>:memory:</code> for testing</li> </ul>"},{"location":"Architecture/Overview/#content-validation","title":"Content Validation","text":"<p>Two levels of content validation for chat applications: 1. System prompt instructions (always active) 2. LLM-based classification of response content (enable with <code>check_response = true</code>)</p>"},{"location":"Architecture/Overview/#dependencies","title":"Dependencies","text":"<ul> <li>Depends on <code>lmm</code> package (private GitLab repository)</li> <li><code>lmm</code> provides markdown parsing, tree utilities, scanning, and base configuration</li> <li>See <code>.venv/src/lmm/</code> for lmm package source (installed in editable mode)</li> </ul>"},{"location":"Architecture/Overview/#common-patterns","title":"Common Patterns","text":""},{"location":"Architecture/Overview/#initializing-configuration","title":"Initializing Configuration","text":"<pre><code>from lmm_education.config.config import ConfigSettings\n\n# Load from config.toml\nconfig = ConfigSettings()\n\n# Override specific settings\nconfig = ConfigSettings(\n    RAG={'questions': True, 'summaries': False},\n    embeddings={'dense_model': 'OpenAI/text-embedding-3-large'}\n)\n</code></pre>"},{"location":"Architecture/Overview/#working-with-qdrant","title":"Working with Qdrant","text":"<pre><code>from lmm_education.stores.vector_store_qdrant_context import global_client_from_config\n\n# Always use global_client_from_config (handles connection lifecycle)\nclient = global_client_from_config(config)\n\n# Client is automatically closed when no longer needed\n</code></pre>"},{"location":"Architecture/Overview/#processing-markdown","title":"Processing Markdown","text":"<pre><code>from lmm_education.ingest import markdown_upload\nfrom lmm_education.config.config import ConfigSettings\n\nconfig = ConfigSettings()\nmarkdown_upload(\n    \"MyLecture.md\",\n    config_opts=config,\n    save_files=True,  # Save intermediate files\n    ingest=True,      # Upload to database\n    logger=logger\n)\n</code></pre>"},{"location":"Architecture/Overview/#type-checking","title":"Type Checking","text":"<p>The codebase uses strict type checking. When working with Pydantic models, type checkers may flag dictionary-based initialization, but this is intentional and validated at runtime.</p>"},{"location":"Handbook/ChatApp/","title":"Configuring and starting the chat app","text":"<p>To start the chat app, open a console in the folder where LM Markdown for Education was installed, and activate the Python environment (when using poetry, this may be done with <code>poetry shell</code> of you have installed the shell plugin (see Installation)). Then start the app:</p> <pre><code>appChat\n</code></pre> <p>The app will print on the console a list of messages like the following</p> <pre><code>appchat.toml created in app folder, change as appropriate\nINFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n* Running on local URL:  http://127.0.0.1:7860\nINFO - HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\nINFO - HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n* To create a public link, set `share=True` in `launch()`.\nINFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\nINFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n</code></pre> <p>If everything is working, you have the app running locally at http://127.0.0.1:7860/. You can close the app by pressing Ctrl-C, and look at appchat.toml file that the app has created in the app folder. This file contains a template to customize the app, initialized to default values.</p> <p>There are three types of settings that you will probably want to change: the title and description that appear on the website, the system prompt or perhaps also other prompts, and the configuration of the server.</p>"},{"location":"Handbook/ChatApp/#title-and-description","title":"Title and description","text":"<p>The title and description should be changed to reflect the course you are teaching. The defaults refer to a statistics course. Replace them with new textual values, not forgetting to include them within the quotes (\").</p> <pre><code>title = \"VU Study Assistant\"\ndescription = \"\\nStudy assistant chatbot for VU Specific Scientific Methods \\nin Psychology: Data analysis with linear models in R. \\nAsk a question about the course, and the assistant will provide a \\nresponse based on it. \\nExample: \\\"How can I fit a model with kid_score as outcome and mom_iq as predictor?\\\" \\n\"\n</code></pre>"},{"location":"Handbook/ChatApp/#prompts","title":"Prompts","text":"<p>You will probably also want to change the system prompt. The other messages that you see in this part of the configuration file may not need to be changed, unless you want to offer the app in a language other than English.</p> <pre><code>SYSTEM_MESSAGE = \"\\nYou are a university tutor teaching undergraduates in a statistics course \\nthat uses R to fit models, explaining background and guiding understanding. \\nPlease assist students by responding to their QUERY by using the provided CONTEXT.\\nIf the CONTEXT does not provide information for your answer, integrate the CONTEXT\\nonly for the use and syntax of R. Otherwise, reply that you do not have information \\nto answer the query.\\n\"\n</code></pre>"},{"location":"Handbook/ChatApp/#server-settings","title":"Server settings","text":"<p>The server settings are in the server section:</p> <pre><code>[server]\nmode = \"local\"\nport = 61543\nhost = \"localhost\"\nauth_user = \"\"\nauth_pass = \"\"\n</code></pre> <p>If you want to offer the server over the internet, change <code>local</code> into <code>remote</code>. You can also change the port to which the app is listening. The host parameter may be set to your internet address. To enable authentication for the chat interface, specify a username in <code>auth_user</code> and a password in <code>auth_pass</code>. If left empty, the application will start without authentication.</p>"},{"location":"Handbook/ChatApp/#instructing-the-server-to-check-the-chat-content","title":"Instructing the server to check the chat content","text":"<p>You can instruct the language model to check that the chat is taking place within the topic of the course. LM Markdown for Education uses a language model to classify the response of the model before releasing it to the chat. In the following, the chat is configured to be limited to statistics and software programming.</p> <pre><code>[check_response]\ncheck_response = true\nallowed_content = ['statistics', 'software programming']\n</code></pre> <p>Put your topics within the square brackets. For example, if you have only one topic, code <code>allowed_content = ['statistics']</code>. To switch off content checking, set <code>check_response = false</code>.</p>"},{"location":"Handbook/ChatApp/#other-settings","title":"Other settings","text":"<p>If you have not used LM Markdown for Education before starting the app, it will also have written a second configuration file with default settings, config.toml. This file contains settings that are common to the app and the rest of LM Markdown for Education (for example, when interacting with it through the CLI). You may want to configure this file too (see configuration for details).</p>"},{"location":"Handbook/Configuration/","title":"Configuration","text":"<p>The files <code>config.toml</code> and <code>appchat.toml</code> in the directory of the project contains the instructions to configure its function. Essentially all user-configurable options are specified in these files.</p>"},{"location":"Handbook/Configuration/#configuration-file-configtoml","title":"Configuration file config.toml","text":""},{"location":"Handbook/Configuration/#language-models","title":"Language models","text":"<p>The language models used in LM Markdown for Education are specified through the sections <code>major</code>, <code>minor</code>, and <code>aux</code>. </p> <pre><code>[major]\nmodel = \"OpenAI/gpt-4.1-mini\"\n\n[minor]\nmodel = \"OpenAI/gpt-4.1-nano\"\n\n[aux]\nmodel = \"OpenAI/gpt-4.1-nano\"\n</code></pre> <p>These sections constitute three tiers of models, grading cost and latency depending on the task. </p> model use (examples) major in direct interactions with the end user minor to form summaries, generate annotations aux to classify text, tasks with short latency requirements <p>The language models are specified by separating the model provider and the model name by '/':</p> <pre><code>OpenAI/gpt-4.1-mini\nOpenAI/gpt-4.1-nano\nMistral/mistral-small-latest\n</code></pre> <p>It is also possible to specify other parameters of the model, such as temperature. Some parameters are model-specific, and need to be set following the specific instructions of the model vendor.</p> <p>A special language model specification is Debug/debug, which avoids using a real language model.</p>"},{"location":"Handbook/Configuration/#embeddings","title":"Embeddings","text":"<p>The embedding model is specified in the section <code>embeddings</code>. </p> <pre><code>[embeddings]\ndense_model = \"SentenceTransformers/distiluse-base-multilingual-cased-v2\"\nsparse_model = \"Qdrant/bm25\"\n</code></pre> <p>Embeddings are used to store and retrieve text from vector databases. The embeddings that are used by the project are dense embeddings (classically used by RAG applications) and sparse embeddings.</p> <p>Specification of dense embeddings contain an embedding provider and a model name, separated by '/':</p> <pre><code>OpenAI/text-embedding-3-small\nSentenceTransformers/all-MiniLM-L6-v2\nSentenceTransformers/distiluse-base-multilingual-cased-v1\"\n</code></pre> <p>Two sparse embeddings are supported at present.</p> <pre><code>Qdrant/bm25\nprithivida/Splade_PP_en_v1\n</code></pre> <p>The default sparse embedding is Qdrant/bm25 as it is a multilingual embeddings. distiluse-base-multilingual-cased-v1 is a multilingual open-source embedding model.</p>"},{"location":"Handbook/Configuration/#storage","title":"Storage","text":"<p>This section configures where the vector database is located. Specify a folder to save the vector database locally, or a valid internet address/port number for a Qdrant server.</p> <pre><code>[storage]\nfolder = \"./storage\"\n\n[database]\ncollection_name = \"chunks\"\ncompanion_collection = \"documents\"\n</code></pre> <p>The <code>database</code> section contains the names of the collections used to store the data. These names can be left as they are. However, the existence of a non-empty <code>companion_collection</code> property means that whole text sections, not the chunks used to generate embeddings, will be stored in the database (more details on this are in RAG authoring). To switch retrieval mode, use the setting <code>retrieve_companion_docs</code> in the <code>[RAG]</code> section of config.toml.   </p>"},{"location":"Handbook/Configuration/#rag-section-annotation-model","title":"RAG section: annotation model","text":"<p>This section is used to activate a language model to analyze the content of the markdown file and produce additional content. This content may either be a summary of the text under the heading (option <code>summaries</code>), or metadata describing the text that will be using to encode the semantic content (\"annotations\"). These metadata properties can also be filled in or changed manually. At present, you can specify <code>titles</code> and <code>questions</code> as metadata for endoding.</p> <pre><code>[RAG]\ntitles = true\nquestions = false\nsummaries = false\nencoding_model = \"content\"\nretrieve_companion_docs = true\nmax_companion_docs = 2\n\n[RAG.annotation_model]\ninherited_properties = []\nown_properties = []\nfilters = []\n</code></pre> <p>The [RAG.annotation_model] section allows specifying user-defined metadata properties to be used in the encoding. The annotation model is specified as a list of keys under <code>inherited_properties</code> and <code>own_properties</code>. These two types of specifications refer to how annotations are sought. When specified as inherited, the property is sought in the node and in all its ancenstor, until it is found (if present anywhere in the ancestor tree). When specified as own, only the node is sought.</p> <p>When one specifies a predefined annotation, for example <code>questions = true</code>, this metadata property will be automatically added to the annotation model -- there is no need to repeat it in the RAG.annotation_model section.</p> <p>'encoding_model' specifies how annotations are used to generate an embedding. The options here are described in detail in the encoding and embedding part of the manual.</p> <p>The specification <code>filters</code> in the annotation model is reserved for future use. Setting this specification has no effect on the working of the program at present.</p> <p>The specification <code>retrieve_companion_docs</code> retrieves whole documents instead of chunks. For this directive to have effect, a companion collection must be specified at ingestion. The setting <code>max_companion_docs</code> controls how many documents are retrieved.</p>"},{"location":"Handbook/Configuration/#text-splitting","title":"Text splitting","text":"<p>Choose here the text splitter to generate the chunks of the embeddings and their size. Default options are generally ok.</p> <pre><code>[text_splitter]\nsplitter = \"default\"\nchunk_size = 1000\nchunk_overlap = 200\n</code></pre>"},{"location":"Handbook/Configuration/#application-servers","title":"Application servers","text":"<p>The application servers start listening for connection at the specified port. They may be configured using their specific settings. The system settings of appchat.toml apply to all application servers.</p>"},{"location":"Handbook/Configuration/#appchat","title":"appChat","text":"<p>This configuration file controls the text that appears on the web application, the type of content that is allowed in the chat, and the port where the server is listening.</p>"},{"location":"Handbook/Configuration/#server-configuration","title":"Server configuration","text":"<p>The server section allows specifying how the web server will be running. The <code>mode = local</code> is useful for testing and debugging, running a local server. The settings <code>mode = remote</code> sets up a web server listening at the port <code>port</code>.</p> <pre><code>[server]\nmode = \"local\"\nport = 61543\nhost = \"localhost\"\nauth_user = \"\"\nauth_pass = \"\"\n</code></pre> <p>To restrict access, set <code>auth_user</code> and <code>auth_pass</code> to your desired username and password credentials. When left empty, the application skips authentication.</p>"},{"location":"Handbook/Configuration/#text-appearing-in-the-web-application","title":"Text appearing in the web application","text":"<p>The following settings control the text displayed in the web app.</p> <ul> <li> <p><code>title</code>, <code>description</code>, <code>comment</code>: the name of the lecture and descriptive information. See the documentation of the chat application for a more detailed description of the text that is displayed on the web page.</p> </li> <li> <p><code>MSG_EMPTY_QUERY</code>, <code>MSG_WRONG_CONTENT</code>, <code>MSG_LONG_QUERY</code>, <code>MSG_ERROR_QUERY</code>: text of the responses displayed in several user error conditions. If the English language is acceptable, these settings can be left as they are.</p> </li> <li> <p><code>SYSTEM_MESSAGE</code>: Customize this message to specify the personality of the chatbot and -- importantly -- to specify constraints on the content the chatbot may deliver.</p> </li> <li> <p><code>PROMPT_TEMPLATE</code>: used in a simple chat workflow to prompt the model with the query and the context from the vector database.</p> </li> </ul>"},{"location":"Handbook/Configuration/#chat-settings","title":"Chat settings","text":"<ul> <li> <p><code>max_query_word_count</code>: the maximal number of words contained in a query. Larger query texts will be rejected. The application may use  MSG_LONG_QUERY as a response to the user.</p> </li> <li> <p><code>history_integration</code>: the approach used to integrate past chat history when retrieving context. Possible values: 'none', 'summary', 'content_extraction', and 'rewrite'. 'none' attaches a fixed number of past messages in the exchange. It is fast and the most economical choice, but tends to reproduce the original context in every new chat turn. The most performant options are 'content_extraction' and 'rewrite'.</p> </li> <li> <p><code>history_length</code>: the number of past messages sent to the language model when chatting.</p> </li> <li> <p><code>workflow</code>: the agent used for chatting. Possible options are 'workflow' and 'agent'. 'workflow' provides a sequential series of steps to process the query. This is the most efficient option when chatting, while delivering good results to reasonably behaved users. 'agent' is a more sophisticated strategy that autonomously rewrites the query and retrieves context from the vector database. It can respond meaningfully in a number of situations when 'workflow' performs poorly, but is slower and more expensive.</p> </li> </ul>"},{"location":"Handbook/Configuration/#content-validation","title":"Content validation","text":"<p>Content validation can be implemented at two levels. The first is the prompt, instructing the language model only to reply to queries about a certain content, or that are represented in the retrieved context. The second is to send query and the first part of the response to a LMM to classify the content.</p> <p>The default chat prompt includes text for the first level of validation. To switch on the second, you set the <code>check_response</code> property in config.toml to true:</p> <pre><code>[check_response]\ncheck_response = true\nallowed_content = [\"statistics\", \"R programming\"]\ninitial_buffer_size = 320\n</code></pre> <p>If you do that, you must also provide one or more allowed contents in the <code>allowed_content</code> list. The <code>initial_buffer_size</code> controls how much of the initial response of the model is sent for content classification (in characters). If the chatbot is replying that it cannot respond to legitimate queries due to misclassification, try and increase the buffer size. There is a tredeoff between the accuracy of content assessment and the latency with which the response starts streaming in the chatbot.</p> <p>In summary, the any strategies to control content can be activie at two levels:</p> <ul> <li>introduce a content limitation in the system prompt, instead of leaving to something generic like \"You are a helpful assistant\". The system prompt is present in all chat interactions, irrespective of how prolonged.</li> <li>switch on content validation, which checks each response of the language model.</li> </ul>"},{"location":"Handbook/Configuration/#exchange-database-logging","title":"Exchange database logging","text":"<p>The exchanged messages and the retrieved context are saved to a database as specified in the <code>chat_database</code>setting. </p> <pre><code>[chat_database]\nmessages_database_file = \"messages.csv\"\ncontext_database_file = \"context.csv\"\n</code></pre> <p>The application uses .csv files to store the exchanged messages and the retrieved context. The following fields are used:</p> <ul> <li> <p><code>messages_database_file</code>: the file saving queries and responses.</p> </li> <li> <p><code>context_database_file</code>: saves the retrieved context.</p> </li> </ul>"},{"location":"Handbook/Configuration/#appwebcast","title":"appWebcast","text":"<p>This application displays videos and a chat over the content of the video. The same settings as in appChat apply. In addition, the following settings may be set here.</p> <ul> <li> <p><code>SOURCE_DIR</code>: the folder containing video/audio/image files for presentations.</p> </li> <li> <p><code>OPENAI_VOICE</code>: the OpenAI voice selected for rendering. Defaults to 'nova'.</p> </li> <li> <p><code>OPENAI_VOICE_INSTRUCTION</code>: the prompt for the OpenAI voice. </p> </li> </ul>"},{"location":"Handbook/EncodingEmbedding/","title":"Encoding and embeddings","text":"<p>The basis of retrieval-augmented generation (RAG) is the selection of textual material from a database. This material is given to the large language model to provide an answer to the user's query. The retrieval of correct material is therefore crucial for the performance of the RAG system. The material of a RAG system is stored in a database which is accessed during the generation of responses.</p> <p>Vector database are special because they allow retrieving data based on a search query that contains natural text, such as a question (semantic search). This differs from standard relational databases, where the search is based on matching the content of a field or searching part of text for keywords. The advantage of semantic search, relative to keyword search (such as the one used to search the web) is the capacity to disambiguate a word using its context. For example, the word 'black' has very different meanings in \"dressed in black\" and \"black friday\". Large language model provide representations of the meaning of whole sentences (embeddings), which contain the context required for this disambiguation. But another big advantage of semantic search is that the query is given by the question directly: there is no need to transform it into a set of keywords or into the specialized format of relational database queries. An embedding is created for the question itself using the same procedure for creating embeddings of text. The text in the database is then retrieved with the embeddings that are most similar to the embedding of the question.</p> <p>Initial typical implementations of RAG made extensive use of semantic search and vector databases. The text is chunked into pieces, and these pieces are inserted into the database together with their embeddings. There are two issues with using semantic search directly to feed the language model, however. First, while the chunks may be retrieved correctly, they may lack the overall context. Chunk-based retrieval has a tendency to provide disorganized fragments of text. The second is simply failure to retrieve relevant fragments.</p> <p>There are two ways to address these shortcomings.</p> <ol> <li>Instead of retrieving the chunks, the system may retrieve the surrounding text as well, or the paragraphs, or the chapters of text, where the chunks are located. LM Markdown for Education adopts the approach of optionally retrieving whole chapters, which in markdown are defined by headings, instead of chunks.</li> <li>Instead of creating embeddings of the chunk content, the text is preliminary processed to extract information that is relevant for retrieval: what we call here an encoding. This information may then be used for embeddings or in a traditional keyword-based search. We refer here to the specification of a strategy to extract or define this information as to the encoding model. More generally, the encoding model may refer to the content of the chunks or of any property extracted from text, or any mixture of those. The original text is the content of the chunk, while other properties are referred to as metadata or annotations.</li> </ol> <p>There are two types of embeddings: dense and sparse. Dense embeddings are provided by language models or other models trained from large corpora of text in the same way. Sparse embeddings are more similar to keywords, but are capable to match semantically related words even if the words are not identical. Depending on the kind of encoding, either or both may be used to represent content in the database. Here, we refer to the combination chosen to represent an encoding as the embedding model: dense, sparse, or a combination of both.</p> <p>While the encoding model specifies how and what parts of the text are used to represent meaning, the embedding model specifies if the representation consists of dense or of sparse embeddings. In general, dense embeddings are used for the chunks of text that represent the text stored in the RAG database or from complex textual annotations (for example, summaries). In constrast, sparse embeddings, and possibly dense embeddings, are usually formed from keyword-like annotations. Both types of annotations enrich the text so as to increase the efficiency of its retrieval. One common strategy is to couple text with the questions that the text answers, or other metadata such as keywords, the title of the heading, etc. These data can then be combined with the text of the dense embeddings, or be kept separate for an additional embedding, usually of the sparse variety.</p> <p>Most often, annotations are produced by a language model prior to ingestion, but the framework allows one to add or review them manually.</p>"},{"location":"Handbook/EncodingEmbedding/#preparing-text-for-ingestion","title":"Preparing text for ingestion","text":"<p>The encoding model determines how the text is prepared for ingestion:</p> <p>annotation model          --&gt;   encoding model  --&gt;   embedding model (and its implementation)</p> <p>Note that the implementation of the annotation model is not a monolithic step, as it depends on whether the annotations should be created by code, a language model, or manually. When the annotations are created by code or a language model, they are automatically added to the annotation model. However, it is also possible to annotate parts of text manually and add the relevant information to the annotation model. In short, annotations may be those that are added automatically when code or language models are directed to process text, or those that are added by explictly listing them in the annotation model.</p> <p>Annotations are written in the markdown text in metadata blocks, i.e. special markdown blocks that refer to the text block that follows. When they precede a heading, they refer to all text blocks under that heading, unless overridden by metadata preceding sub-headings. Therefore, the process to which annotiation contribute to the semantic encoding of text is transparent. Properties in metadata are not automatically considered annotations; they must be added to the annotation model.</p>"},{"location":"Handbook/EncodingEmbedding/#language-model-directives-or-code-that-create-annotations","title":"Language model directives or code that create annotations","text":"<p>Markdown files may be reviewed by a large language model, and part of this review is the creation of annotations. When annotations are produced in this way, they are automatically added to the annotation model.</p> <p>The following directives create annotations: <code>questions = True</code>, <code>titles = True</code>. The directive <code>summaries = True</code> creates additional text, not annotations. These directives are added to the [RAG] section of config.toml:</p> <pre><code>[RAG]\nquestions = True\ntitles = True\n</code></pre> <p>This has the following effect on the markdown:</p> <pre><code>---\n~txthash: S/ec1NIX0xV3NhFFuHY6bQ\ntitles: Chapter 1 - What are linear models?\nquestions: What are the two main ways linear models are used in practice? - How do linear models relate predictors to an outcome variable? - Why is understanding the output of linear models important for their correct application?  - How do generalizations of linear models enable capturing more complex associations? - In what way does the purpose of a linear model (prediction vs. inference) affect the consideration of confounding factors?\n---\n## What are linear models?\n\nLinear models and their generalizations constitute the majority of the statistical models used in practice. Here, we will look at linear models from a practical perspective, emphasizing the issues in correctly applying them and in understanding their output.\n\n(further text omitted) ....\n\n</code></pre> <p>As the example shows, setting the <code>questions</code> and the <code>titles</code> in the configuration file has the effect of creating headiers before all headings in a markdown files with two metadata properties containing the annotations (the example only shows one of such headings). The metadata block may contain any other property, but some property keys (such as <code>questions</code>, <code>titles</code> and <code>summary</code>) are reserved for LM Markdown for Education. Note also the <code>~txthash</code> property. This is an automatically generated hash that checks that the text has not changed since the last time annotations were computed. Annotations are not recomputed if the text to which they refer was not changed.</p> <p>When an encoding model is specified in the <code>[RAG]</code> section of the configuration file, the annotations are automatically generated with a language model prior to ingesting the text in the vector database. If a metadata property containing an annotation is removed, or the whole metdata block is deleted, it will be recreated prior to ingesting.</p>"},{"location":"Handbook/EncodingEmbedding/#manual-annotations","title":"Manual annotations","text":"<p>Manual annotations may be added, if desired, directly in the markdown file by the human RAG author. An annotation is simply a metadata property. To tell the framework that this property is an annotation, you can specify it in config.toml.</p> <p>For example, here you introduce a manual annotation called 'topic':</p> <pre><code>---\ntopic: 'review'\n---\n\nWe provide here a reviwe of logistic regression... (further text omitted)\n\n</code></pre> <p>Then, in config.toml:</p> <pre><code>[RAG.annotation_model]\nown_properties = [\"topic\"]\n</code></pre> <p>Manual annotations can be \"own\" or \"inherited\". Inherited annotations are taken from ancestor nodes, i.e. headings higher in the hierarchy.</p> <p>All annotations that are included in the annotation model are concatenated together prior to be embedded.</p>"},{"location":"Handbook/EncodingEmbedding/#reviewing-annotations","title":"Reviewing annotations","text":"<p>LM Markdown for Education is designed to allow the RAG author to review all annotations manually prior to ingesting the documents. In the command window, start the LM markdown terminal (this may take some time and require an internet connection, depending on the embedding library in use):</p> <pre><code>lmme terminal\n</code></pre> <p>This command prints some information on the emebdding configuration of the system and prepares a prompt waiting for commands. These commands work on a file (generally, the file will be open in a markdown editor to view the effects of commands).</p> <p>To direct the system to create a specific annotation, use the <code>scan_rag</code> command with the annotation that you need to produce. For example, to produce questions for the my_markdown.md document, type</p> <pre><code>&gt; scan_rag --questions my_markdown.md\n</code></pre> <p>and look in the editor for its effects (type <code>scan_rag --help</code> to get an overview of all subcommands). You can edit the question generated by the language model by typing directly in the markdown editor.</p> <p>While the <code>ingest</code> command is used to send the file to the vector database, it may also be used to create the annotations specified by the annotation model without ingesting the document. To this end, use it as follows:</p> <pre><code>&gt; ingest --save_files=true --skip_ingest=true my_markdown.md\n</code></pre> <p>This will prepare the markdown for ingestion without ingesting it, and save it instead to disk for review and editing.</p> <p>As mentioned, the annotations may be edited manually prior to ingestion. At the time of ingestion, annotations will not be recomputed if the text to which they refer does not change. However, they will after any change to the text to which they refer (that is, under the heading that the metadata block precedes). To prevent annotations from being recomputed when the text changes, add the property <code>frozen</code> to the metadata:</p> <pre><code>---\n~txthash: S/ec1NIX0xV3NhFFuHY6bQ\ntitles: Chapter 1 - What are linear model\nquestions: What are the two main ways linear models are used in practice? - How do linear models relate predictors to an outcome variable? - Why is understanding the output of linear models important for their correct application?  - How do generalizations of linear models enable capturing more complex associations? - In what way does the purpose of a linear model (prediction vs. inference) affect the consideration of confounding factors?\nfrozen = True\n---\n\nChanged text...\n\n</code></pre>"},{"location":"Handbook/EncodingEmbedding/#encoding-model","title":"Encoding model","text":"<p>The encoding model may be specified in the RAG section of config.toml:</p> <pre><code>[RAG]\nencoding_model = \"content\"\n</code></pre> <p>The possible values of the encoding model are the following.</p> encoding model description none no encoding (chunks identified by their UUID) content encode only textual content in dense vector merged encode textual content merged with metadata in the dense vector multivector encode content and annotations using multiple dense vectors sparse sparse encoding of annotations only (textual content ignored) sparse_content sparse encoding of annotations, dense encoding of content sparse_merged sparse encoding of annotations, dense encoding ofmerged content and annotations sparse_multivector sparse encoding of annotations, content and annotations embedded using multivectors"},{"location":"Handbook/Installation/","title":"Installation and quickstart","text":"<p>LM Markdown for Education is still in the early development phase. For this reason, the installation follows the steps you would take for installing a coding project.</p>"},{"location":"Handbook/Installation/#start-a-new-project","title":"Start a new project","text":"<p>To start a new project, plan a new folder (for those who use git, they might want to create a new repository). This folder will contain the local copy of the LM Markdown for Education software and your source material.</p> <p>The first step is to install the LM Markdown for Education software. You need the following software installed on your computer:</p> <ul> <li>Python, at best in version 3.13, but not older than 3.12. LM Markdown for Education will not work on newer versions of Python such as 3.14.</li> <li>poetry. See https://python-poetry.org/docs/#installing-with-the-official-installer for instructions. The following installation instructions were followed with poetry version 2.3.2.</li> <li>git. See https://git-scm.com/install/.</li> </ul> <p>Navigate with your terminal in the folder that will contain the new folder, then clone the code from git:</p> <pre><code>git clone https://github.com/roberto-viviani/lmm-education.git\n</code></pre> <p>Git will create the folder with the code inside. Go into this folder, and install the software using Poetry:</p> <pre><code>cd lmm-education\npoetry install\n</code></pre> <p>Poetry will check that you have a Python version compatible with the project. If Poetry complains that it cannot find a compatible version, install Python 3.13 manually or with Poetry:</p> <pre><code>poetry python install 3.13\n</code></pre> <p>It commonly happens that <code>poetry install</code> requires you to run <code>poetry lock</code>. You may get an error message like this,</p> <pre><code>$ poetry install\nThe currently activated Python version 3.11.4 is not supported by the project (&gt;\n=3.12.0,&lt;3.14.0).\nTrying to find and use a compatible version.\nUsing python.exe (3.13.12)\nCreating virtualenv lmm-education in D:\\Scratch_Roberto\\R\\lmm-education\\.venv\nInstalling dependencies from lock file\n\npyproject.toml changed significantly since poetry.lock was last generated. Run `\npoetry lock` to fix the lock file.\n</code></pre> <p>This does not mean that there is any fundamental problem with the installation, but merely that the dependencies have to be regenerated. Repeat the install after giving the <code>lock</code> command:</p> <pre><code>poetry lock\npoetry install\n</code></pre> <p>As in all Python project, you now have to activate the environment. I would recommend it to do it with Poetry's shell plugin, which may be installed with this line:</p> <pre><code>poetry self add poetry-plugin-shell\n</code></pre> <p>After that, activate the environment:</p> <pre><code>poetry shell\n</code></pre> <p>Alternatively, use the command <code>poetry env activate</code> to discover how to do that (alas, this tends to be a bit platform specific).</p> <p>Now you can start using the application. You can check that everything is running by operating the CLI:</p> <pre><code>lmme --help\n</code></pre> <p>which gives you an overview of available commands. This command also writes default configuration files to the folder which you can later modify (see below).</p> <p>Note:     The first time you use <code>lmme</code>, it will take quite some time for the application to start. This is because the application is prepared before starting. Subsequent activations are quicker. You can also enter a dedicated terminal to talk to the application by starting <code>lmme terminal</code>.</p>"},{"location":"Handbook/Installation/#problems-with-activating-the-environment","title":"Problems with activating the environment","text":"<p>When Poetry uses a Python version that differs from the ones used by the project, it may happen that by giving the command <code>poetry env activate</code> the environment activator is damaged. If you have problems activating the environment, do it through Poetry's shell plugin. Install it with <code>poetry self add poetry-plugin-shell</code>. Then use <code>poetry shell</code> to activate the environment.</p>"},{"location":"Handbook/Installation/#installing-in-an-existing-project","title":"Installing in an existing project","text":"<p>You might have an existing project to which you want to add LM Markdown for Education. In this case, use Poetry to add the project. Navigate to the project folder. In a poetry-managed project, make sure that the Python section specifies the version constraints '&gt;=3.12,&lt;3.14'. Then add LM Markdown for Education:</p> <pre><code>poetry add git+https://github.com/roberto-viviani/lmm-education.git\n</code></pre> <p>If your project is not poetry-managed, run <code>poetry init</code> first and follow the prompts to give the required information. Again, make sure to specify the Python version constraints as indicated above.</p>"},{"location":"Handbook/Installation/#working-on-documents","title":"Working on documents","text":"<p>It is a good idea to set up another folder within this main folder to contain your documents, but this is not absolutely necessary. A possible setup is to use the main folder to work on an active document, and save the documents that you want to make up your chatbot in a dedicated sub-folder (for example, .sources/).</p> <p>If you already have a set of documents, copy them in this folder, and go to RAG authoring to get started.</p>"},{"location":"Handbook/Installation/#configuring-the-application","title":"Configuring the application","text":"<p>Details on how to configure the chatbot are in Configuration.</p>"},{"location":"Handbook/Installation/#starting-the-chatbot","title":"Starting the chatbot","text":"<p>Instructions for starting the chatbot are in ChatApp.</p>"},{"location":"Handbook/Markdown/","title":"Working with markdown","text":"<p>Markdown is a convenient format to create textual material that may be then converted into a large variety of formats. In LM Markdown for Education, markdown is used to harness large language model to increase the capability of the author and to produce AI-supported material for teaching, such as a system to ask questions and review the lecture content.</p> <p>Markdown may be seen to contain three general types of material: metadata, headings, and everthing else (largely including the text). A simple rule is that these markdown elements are separated by a blank line. There are exceptions to this, determined by the markdown specification, but putting a blank line between elements keeps things simple. Multiple blank lines are treated as a single blank line.</p>"},{"location":"Handbook/Markdown/#metadata","title":"Metadata","text":"<p>Metadata are very commonly used in markdown tools to create a header with information such as title, author, date, etc. This is how this may look like:</p> <pre><code>---\ntitle: Working with markdown\nauthor: John Sammelwater\ndate: 23.10.2025\n---\n\nText follows here...\n</code></pre> <p>In LM markdown, a rule is that the header should contain a title property. If you create a markdown without a header with this property, LM markdown will try and create one from the name of the file. (If this name is not available, it will create a title property with value 'title', but it is best to avoid this when the markdown is used for RAG.)</p> <p>LM markdown also differs from other markdown systems in that it uses metadata throughout the whole document. Metadata are marked at the beginning and at the end by three dashes starting at the start of the line,</p> <pre><code>---\n</code></pre> <p>These dashes delimit metadata blocks. In LM markdown, metadata blocks are interpreted as referring to whatever follows them. Thus, metadata blocks immediately preceding a heading refer to the heading and all content within it; metadata blocks preceding a text block refer and therefor annotate the text block that follows.</p>"},{"location":"Handbook/Markdown/#headings","title":"Headings","text":"<p>Headings are the markdown elements that mark chapters, giving them a title. Headings are defined by one or more hash symbols <code>#</code>, followed by a blank space, followed by the title. The number of hashes gives the level of the heading, which can be up to 6.</p> <p>LM markdown uses headings to view a markdown file as organized in a tree. The headings constitute the branches of the tree, and the leaves are text elements under the headings. Metadata preceding a heading are then used to annotate with properties the whole portion of the markdown that falls within the heading (and its subheadings). The metadata properties are inheritable. This means that the metadata properties defined in the header refer to the whole markdown file, unless overridden by the same property somewhere in the file.</p> <p>Metadata preceding a text block only refer and annotate the text block that immediately follows.</p>"},{"location":"Handbook/Markdown/#text-blocks","title":"Text blocks","text":"<p>For our purposes, text blocks is everything else. Text blocks may contain text, of course, but other common content is code, equations, links to images. All these elements are subtypes of text blocks, and form blocks whenever they are separated by blank lines. As noted, whenever you form a block by delimiting it with blank lines, you can also annotate it with a metadata block if you wish. Equation blocks start and end with <code>$$</code>. Code blocks start and end with <code>\\</code>```, optionally followed by the language in which the code is written.</p> <p>Here is an example of parts of a markdown. It starts with a header, has a heading, and two blocks of text under the heading, one being an equation.</p> <pre><code>---\ntitle: Working with markdown\nauthor: John Sammelwater\ndate: 23.10.2025\n---\n\nThis text introduces users to the markdown format.\n\n---\nnote: the following is a title at level 1\n---\n# What can markdown contain? \n\nMarkdown can contain text, equations, or code. Below, you see an equation.\n\n---\nnote: the following is an example of an equation, and this is the meatadata block that annotates it.\n---\n\n$$ y ~ \\mu + x + \\epsilon $$\n</code></pre>"},{"location":"Handbook/Markdown/#identification-of-content-in-vector-database","title":"Identification of content in vector database","text":"<p>The data from the markdown are chunked prior to ingestion in the database. Each chunk generates an embedding, so that when a query is made, the correct chunks are retrieved from the database (see the encoding page for details).</p> <p>Because the material may be changed and updated, the question arises as to how new material is integrated with the old. LM Markdown for Education uses properties from metadata to identify the chunks so that, if the text is changed, it replaces old text in the vector database.</p> <p>In the header, the property <code>docid</code> is created automatically by LM markdown to identify a document, and initialized to a random string. You can, however, initialize to an intelligible string, as long as it remains unique across documents:</p> <pre><code>---\ntitle: Working with markdown\nauthor: John Sammelwater\ndate: 23.10.2025\ndocid: workmd\n---\n</code></pre> <p>Based on this property, LM markdown adds a <code>textid</code> property to all text blocks in the document. The texdid property has the format {docid}.{sequential_number}, where docid is the value of the docid property in the header, and the second element is a sequential number.</p> <p>As long as these textid elements are present in a document, they will be re-ingested and replace the old text in the vector database. Note that the text can become longer, as new docid's will add new material in the database. If the document has not changed, ingesting the document in the database results in the very same content as it had had before. Note that embeddings are always recomputed when ingesting documents, but there will be no difference with the old embedding if the content hasn't changed. Because embeddings are recomputed, the order of the material in the markdown file can change without affecting the outcome.</p> <p>If there are parts of the document that have changed, or new parts, they replace or add the material in the vector database. (To remove parts of the database, it is at present necessary to delete the old database and ingest all documents again).</p> <p>For legibility, it is a good idea to set the docid property in the header before ingesting a document, and never change it after that. If docid is changed, LM markdown will consider the document a new separate document and will ingest duplicate material in the database.</p>"},{"location":"Handbook/Markdown/#the-syntax-used-in-markdown","title":"The syntax used in markdown","text":""},{"location":"Handbook/Markdown/#headers-and-metadata-blocks","title":"Headers and metadata blocks","text":"<p>The header is an example of what markdown defines as a metadata block. Most markdown editors use metadata blocks only for the header, so that these two concepts may appear interchangeable. They are not, however. The header is a metadata block that appears first thing in a markdown document. LM markdown uses metadata blocks throughout the document.</p> <p>Metadata blocks contain entries in YAML, a specification for encoding data in text. YAML is a very complex and rich specification, and care is needed when using it within markdown, as not all its features may be handled in the same way by the markdown editors. It is therefore a good idea to keep the syntax of your metadata blocks simple.</p> <p>Each entry in the block defines a property. It is constituted by a key and a value pair, separated by a colon <code>:</code> . You can use lists of values by including the values in square brackets and separating them with commas:</p> <pre><code>keywords: [logistic regression, binary outcome variable]\n</code></pre> <p>Simple entries are contained in one line. However, it is also possible to define lists of entries or elements on multiple lines. In this case, the lines defining the elements of the list are indented and start with <code>-</code> .</p> <pre><code># list of entries\nentry list:\n    - first entry: 1\n    - second entry: 2\n</code></pre> <p>Values can also be put on multiple lines, but this makes no difference to the previous syntax.</p> <pre><code># list of values\nvalue list:\n    - 1\n    - 2\n</code></pre> <p>This is the same as <code>value list: [1, 2]</code>.</p>"},{"location":"Handbook/Markdown/#equations","title":"Equations","text":"<p>Equations are coded in the LateX language for equations and, when within text, are enclosed by <code>$</code>:</p> <pre><code>The model equation in a call to `lm` is defined like this: $y ~ x_1 + x_2$\n</code></pre> <p>Put an equation on its own line, enclosed by <code>$$</code>, to create a stand-alone equation in the middle of the page.</p>"},{"location":"Handbook/Markdown/#code","title":"Code","text":"<p>When within text, code elements are enclosed by a backspace (see the example of lm in the previous section.</p> <p>Whole text blocks are delimited by text lines containing three backstops:</p> <pre><code>```r\nfit &lt;- lm(y ~ x1 + x2, data = mydata)\n```\n</code></pre> <p>Optionally, after the first three backstops the language of the code may be given.</p> <p>LM markdown considers equations and code like text blocks, but a markdown editor like R Studio can evaluate the code blocks and produce the results in the markdown.</p>"},{"location":"Handbook/RAGauthoring/","title":"RAG authoring","text":"<p>When creating a vector database for RAG, it is not necessary to curate the content: LM Markdown for Education will ingest all markdown documents in the input directory and add the relevant information automatically. However, it is also possible to curate the way in which the document is encoded in the database. </p> <p>To retrieve the information effectively, RAG systems use two complementary sources of information. The first is the text itself that is being ingested in the database. The second is metadata on the text (such as the questions the text is answering). Here, the metadata that are used for this purpose are called annotations. In addition, it is also often useful to ingest summaries of the text. This additional information is usually generated by the language models themselves in a first round of work on the documents.</p> <p>The process of curation allows the RAG author to be part of the loop that creates the annotations and the summaries of text. As noted, since the annotations and summaries are created automatically, this is only an optional step. Note that the kind of annotations added to the text, and whether to generate summaries, may be specified in the <code>[RAG]</code> section of the config.toml file.</p> <p>LM Markdown for Education allows the RAG author to interact with the language model at each step using a command language interface (CLI), used together with a text editor with the markdown files open. There are two CLI interfaces available. The first allows to give commands directly from bash (Linux/Max) or Powershell (Windows). This interface is most useful when one needs to send just one or two commands. Its drawback is that it starts the Python interpreter and all libraries each time that a command is given, and this takes time (especially when using a local library to interact with the language model). The second starts the Python interpreter with an internal small CLI interpreter program. This first start takes some time to run, but once it has run all other commands are executed very much more quickly.</p> <p>The first CLI interface is accessed by typing the command <code>lmme</code> from the command window, followed by the subcommands one wants to execute. The second interface is accessed by the command <code>lmme terminal</code>. This starts the CLI interpreter, and all subsequent subcommands may be given from here. </p>"},{"location":"Handbook/RAGauthoring/#step-1-preparing-content","title":"Step 1: preparing content","text":"<p>Creating a RAG application begins with gathering the documents that the application will serve. You can prepare all documents at once or add them incrementally.</p> <p>The language model can assist the RAG author in content creation by proofreading the text. By using the CLI, you may ask the model to review your content for clarity and conciseness, suggest improvements, or generate questions that verify the points your text addresses.</p> <p>The interaction with the model takes place through metadata blocks. You create a metadata block before the heading containing the text you want to proofread, insert the property <code>query:</code> in the block with the text of your request, and send the markdown to the language model for response. You will need to have a text or markdown editor open (to visualize changes), and a CLI to give commands to LM Markdown for Education. </p> <p>For example, in the following markdown, a query property was manually added to prepare a request for an evaluation of the text for clarity and conciseness:</p> <pre><code>---\nquery: evaluate the text for clarity and conciseness.\n---\n\n## What are linear models?\n\nLinear models and their generalizations constitute the majority of the statistical models used in practice. Here, we will look at linear models from a practical perspective, emphasizing the issues in correctly applying them and in understanding their output.\n\nLinear models capture the association between a set of variables, the *predictors* (also known as *independent variables*), and an *outcome* variable (or *dependent variable*). In the simplest setting, this association may be approximated and displayed by a line relating a predictor and the outcome. However, the generalizations of linear model allow capturing much more complex associations.\n\n## The generalization of linear models\n\nLinear models can be generalized to cover situations where the outcome is not continuous, and therefore linearity cannot apply. Further text omitted...\n</code></pre> <p>Here, part of the message that is sent to the language model is \"evaluate the text for clarity and conciseness\". This message part is integrated with the text to which the metadata refers, which is here the text under \"What are linear models?\", but not the text under \"The generalization of linear models\". If there had been subheadings after \"What are linear models\", the text of the subheadings would also be sent to the language model. The term 'text' here is key: the language model knows that your question refers to the parts of the markdown that are sent over, because these are qualified as \"text\" in the message.</p> <p>To send the message to the model, first make sure to have saved the file with the query property. Then open a command window, and use the following command:</p> <pre><code>lmme scan-messages Lecture01.md\n</code></pre> <p>Alternatively, after having started the internal CLI interpreter with <code>lmme terminal</code>:</p> <pre><code>&gt; scan_messages Lecture01.md\n</code></pre> <p>Finally, for those that are used to writing Python scripts, one can call the library directly from there:</p> <pre><code>from lmm_education import scan_messages\n\nscan_messages(\"Lecture01.md\")\n</code></pre> <p>where Lecture01.md is the markdown file in questions. The response of the language model appears in the editor under the property <code>~chat</code>.</p> <pre><code>---\nquery: evaluate the text for clarity and conciseness.\n~chat:\n- evaluate the text for clarity and conciseness.\n- The text is generally clear and informative, providing a good introduction to linear models and their practical application. However, it can be made more concise by reducing redundancy and simplifying some phrases without losing meaning.\n\nHere is a revised version for improved clarity and conciseness:\n\n\"Linear models and their generalizations make up most statistical models used in practice. This discussion focuses on applying linear models correctly and interpreting their output. Linear models describe the relationship between predictors (independent variables) and an outcome (dependent variable). In the simplest case, this relationship can be represented by a line relating a predictor to the outcome. More advanced linear models capture more complex associations.\"\n\nThis version removes some repetitive wording and tightens the explanation while maintaining the original intent.\n---\n\n## What are linear models?\n\nLinear models and their generalizations constitute the majority of the statistical models used in practice. Here, we will look at linear models from a practical perspective, emphasizing the issues in correctly applying them and in understanding their output.\n\nLinear models capture the association between a set of variables, the *predictors* (also known as *independent variables*), and an *outcome* variable (or *dependent variable*). In the simplest setting, this association may be approximated and displayed by a line relating a predictor and the outcome. However, the generalizations of linear model allow capturing much more complex associations.\n\n## The generalization of linear models\n\nLinear models can be generalized to cover situations where the outcome is not continuous, and therefore linearity cannot apply. Further text omitted...\n</code></pre> <p>You can use the interaction with the language model to have it criticize your text, and adopt the improvements that the model suggests if they do improve the quality of your writing. Try and change the prompt to obtain differnt responses from the model. For example, if you ask for comments, it may list its observation on the text without providing an alternative text; if you ask for a revision, it will provide a new text draft.</p> <p>If you ask the model to criticize text or suggest improvements, it will always find something to criticize and improve. Hence, you can keep improving until you think that the suggestions of the language model are superfluous.</p> <p>The text that is sent to the language model depends on the orgnization of your markdown document. LM Markdown for Education sees the file as a hierachy of chapters, depending on the level of the heading. The metadata blocks are always meant to annotate the part of the markdown that follows. A metadata block before a heading annotates all text under the heading, while a metadata block before a text block annotates only the following text block. Therefore, when interacting with the language model, the position of the block where the query is made determines what text is sent to the language model. A common approach to interact with the language model is to create provisional sub-sections to stake out the parts of the text that the language model should work on. After the interaction is completed, the sub-sections are merged back into the main text.</p> <p>You can ask new queries on the same text by editing the <code>query</code> properties, and run <code>scan_messages</code> again. Queries that have already been responded are not responded again. If you want the model to respond to the same message, delete the text of the query in the <code>~chat</code> property.</p> <p>When you are finished with interacting with the language model, you can remove the chat with the <code>scan-clear-messages</code> command:</p> <pre><code>lmme scan-clear-messages Lecture01.md\n</code></pre> <p>or, in the internal CLI interpreter started with <code>lmme terminal</code>,</p> <pre><code>&gt; scan_clear_messages Lecture01.md\n</code></pre> <p>Those preferring a Python REPL or script may use</p> <pre><code>from lmm_education import scan_clear_messages\n\nscan_clear_messages(\"Lecture01.md\")\n</code></pre> <p>It is not necessary to clear the markdown document from these messages prior to ingestion into the RAG database. Even if not removed explicitly, queries and chats are not used when ingesting the markdown in the vector database.</p>"},{"location":"Handbook/RAGauthoring/#step-2-generation-of-annotations-to-facilitate-retrieval-of-text","title":"Step 2: Generation of annotations to facilitate retrieval of text","text":"<p>Annotations are metadata properties that facilitate the retrieval of text from the vector database. Consider the following text.</p> <pre><code>### Observational studies\n\nThese are studies with models that contain predictors that were observed in the field, and could not or were not the result of an experimental manipulation of the predictor value. For example, a model of depressive symptoms in adults as predicted by childhood trauma would be such a model.\n</code></pre> <p>Here, the text that is sent to the vector database never contains the word \"observational\". Hence, the database will have a hard time figuring out that this text is relevant to answer the question \"What are observational studies?\". The only common point between the question and text here is the word \"study\", but there will be a lot of text in the database with this word. Worse, a (somewhat misformulated) question like \"What are observational models?\" has no points of contact with the text.</p> <p>Furthermore, other text in the database may contain the term \"observational study\" even if it does not explain at all what observational models are. Consider the following example.</p> <pre><code>### Somewhere in text\n\nWhen we look at the significance of the association between predictors and outcomes, it is important to distinguish between two different settings. In the first, we have variables that we have observed in the field. For example, early traumas may exert an influence on the predisposition to psychopathology in adult age. We do not have any way to change the occurrence of traumas in the past with an experiment, so we look at their consequences in a sample from the population. Studies of this type are called *observational*.\n\n### Elsewhere in text\n\nLinear models are not oracles that can divine aspects of reality: all that they see is just numbers. Therefore, an important task when estimating linear models is our capacity to relate these numbers to measurements and observations in the real world, and interpret the output of the model in the light of this knowledge. The distinction between the interpretation of the associations in observational and experimental studies is one example of this task.\n</code></pre> <p>Here, questions on \"observational studies\" may retrieve the second chunk, even if the chunk that explains what observational studies are is the first. This is because the fragment \"observational and experimental studies\" is closer semantically to a questions on observational studies than the fragment \"studies of this type are called observational.\" This example shows that a mechanism is needed to characterize the semantics of pieces of text explicitily to facilitate their retrieval.</p> <p>One thing to keep in mind is that the performance of sentence embeddings to capture semantic similarity varies across providers and embeddings size. Hence, the efficiency of retrieval may improve by adopting better sentence embeddings. However, a downside of this approach is that one embedding type must be used for the whole database, so it is not possible to improve the embedding selectively for parts of text that do not perform. To update the embedding, the whole vector database must be re-ingested anew.</p> <p>Annotations are the best mechanism to improve the capacity of the vector database to retrieve the right parts of text, because they more precisely enhence the capacity to represent specific content. Annotations constitute additional information that is used in the embedding. Annotations that may improve embedding performance are:</p> <ul> <li>the title or the concatenated title of the heading over the text</li> <li>questions that the text answers</li> <li>keywords (forthcoming)</li> </ul> <p>LM Markdown for Education uses a language model to create the annotations. However, they can always be integrated or edited in an interactive loop. What annotations should be generated is specified in the \"RAG\" section of config.toml:</p> <pre><code>[RAG]\nquestions=true\ntitles=true\nkeywords=false\n</code></pre> <p>'true' and 'false' switch the automatic generation of annotations on and off. In general, you would use the same annotation model in the whole project.</p> <p>To have LM markdown generate the annotations for inspection, i.e. without ingesting the document, use the following CLI commands:</p> <pre><code>lmme scan-rag Lecture01.md\n</code></pre> <p>or after starting the internal CLI with <code>lmme terminal</code>:</p> <pre><code>&gt; scan_rag Lecture01.md\n</code></pre> <p>or from a Python script or REPL:</p> <pre><code>from lmm_education import scan_rag\n\nscan_rag(\"Lecture01.md\")\n</code></pre> <p>After this, any modern editor that has Lecture01.md open will display the annotations added by the langauge model:</p> <pre><code>---\n~txthash: 7JdFs+GLpXTfvmONOnyc5g\ntitles: Chapter 1 - What are linear models? - Observational studies\nsource: Ch1\nquestions: What are observational studies? - How does confounding affect the interpretation of associations in observational studies?\n---\n### Observational studies\n\nWe will first discuss issues arising from assessing the significance of associations.\n\nWhen we look at the significance of the association between predictors and outcomes, it is important to distinguish between two different settings. In the first, we have variables that we have observed in the field. For example, early traumas may exert an influence on the predisposition to psychopathology in adult age. We do not have any way to change the occurrence of traumas in the past with an experiment, so we look at their consequences in a sample from the population. Studies of this type are called *observational*. An important issue in observational studies is that predictors may be *confounded* by other factors affecting the outcome. Confounding occurs when the relationship between the predictor and the outcome is distorted by the presence of a third variable. For example, traumas may occur more frequently in adverse socioeconomic conditions, and these conditions may in turn adversely affect the predisposition to psychopathology. When we assess the association between traumas and psychopathology, the association we find may inadvertently include the effects of socioeconomic conditions, as traumatized individuals are often those who are most disadvantaged socioeconomically. Importantly, the existence of an assocatio between an observational variable and an outcome cannot be drectly interpreted as providing evidence of a _causal_ relationship between the variables.\n</code></pre> <p>To edit or add questions, just delete, rewrite, or add them in the metadata block in the <code>questions</code> property.</p> <p>The first time you call <code>scan_rag</code>, the text is scanned and a special code is added in a property <code>~txthash</code>. This property is used to check what annotations should be recomputed automatically. This happens whenever the text changes. Hence, if you change an annotation manually, it will not be recomputed the next time you call <code>scan_rag</code>. If you change the text under that heading, however, the annotation will be recomputed. To prevent this, add the property <code>frozen = true</code> to the metadata block.</p> <p>You can check which headings have changed and will be recomputed from the CLI,</p> <pre><code>lmme scan-changed-titles Lecture01.md\n</code></pre> <p>or, after having started the internal CLI with <code>lmme terminal</code>:</p> <pre><code>&gt; scan_changed_titles Lecture01.md\n</code></pre> <p>Frozen metadata will not be listed here. The command will only list the titles of the headings whose metadata will be recomputed the next time that scan_rag command is given.</p>"},{"location":"Handbook/RAGauthoring/#manual-annotations","title":"Manual annotations","text":"<p>The properties <code>questions</code> and <code>keywords</code> are special, because LM Markdown for Education uses language models to fill them. However, you can also add you own property. For example, you might want to add a property <code>concepts</code> or <code>topic</code>, where you manually insert the values of the property in the metadata block:</p> <pre><code>---\n~txthash: 7JdFs+GLpXTfvmONOnyc5g\ntitles: Chapter 1 - What are linear models? - Observational studies\nsource: Ch1\nquestions: What are observational studies? - How does confounding affect the interpretation of associations in observational studies?\nconcepts: observational studies - confounders - confounding\n---\nText follows...\n</code></pre> <p>To tell LM markdown that these metadata properties are meant for annotations, include them in the <code>RAG.annotation_model</code> section of config.toml:</p> <pre><code>[RAG.annotation_model]\ninherited_properties = []\nown_properties = [concepts]\n</code></pre> <p>There are two entries here. The entry <code>inherited_property</code> means that an annotation of a heading will be propagated to all subheadings. This does not happen if the annotation is listed in <code>own_properties</code>.</p> <p>You can list more than one property as an annotation. In this case, separate the properties with a comma: <code>own_propertes = [concepts, topics]</code>.</p> <p>It is not necessary that a manual annotation, when listed in config.toml, is present in all headings of the markdown. However, if the encoding model uses annotations to compute the semantic representation of text (see next section), then some annotation must be present. Annotations are added to each other when computing the semantic representation. In the example above, the annotations used for this computations are <code>titles</code>, <code>questions</code>, and <code>concepts</code>, after adding this latter to the annotation model in config.toml. So, if <code>concepts</code> is missing, the other two annotations are still available to compute the semantic of the text.</p> <p>If a property is added to a header without listing it in config.toml, then it is not used to encode the semantics of the text. For example, you could add a <code>comments</code> property, or a <code>TODO</code> property for your own use.</p>"},{"location":"Handbook/RAGauthoring/#structuring-the-markdown-text","title":"Structuring the markdown text","text":"<p>LM markdown allows for inserting manual annotations for each text block individually, while automatic annotations are created on a per-heading basis. Keep this in mind when structuring the text. As a rule of thumb, it is better to insert sub-headings to cluster text that goes together and put the related annotation in the metadata of the subheading, rather than inserting metadata blocks for individual text blocks. However, this latter strategy allows for differentiating retrieval of individual blocks. It is not necessary to create manual annotations for that - one could insert a <code>questions</code> property in a metadata block before a text block. However, keep in mind that this annotation completely replaces the annotation of the heading.</p> <p>When responding to a query, the vector database retrieves parts of text (called chunks). How text is split into chunks depends on the text splitter used to prepare chunks (see the section on text splitting below). Therefore, the structuring of the text in text blocks and headings should be made keeping in mind what the language model may obtain when formulating a response to the question of the student. This encourages text organization styles that group content into relatively well contained blocks or headings.</p> <p>In LM Markdown for Education, you can direct the software to retrieve the whole text of a subheading where the chunk is located, instead of the chunk. This decouples the splitting of text for the purposes of establishing its semantics and relevance for the query, and the coherence of the material the language model can use to formulate the response. In this case, keep in mind that the retrieved text is that of the subheading. Text blocks can be annotated liberally as this will increase the precision of their semantic encoding, without having an impact on the retrieval of background text.</p>"},{"location":"Handbook/RAGauthoring/#encoding-models-and-embeddings","title":"Encoding models and embeddings","text":"<p>Vector databases retrieve data based on embeddings. These are vectors (ordered sets of numbers) with the property that vectors with similar numbers represent similar meanings. Therefore, vector databases are queried by providing some text (for example, the question of the user). The embedding of the query text is computed, and the chunks of text in the database are retrieved with the most similar meaning, according to the representation afforded by the embedding.</p> <p>Without annotations, embeddings are created from the text that is stored in the database. With annotations, there are a number of ways to create embeddings that better represent the content of the text.</p> encoding type description merged the content of the annotations and the content of text are concatenated prior to forming the embedding multivector two separate vectors are used as embeddings, one from the annotations and one from the text sparse annotations are treated as keywords (as in web search) and are the only ones that are encoded sparse_content annotations are embedded as keywords, and text through a vector sparse_merged annotations are both embedded as keywords and concatenated to text prior to create the vector embeddings sparse_multivector annotations are embedded as keywords, separate vector embed annotations and text content the case when there are no annotations (only text embedded as vector) <p>These options trade off accurate encoding of annotations on the one hand and cost on the other. In the merged encoding option, there is little increase in cost but more accurate representation of annotation semantics in the embeddings. The multivector and sparse_content encoding options provide separate encoding of annotations, so that they automatically receive more weight when the text is large. However, two encodings are computed and stored for each data point. the sparse_multivector is the most expensive and the option that puts most weight to the annotations relative to the text.</p> <p>You can customize the encoding model by entering the option in config.toml in the RAG section:</p> <pre><code>[RAG]\nencoding_model=sparse_merged\n</code></pre> <p>Remember that the same encoding model must be used in all interactions with the database.</p> <p>The encoding model influences how the text is encoded, but the retrieved text is not necessarily the same as the one from which the encoding was computed. This is because the retrieved text must contain context for its use in generating a response well. LM Markdown for Education can be directed to save in the database the whole text of the subheading where the chunk is located. This directive is in the <code>database</code> section:</p> <pre><code>[database]\ncollection_name = \"chunks\"\ncompanion_collection = \"documents\"\n</code></pre> <p>After storing the whole text subheadings, these text may be provided at retrieval (instead of the chunks) by setting the <code>retrieve_companion_docs</code> key in the <code>[RAG]</code> section. </p> <pre><code>[RAG]\nretrieve_companion_docs=true\n</code></pre> <p>If <code>retrieve_companion_docs</code> is set to true and the <code>companion_collection</code> is empty, then the <code>retrieve_companion_docs</code> setting is ignored.</p> <p>The following table summarizes the models used in RAG.</p> model explanation annotation model what properties of metadata, if any, are used to compute the encoding encoding model how to combine annotations and main text to create embeddings (a technical question about how to structure the database)"},{"location":"Handbook/RAGauthoring/#skipping-parts-of-the-markdown","title":"Skipping parts of the markdown","text":"<p>You can annotate text blocks or entire headings and their underlying material such as to exclude them from the RAG. To this end, use the annotation <code>skip: True</code>:</p> <pre><code>---\ntitle: chapter 1\n---\n\n# Introduction\n\nThis text is sent to the RAG database.\n\n---\nskip: true\n---\nThis text is not sent.\n\nThis text is sent again.\n\n---\nskip: true\n---\n## More introductory words\n\nEverything that lies under this heading is not included in the database.\n---\n</code></pre>"},{"location":"Handbook/RAGauthoring/#step-3-test-retrieval","title":"Step 3: test retrieval","text":"<p>When forming the database, or after adding new text, you may want that the right text is retrieved by queries. LM Markdown for Education is also designed for the database to grow with the experience of the questions asked by students and that the system did not address. You can write a new markdown document with the material to answer these new question, and add it to the database.</p> <p>When doing this, you may want to be sure that the original question or similar questions retrieve this material if asked again. This is what test retrieval is for.</p> <p>You start with ingesting the document with Python, and then you test that the material is retrieved when you ask the question:</p> <pre><code>lmme ingest AddedMaterial.md\nlmme querydb \"What is the reason to add a family parameter to glm call?\"\n</code></pre> <p>or, after having started the internal CLI with <code>lmme terminal</code>:</p> <pre><code>&gt; ingest AddedMaterial.md\n&gt; querydb \"What is the reason to add a family paramter to a glm call?\"\n</code></pre> <p>or from the Python REPL:</p> <pre><code>from lmm_education import ingest\nfrom lmm_education import querydb\n\ningest(\"AddedMaterial.md\")\nresponse = querydb(\"What is the reason to add a family parameter to a glm call?\")\nprint(response)\n</code></pre> <p>If the material is not retrieved as you expected, go back to the document and change the annotations, trying to be specific. If even this does not work, it may help changing the text to be more explicit about what the text explains (for example, by mentioning the textual fragment \"family parameter to glm call\"). </p> <p>To replace the old content in the database with the new, just repeat the ingestion process. LM Markdown for Education uses the <code>docid</code> property in the header of the documents (in the example, AddedMaterial.md) to identify this content. Hence, as long this property is not changed, the new material will replace the old. Keeping this property to the same value ensures that the edited markdown replaces the old in the database.</p> <p>After you are satisfied with the way the material is being retrieved from the database in response to your queries, you can test the response from the language model with the <code>query</code> function:</p> <pre><code>lmme query \"What are observational studies?\"\n</code></pre> <p>or, after having started the internal CLI with <code>lmme terminal</code>:</p> <pre><code>&gt; query \"What are observational studies?\"\n</code></pre> <p>Using the Python REPL:</p> <pre><code>from lmm_education import query\n\nresult = query(\"What are observational studies?\")\nprint(result)\n</code></pre> <p>The configuration file contains three model specifications: <code>major</code>, <code>minor</code>, and <code>aux</code>. By default, the end-user interactions with the language model use the major model. You can experiment with other models by specifying them as an argument to the query, as shown below. You can also specify other arguments such as temperature:</p> <pre><code>lmme query \"What are observational studies?\" --model minor --temperature 0.2\n</code></pre> <p>or, after having started the internal CLI with <code>lmme terminal</code>:</p> <pre><code>&gt; query \"What are observational studies?\" --model minor --temperature 0.2\n</code></pre> <p>You can also specify a model directly, using the format modelprovider/modelname:</p> <pre><code>lmme query \"What are observational studies?\" --model OpenAI/gpt-4-mini\n</code></pre> <p>(<code>lmme query --help</code> will provide a list of all optional arguments).</p> <p>With the internal CLI,</p> <pre><code>&gt; query \"What are observational studies?\" --model OpenAI/gpt-4-mini\n</code></pre> <p>Here too to get a list of available options, </p> <pre><code>&gt; query --help\n</code></pre> <p>When using the Python REPL or from code, you can specify a model to which you have access directly using the following syntax.</p> <pre><code>from lmm_education import query\n\nresponse = query(\"What are observational studies?\", {'model': \"OpenAI/gpt-4-mini\", 'temperature': 0.3})\nprint(response)\n</code></pre>"},{"location":"Handbook/RAGauthoring/#forming-a-database-of-questions","title":"Forming a database of questions","text":"<p>It may be a good a idea to keep a note of the questions that created problems during the course, or of questions that are asked frequently. If the encoding model is changed and the database formed anew, or if the RAG material is subsequently substantially revised, these questions may be used to create a battery to evaluate the performance of retrieval.</p>"}]}